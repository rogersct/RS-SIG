{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import timeit\n",
    "tqdm.pandas()\n",
    "import omap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-14 17:00:00\n",
      "2020-10-26 16:26:04\n",
      "2018-02-26 00:00:00\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "ts = int(\"1573750800\")\n",
    "\n",
    "# if you encounter a \"year is out of range\" error the timestamp\n",
    "# may be in milliseconds, try `ts /= 1000` in that case\n",
    "print(datetime.utcfromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "ts = int(\"1603729564\")\n",
    "\n",
    "# if you encounter a \"year is out of range\" error the timestamp\n",
    "# may be in milliseconds, try `ts /= 1000` in that case\n",
    "print(datetime.utcfromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "ts = int(\"1519603200\")\n",
    "\n",
    "# if you encounter a \"year is out of range\" error the timestamp\n",
    "# may be in milliseconds, try `ts /= 1000` in that case\n",
    "print(datetime.utcfromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datetime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5a6d65b972ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m \u001b[0mdf_c2_clean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_od_clean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_evr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_folder_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_output_filename\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-5a6d65b972ea>\u001b[0m in \u001b[0;36mprocess_evr\u001b[1;34m(input_folder_path, start_time, end_time)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m# Determine time difference between start and end time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mdatetime_format\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'%Y/%m/%d %H:%M:%S'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatetime_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend_time\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatetime_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mstart_time_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%m/%d/%Y %H:%M:%S'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'datetime' is not defined"
     ]
    }
   ],
   "source": [
    "# input_folder_path =  './EVR_T23_Car46_261020/'\n",
    "# #input_folder_path = './Train 23 14NOV2019 Work Folder/raw logs'\n",
    "# start_time = '2020/10/26 15:50:00' \n",
    "# end_time = '2020/10/26 16:50:00'\n",
    "# #start_time = '2020/10/26 04:36:45'\n",
    "# #end_time = '2020/10/26 04:37:09'\n",
    "# sample_output_filename = ['./EVR_T23_Car46_261020/preprocessing_output/EVR_20201026_1550_to_20201026_1650.csv']\n",
    "\n",
    "input_folder_path = './Train 23 14NOV2019 Work Folder/raw logs'\n",
    "#input_folder_path = './Train 23 14NOV2019 Work Folder/raw logs'\n",
    "start_time = '2019/11/14 17:00:00' \n",
    "end_time = '2019/11/14 18:00:00'\n",
    "sample_output_filename = ['./Train 23 14NOV2019 Work Folder/preprocessing_output/EVR_20191114_1700_to_20191114_1800.csv']\n",
    "\n",
    "# input_folder_path = './Train 23 14NOV2019 Work Folder/raw logs'\n",
    "# #input_folder_path = './Train 23 14NOV2019 Work Folder/raw logs'\n",
    "# start_time = '2019/11/14 18:00:00' \n",
    "# end_time = '2019/11/14 19:00:00'\n",
    "# sample_output_filename = ['./Train 23 14NOV2019 Work Folder/preprocessing_output/EVR_20191114_1800_to_20191114_1900.csv']\n",
    "\n",
    "def process_evr (input_folder_path, start_time, end_time):\n",
    "    # Determine time difference between start and end time\n",
    "    datetime_format = '%Y/%m/%d %H:%M:%S'\n",
    "    start_time = datetime.datetime.strptime(start_time, datetime_format)\n",
    "    end_time = datetime.datetime.strptime(end_time, datetime_format)\n",
    "    start_time_new = start_time.strftime('%m/%d/%Y %H:%M:%S')\n",
    "    end_time_new = end_time.strftime('%m/%d/%Y %H:%M:%S')\n",
    "    # Check if EVR folder exist\n",
    "    if not os.path.exists(os.path.join(input_folder_path, 'EVR')):\n",
    "        print(\"EVR folder does not exist. Kindly check the folder strucutre. No EVR processing will be performed\")\n",
    "    else:\n",
    "        print(\"EVR folder exist!\")\n",
    "        # Check if C2 file exist\n",
    "        c2_pattern = 'EVR_Car\\d+_\\d+_C2.txt'\n",
    "        norm_pattern = 'EVR_Car\\d+_\\d+.txt'\n",
    "        c2_flag = False\n",
    "        norm_flag = False\n",
    "        for evr_file in os.listdir(os.path.join(input_folder_path, 'EVR')):\n",
    "            if re.search(c2_pattern, evr_file):\n",
    "                c2_filename = evr_file\n",
    "                c2_flag = True\n",
    "            if re.search(norm_pattern,evr_file):\n",
    "                norm_filename = evr_file\n",
    "                norm_flag = True\n",
    "\n",
    "        if not (c2_flag and norm_flag):\n",
    "            sys.exit(\"Missing EVR files! Kindly check EVR folder!\")\n",
    "        else:\n",
    "            # Process C2 file\n",
    "            df_c2 = pd.read_csv(os.path.join(input_folder_path, 'EVR', c2_filename), sep=\";\")\n",
    "            df_c2['Date'] = pd.to_datetime(df_c2['Date'], format='%m/%d/%Y %H:%M:%S')\n",
    "            df_c2_period = df_c2.loc[(df_c2['Date'] >= start_time_new) & (df_c2['Date'] <= end_time_new)]\n",
    "            #df_c2_clean_timestamp = preprocess_timestamp_old(df_c2_period)\n",
    "            df_c2_clean = clean_dataframe(df_c2_period, 'C2')\n",
    "            #df_c2_clean.to_csv('./result_c2.csv', index=False, header=True)\n",
    "            \n",
    "            # Processs Operating Data file\n",
    "            df_od = pd.read_csv(os.path.join(input_folder_path, 'EVR', norm_filename), sep=\";\")\n",
    "            df_od['Date'] = pd.to_datetime(df_od['Date'], format='%m/%d/%Y %H:%M:%S')\n",
    "            df_od_period = df_od.loc[(df_od['Date'] >= start_time_new) & (df_od['Date'] <= end_time_new)]\n",
    "            #df_od_clean_timestamp = preprocess_timestamp_old(df_od_period)\n",
    "            df_od_clean = clean_dataframe(df_od_period, 'OD')\n",
    "            #df_od_clean.to_csv('./result_od.csv', index=False, header=True)\n",
    "            \n",
    "            # Merge other Operating Data to C2 Dataframe based on Timestampms (ordered manner)\n",
    "            df_result = pd.merge_ordered(df_c2_clean, df_od_clean, how='outer', on='Timestampms')\n",
    "            #df_result.to_csv('./result_1.csv', index=False, header=True)\n",
    "            \n",
    "            \n",
    "            df_result = process_timestamp(df_result, df_c2_clean, df_od_clean, start_time, end_time)\n",
    "            #df_result.to_csv('./result_9.csv', index=False, header=True)\n",
    "            # Merge C2 and OD files first\n",
    "            \n",
    "            # Remove similar record numbers for both EC2 and EOD\n",
    "            #df_duplicate = df[df.duplicated('EC2_Record Number',keep='first')]\n",
    "            filter_col = [col for col in df_result if col.startswith('EC2_')]\n",
    "            df_result.loc[df_result.duplicated('EC2_Record Number',keep='first'), filter_col] = np.NaN\n",
    "            filter_col = [col for col in df_result if col.startswith('EOD_')]\n",
    "            df_result.loc[df_result.duplicated('EOD_Record Number',keep='first'), filter_col] = np.NaN\n",
    "            \n",
    "            # Group resulting same real timestamp together\n",
    "            df_result = df_result.groupby(\"Timestampms\").last().reset_index()\n",
    "\n",
    "            #df_result.to_csv('./result_8.csv', index=False, header=True)\n",
    "            # Clean Timestamp\n",
    "            #df_result = preprocess_timestamp(df_result)\n",
    "            \n",
    "            \n",
    "    return df_c2_clean, df_od_clean, df_result\n",
    "\n",
    "\n",
    "df_c2_clean, df_od_clean, df_result = process_evr(input_folder_path, start_time, end_time)\n",
    "\n",
    "output(sample_output_filename[0], df_result)\n",
    "#unit_test(sample_output_filename[0], 'C2', df_c2_clean)  \n",
    "#unit_test(sample_output_filename[0], 'OD', df_od_clean)  \n",
    "#unit_test_all(sample_output_filename[0], df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def clean_dataframe(df, name):\n",
    "    # Drop columns with NaNs\n",
    "    df = df.dropna(how='any',axis=1)\n",
    "    # Sort df based on column 'Date'\n",
    "    df = df.sort_values(by='Date',ascending=True).reset_index(drop=True)\n",
    "    # Convert datetime to timestamp (Unix) in milisecond resolution\n",
    "    df['Timestamp'] = df.Date.values.astype(np.int64) // 10 ** 9 *1000\n",
    "    # Add column for Real_Time Timestampms\n",
    "    df['Timestampms'] = df['Timestamp'] \n",
    "    \n",
    "    # Add Prefix to Columns name (e.g. EC2/EOD_***) except Timestampms\n",
    "    if name == 'C2':\n",
    "        df.columns = df.columns.map(lambda x : 'EC2_'+x if x !='Timestampms' else x)\n",
    "    elif name == 'OD':\n",
    "        df.columns = df.columns.map(lambda x : 'EOD_'+x if x !='Timestampms' else x)\n",
    "                \n",
    "    df = df[ ['Timestampms'] + [ col for col in df.columns if col != 'Timestampms' ] ]    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_timestamp(df, df_c2_clean, df_od_clean, start_time, end_time):\n",
    "    # Find duplicates datetime in column 'Timestampms'\n",
    "    df_duplicate = df[df.duplicated('Timestampms',keep=False)]\n",
    "    previous_datetime = 0\n",
    "    consec_flag = False\n",
    "    for index, unique_datetime in enumerate (df_duplicate['Timestampms'].unique()):\n",
    "        timestamp_df = df_duplicate.loc[df_duplicate['Timestampms'] == unique_datetime]\n",
    "              \n",
    "        # Find difference from previous and next timestamp\n",
    "        difference_previous,  difference_next = find_difference(timestamp_df, unique_datetime, df)\n",
    "        \n",
    "        #Check if record number is in duplicate\n",
    "        df_ec_record_number = timestamp_df[timestamp_df.duplicated('EC2_Record Number',keep=False)]\n",
    "        df_od_record_number = timestamp_df[timestamp_df.duplicated('EOD_Record Number',keep=False)]\n",
    "        \n",
    "        # EC2 Data has duplciate timestamp\n",
    "        if len(df_ec_record_number) == 0 and len(df_od_record_number) > 0:\n",
    "            df = split_equally(timestamp_df, df, 'C2')\n",
    "            \n",
    "        # OD Data has duplicate timestamp\n",
    "        elif len(df_ec_record_number) > 0 and len(df_od_record_number) == 0:\n",
    "            df_od_timestamp = df_od_clean.loc[df_od_clean['Timestampms'] == unique_datetime]\n",
    "            df_od_timestamp_next = df_od_clean.loc[df_od_clean['Timestampms'] == df_duplicate['Timestampms'].unique()[index+1]]\n",
    "                        \n",
    "            od_difference_previous,  od_difference_next = find_difference(df_od_timestamp, unique_datetime, df_od_clean)\n",
    "            \n",
    "            handled_previous =  df.loc[timestamp_df.index[0]-1,'Timestampms'] - df.loc[timestamp_df.index[0]-1,'EOD_Timestamp']\n",
    "            print(consec_flag)\n",
    "            \n",
    "            if difference_previous < 1000 or df.loc[timestamp_df.index[0],'EOD_Record Number'] > df.loc[timestamp_df.index[-1],'EOD_Record Number']:\n",
    "                df = split_equally(timestamp_df, df, 'OD')    \n",
    "            \n",
    "            elif df.loc[timestamp_df.index[0],'EOD_Record Number'] < df.loc[timestamp_df.index[-1],'EOD_Record Number']:\n",
    "                #print(df_od_clean.loc[df_od_timestamp.index[0],'Timestampms'],  df.loc[timestamp_df.index[0]-1,'Timestampms'])\n",
    "                if difference_previous < 1000:\n",
    "                    df = split_equally(timestamp_df, df, 'OD')\n",
    "                    \n",
    "                elif od_difference_previous == 1000 and od_difference_next == 1000:\n",
    "                    df = split_equally(timestamp_df, df, 'OD')\n",
    "                    \n",
    "                elif df_od_clean.loc[df_od_timestamp.index[0],'Timestampms'] - df.loc[timestamp_df.index[0]-1,'Timestampms'] == 1000 and (od_difference_previous >= 2000 and od_difference_next == 1000) and consec_flag:\n",
    "                    df = split_equally(timestamp_df, df, 'OD')\n",
    "                    \n",
    "                elif difference_previous == 1000 and od_difference_previous >= 2000 and od_difference_next == 1000 and not consec_flag:\n",
    "                    df = split_backward(timestamp_df, df, 'OD')\n",
    "                    \n",
    "                else:\n",
    "                    df = split_forward(timestamp_df, df, 'OD')\n",
    "                    \n",
    "                    \n",
    "            if not df_od_timestamp_next.empty and (df_od_timestamp_next.index[0]-df_od_timestamp.index[-1]==1):\n",
    "                consec_flag = True\n",
    "            else:\n",
    "                consec_flag = False\n",
    "            \n",
    "        previous_datetime = unique_datetime\n",
    "            \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_difference(timestamp_df, unique_datetime, df):\n",
    "    difference_previous, difference_next = 0 , 0\n",
    "    if timestamp_df.index[0]-1 >=0 and timestamp_df.index[-1]+1 <= len(df)-1:\n",
    "        difference_previous = unique_datetime - df.loc[timestamp_df.index[0]-1,'Timestampms']\n",
    "        difference_next = df.loc[timestamp_df.index[-1]+1,'Timestampms'] - unique_datetime\n",
    "    elif timestamp_df.index[-1] == len(df)-1:\n",
    "        difference_previous = unique_datetime - df.loc[timestamp_df.index[0]-1,'Timestampms']\n",
    "    elif timestamp_df.index[0] == 0:\n",
    "        difference_next = df.loc[timestamp_df.index[-1]+1,'Timestampms'] - unique_datetime\n",
    "    \n",
    "    return difference_previous, difference_next\n",
    "\n",
    "def split_equally(timestamp_df, df, name):\n",
    "    print(\"--- \" + name +\" Handling Duplicates, Split duplicate evenly---\")\n",
    "    # For every Timestamp (1000 millisecond), find the interval\n",
    "    interval_ms = round(1000 / len(timestamp_df.index))\n",
    "    for counter in range (0, len(timestamp_df.index)):\n",
    "        df.loc[timestamp_df.index[counter],'Timestampms'] = df.loc[timestamp_df.index[counter],'Timestampms'] + counter*interval_ms\n",
    "        print(df.loc[timestamp_df.index[counter],'E'+name+'_Record Number'], df.loc[timestamp_df.index[counter],'E'+name+'_Date'],\n",
    "              df.loc[timestamp_df.index[counter],'Timestampms'],df.loc[timestamp_df.index[counter],'E'+name+'_Record Number'])\n",
    "    return df\n",
    "\n",
    "def split_forward(timestamp_df, df, name):\n",
    "    print(\"---Handling Duplicates, Fill gap with duplicate, Forward---\")\n",
    "    for counter in range (0, len(timestamp_df.index)):\n",
    "        df.loc[timestamp_df.index[counter],'Timestampms'] = df.loc[timestamp_df.index[counter],'Timestampms'] + counter*1000\n",
    "        print(df.loc[timestamp_df.index[counter],'E'+name+'_Record Number'], df.loc[timestamp_df.index[counter],'E'+name+'_Date'],\n",
    "              df.loc[timestamp_df.index[counter],'Timestampms'],df.loc[timestamp_df.index[counter],'E'+name+'_Record Number'])\n",
    "    return df\n",
    "\n",
    "def split_backward(timestamp_df, df, name):\n",
    "    # Loop backwards\n",
    "    print(\"---Handling Duplicates, Fill gap with duplicate, Backward---\")\n",
    "    count = 0\n",
    "    for counter in range (len(timestamp_df.index), 0, -1):\n",
    "        df.loc[timestamp_df.index[counter-1],'Timestampms'] = df.loc[timestamp_df.index[counter-1],'Timestampms'] - count*1000\n",
    "        count +=1\n",
    "        print(df.loc[timestamp_df.index[counter-1],'E'+name+'_Record Number'], df.loc[timestamp_df.index[counter-1],'E'+name+'_Date'],\n",
    "              df.loc[timestamp_df.index[counter-1],'Timestampms'],df.loc[timestamp_df.index[counter-1],'E'+name+'_Record Number'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def output(sample_output_filename, df_result):\n",
    "    # Import Output File\n",
    "    df_output = pd.read_csv(sample_output_filename)\n",
    "    \n",
    "    df_combine = pd.DataFrame()\n",
    "    df_combine = df_combine.assign(epoch = df_output['epoch'])\n",
    "    df_combine = df_combine.assign(Timestampms = df_result['Timestampms']) \n",
    "    df_combine = df_combine.assign(EC2_Record_Number = df_result['EC2_Record Number']) \n",
    "    df_combine = df_combine.assign(EC2_Date = df_result['EC2_Date']) \n",
    "    df_combine = df_combine.assign(EOD_Record_Number = df_result['EOD_Record Number']) \n",
    "    df_combine = df_combine.assign(EOD_Date = df_result['EOD_Date']) \n",
    "\n",
    "    \n",
    "    df_combine.to_csv('./result_combine.csv', index=False, header=True)\n",
    "    \n",
    "def unit_test(sample_output_filename, name, df_test):\n",
    "    print(\"---Unit Test for \" + name + \" Dataframe---\")\n",
    "    # Unit Test for C2\n",
    "    # Import Output File\n",
    "    df_output = pd.read_csv(sample_output_filename)\n",
    "    # Only retrieve respective columns\n",
    "    if name == 'C2':\n",
    "        df_output = df_output.drop(df_output[(df_output['EC2_002_Record_Number'].isnull())].index)\n",
    "        #drop column with prefix EOD\n",
    "        df_output = df_output.loc[:, ~df_output.columns.str.startswith('EOD')]\n",
    "    elif name == 'OD':\n",
    "        df_output = df_output.drop(df_output[(df_output['EOD_002_Record_Number'].isnull())].index)\n",
    "        #drop column with prefix EOD\n",
    "        df_output = df_output.loc[:, ~df_output.columns.str.startswith('EC2')]\n",
    "    \n",
    "    df_output['epoch'] = df_output.epoch.values.astype(np.float64)\n",
    "    df_output = df_output.reset_index(drop=True)\n",
    "    print(df_output.shape, df_test.shape)    \n",
    "    #Output to CSV\n",
    "    #df_output.to_csv('./df_output.csv', index=False, header=True)\n",
    "\n",
    "    #Output to CSV\n",
    "    #df_test.to_csv('./df_test.csv', index=False, header=True)\n",
    "    #df_drop_test = pd.read_csv('./df_test.csv')\n",
    "    df_drop_test = df_test\n",
    "    df_test['result'] = np.where(df_test['Timestampms'] == df_output['epoch'], 'True', 'False')\n",
    "    df_test.to_csv('./df_test.csv', index=False, header=True)\n",
    "    #df_drop_test = df_drop_test.sort_values(by='ATO_Real_Timestampms',ascending=True).reset_index(drop=True)\n",
    "    # Assert whether sample output and self processed are equal\n",
    "    assert_equal = omap.nan_equal(df_drop_test['Timestampms'].values, df_output['epoch'].values)\n",
    "    print(\"Equality Between Sample Output and Self Processed: \", assert_equal)\n",
    "    assert_equal = omap.nan_equal(df_drop_test['E'+name+'_Record Number'].values, df_output['E'+name+'_002_Record_Number'].values)\n",
    "    print(\"Equality Between Sample Output and Self Processed: \", assert_equal)\n",
    "    \n",
    "    #df_drop_test.columns = df_output.columns\n",
    "    #print(np.testing.assert_allclose(df_drop_test.values, df_output.values, rtol=1e-10, atol=0))\n",
    "    #print(pd.testing.assert_frame_equal(df_drop_test, df_output, check_dtype=False))\n",
    "    #print(df_drop_test.compare(df_output, align_axis=0))\n",
    "    #assert_equal = nan_equal(df_drop_test.values, df_output.values)\n",
    "    #assert_equal = nan_equal(df_drop['ATO_* General'].values, df_output['ATO_0101__General'].values)\n",
    "    #print(\"Equality Between Sample Output and Self Processed: \", assert_equal)\n",
    "    #print(np.testing.assert_equal(df_drop_test.values, df_output.values))\n",
    "    \n",
    "def unit_test_all(sample_output_filename, df_test):\n",
    "    print(\"---Unit Test for entire Dataframe---\")\n",
    "    # Import Output File\n",
    "    df_output = pd.read_csv(sample_output_filename)\n",
    "    print(df_output.shape, df_test.shape)\n",
    "    assert_equal = omap.nan_equal(df_test['Timestampms'].values, df_output['epoch'].values)\n",
    "    print(\"Equality Between Sample Output and Self Processed: \", assert_equal)\n",
    "    df_test['result'] = np.where(df_test['Timestampms'] == df_output['epoch'], 'True', 'False')\n",
    "    #df_test.to_csv('./df_test.csv', index=False, header=True)\n",
    "        \n",
    "    df_test['result_1'] = np.where(df_test['EC2_Record Number'] == df_output['EC2_002_Record_Number'], 'True', 'False')\n",
    "    df_test.to_csv('./df_test.csv', index=False, header=True)\n",
    "    assert_equal = omap.nan_equal(df_test['EC2_Record Number'].values, df_output['EC2_002_Record_Number'].values)\n",
    "    print(\"Equality Between Sample Output and Self Processed: \", assert_equal)\n",
    "    assert_equal = omap.nan_equal(df_test['EOD_Record Number'].values, df_output['EOD_002_Record_Number'].values)\n",
    "    print(\"Equality Between Sample Output and Self Processed: \", assert_equal)\n",
    "    print(np.testing.assert_equal(df_test['EC2_Record Number'].values, df_output['EC2_002_Record_Number'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
