{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import timeit\n",
    "tqdm.pandas()\n",
    "import omap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Processing Train 20 Car 39---\n",
      "---Processing ATO Dataframe---\n",
      "Number of null value entry =  0\n",
      "Found non-monotonic sequence at index:  Int64Index([19474, 59862, 68624], dtype='int64')\n",
      "[Timestamp('2020-01-16 05:20:10'), Timestamp('2020-01-16 05:20:11')]\n",
      "[17, 16]\n",
      "[17, 16]\n",
      "19461 2020-01-16 05:20:10\n",
      "19462 2020-01-16 05:20:10\n",
      "19463 2020-01-16 05:20:10\n",
      "19464 2020-01-16 05:20:10\n",
      "19465 2020-01-16 05:20:10\n",
      "19466 2020-01-16 05:20:10\n",
      "19467 2020-01-16 05:20:10\n",
      "19468 2020-01-16 05:20:10\n",
      "19469 2020-01-16 05:20:10\n",
      "19470 2020-01-16 05:20:10\n",
      "19471 2020-01-16 05:20:10\n",
      "19472 2020-01-16 05:20:10\n",
      "19473 2020-01-16 05:20:10\n",
      "19474 2020-01-16 05:20:10\n",
      "19475 2020-01-16 05:20:10\n",
      "19476 2020-01-16 05:20:10\n",
      "19477 2020-01-16 05:20:10\n",
      "19478 2020-01-16 05:20:11\n",
      "19479 2020-01-16 05:20:11\n",
      "19480 2020-01-16 05:20:11\n",
      "19481 2020-01-16 05:20:11\n",
      "19482 2020-01-16 05:20:11\n",
      "19483 2020-01-16 05:20:11\n",
      "19484 2020-01-16 05:20:11\n",
      "19485 2020-01-16 05:20:11\n",
      "19486 2020-01-16 05:20:11\n",
      "19487 2020-01-16 05:20:11\n",
      "19488 2020-01-16 05:20:11\n",
      "19489 2020-01-16 05:20:11\n",
      "19490 2020-01-16 05:20:11\n",
      "19491 2020-01-16 05:20:11\n",
      "19492 2020-01-16 05:20:11\n",
      "19493 2020-01-16 05:20:11\n",
      "[Timestamp('2020-01-16 06:29:07'), Timestamp('2020-01-16 06:29:08')]\n",
      "[17, 17]\n",
      "59848 2020-01-16 06:29:07\n",
      "59849 2020-01-16 06:29:07\n",
      "59850 2020-01-16 06:29:07\n",
      "59851 2020-01-16 06:29:07\n",
      "59852 2020-01-16 06:29:07\n",
      "59853 2020-01-16 06:29:07\n",
      "59854 2020-01-16 06:29:07\n",
      "59855 2020-01-16 06:29:07\n",
      "59856 2020-01-16 06:29:07\n",
      "59857 2020-01-16 06:29:07\n",
      "59858 2020-01-16 06:29:07\n",
      "59859 2020-01-16 06:29:07\n",
      "59860 2020-01-16 06:29:07\n",
      "59861 2020-01-16 06:29:07\n",
      "59862 2020-01-16 06:29:07\n",
      "59863 2020-01-16 06:29:07\n",
      "59864 2020-01-16 06:29:07\n",
      "59865 2020-01-16 06:29:08\n",
      "59866 2020-01-16 06:29:08\n",
      "59867 2020-01-16 06:29:08\n",
      "59868 2020-01-16 06:29:08\n",
      "59869 2020-01-16 06:29:08\n",
      "59870 2020-01-16 06:29:08\n",
      "59871 2020-01-16 06:29:08\n",
      "59872 2020-01-16 06:29:08\n",
      "59873 2020-01-16 06:29:08\n",
      "59874 2020-01-16 06:29:08\n",
      "59875 2020-01-16 06:29:08\n",
      "59876 2020-01-16 06:29:08\n",
      "59877 2020-01-16 06:29:08\n",
      "59878 2020-01-16 06:29:08\n",
      "59879 2020-01-16 06:29:08\n",
      "59880 2020-01-16 06:29:08\n",
      "59881 2020-01-16 06:29:08\n",
      "[Timestamp('2020-01-16 06:44:02'), Timestamp('2020-01-16 06:44:03')]\n",
      "[18, 18]\n",
      "68608 2020-01-16 06:44:02\n",
      "68609 2020-01-16 06:44:02\n",
      "68610 2020-01-16 06:44:02\n",
      "68611 2020-01-16 06:44:02\n",
      "68612 2020-01-16 06:44:02\n",
      "68613 2020-01-16 06:44:02\n",
      "68614 2020-01-16 06:44:02\n",
      "68615 2020-01-16 06:44:02\n",
      "68616 2020-01-16 06:44:02\n",
      "68617 2020-01-16 06:44:02\n",
      "68618 2020-01-16 06:44:02\n",
      "68619 2020-01-16 06:44:02\n",
      "68620 2020-01-16 06:44:02\n",
      "68621 2020-01-16 06:44:02\n",
      "68622 2020-01-16 06:44:02\n",
      "68623 2020-01-16 06:44:02\n",
      "68624 2020-01-16 06:44:02\n",
      "68625 2020-01-16 06:44:02\n",
      "68626 2020-01-16 06:44:03\n",
      "68627 2020-01-16 06:44:03\n",
      "68628 2020-01-16 06:44:03\n",
      "68629 2020-01-16 06:44:03\n",
      "68630 2020-01-16 06:44:03\n",
      "68631 2020-01-16 06:44:03\n",
      "68632 2020-01-16 06:44:03\n",
      "68633 2020-01-16 06:44:03\n",
      "68634 2020-01-16 06:44:03\n",
      "68635 2020-01-16 06:44:03\n",
      "68636 2020-01-16 06:44:03\n",
      "68637 2020-01-16 06:44:03\n",
      "68638 2020-01-16 06:44:03\n",
      "68639 2020-01-16 06:44:03\n",
      "68640 2020-01-16 06:44:03\n",
      "68641 2020-01-16 06:44:03\n",
      "68642 2020-01-16 06:44:03\n",
      "68643 2020-01-16 06:44:03\n",
      "Int64Index([], dtype='int64')\n",
      "---Processing ATP Dataframe---\n",
      "Number of null value entry =  0\n",
      "---Processing COM Dataframe---\n",
      "Number of null value entry =  0\n",
      "---Processing TDMS Dataframe---\n",
      "Number of null value entry =  0\n",
      "---Merging Dataframes---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 80625/80625 [00:32<00:00, 2515.97it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 80625/80625 [00:04<00:00, 16752.62it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 80625/80625 [01:17<00:00, 1040.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Merging Dataframes---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 80468/80468 [00:44<00:00, 1817.17it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 80468/80468 [00:03<00:00, 21569.21it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 80468/80468 [01:14<00:00, 1086.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Merging Dataframes---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 87775/87775 [00:31<00:00, 2767.35it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 87775/87775 [00:04<00:00, 19682.04it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 87775/87775 [01:08<00:00, 1283.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Merging all Dataframes (ATO, ATP, COM, TDMS)---\n",
      "Time Taken: 667.8381876000001\n",
      "---Unit Test between merged ATO and ATP Dataframe---\n",
      "(80625, 546) (80625, 546)\n",
      "None\n",
      "None\n",
      "             ATO_1220_Energy_delta  ATO_2008_ATP_Energy_delta\n",
      "0     self                   8.989                      8.989\n",
      "      other                  8.989                      8.989\n",
      "1     self                   8.989                      8.989\n",
      "      other                  8.989                      8.989\n",
      "2     self                   8.989                      8.989\n",
      "...                            ...                        ...\n",
      "80613 other                677.366                    677.366\n",
      "80614 self                 677.366                    677.366\n",
      "      other                677.366                    677.366\n",
      "80616 self                 677.366                    677.366\n",
      "      other                677.366                    677.366\n",
      "\n",
      "[7122 rows x 2 columns]\n",
      "Equality Between Sample Output and Self Processed:  True\n",
      "---Unit Test between merged ATO and COM Dataframe---\n",
      "(80468, 327) (80468, 327)\n",
      "None\n",
      "None\n",
      "             ATO_1220_Energy_delta  ATO_2008_ATP_Energy_delta\n",
      "0     self                   8.989                      8.989\n",
      "      other                  8.989                      8.989\n",
      "1     self                   8.989                      8.989\n",
      "      other                  8.989                      8.989\n",
      "2     self                   8.989                      8.989\n",
      "...                            ...                        ...\n",
      "80456 other                677.366                    677.366\n",
      "80457 self                 677.366                    677.366\n",
      "      other                677.366                    677.366\n",
      "80459 self                 677.366                    677.366\n",
      "      other                677.366                    677.366\n",
      "\n",
      "[7122 rows x 2 columns]\n",
      "Equality Between Sample Output and Self Processed:  True\n",
      "---Unit Test between merged ATO and TDMS Dataframe---\n",
      "(87775, 438) (87775, 438)\n",
      "None\n",
      "None\n",
      "             ATO_1220_Energy_delta  ATO_2008_ATP_Energy_delta\n",
      "0     self                   8.989                      8.989\n",
      "      other                  8.989                      8.989\n",
      "1     self                   8.989                      8.989\n",
      "      other                  8.989                      8.989\n",
      "2     self                   8.989                      8.989\n",
      "...                            ...                        ...\n",
      "87765 other                677.366                    677.366\n",
      "87766 self                 677.366                    677.366\n",
      "      other                677.366                    677.366\n",
      "87767 self                 677.366                    677.366\n",
      "      other                677.366                    677.366\n",
      "\n",
      "[7122 rows x 2 columns]\n",
      "Equality Between Sample Output and Self Processed:  True\n",
      "(101297, 751) (101297, 751)\n",
      "Equality Between Sample Output and Self Processed:  True\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "starttime = timeit.default_timer()\n",
    "\n",
    "def test_folder_path (input_folder_path, start_time, end_time):\n",
    "    # Retrieve Train number\n",
    "    train_number = re.findall(r'%s(\\d+)' %'Train ', input_folder_path)[0]\n",
    "    \n",
    "    # Determine time difference between start and end time\n",
    "    datetime_format = '%Y/%m/%d %H:%M:%S'\n",
    "    start_time = datetime.datetime.strptime(start_time, datetime_format)\n",
    "    end_time = datetime.datetime.strptime(end_time, datetime_format)\n",
    "    diff = end_time - start_time\n",
    "    hours = diff.total_seconds() /3600\n",
    "\n",
    "    start_time_new = start_time.strftime('%m/%d/%Y %H:%M:%S')\n",
    "    end_time_new = end_time.strftime('%m/%d/%Y %H:%M:%S')\n",
    "    result_list = []\n",
    "    for car in os.listdir(input_folder_path):\n",
    "        car_number = re.findall(r'%s(\\d+)' %'Car ', car)[0]\n",
    "        print(\"---Processing Train \" + train_number + \" Car \" + car_number + \"---\")\n",
    "        date = os.listdir(os.path.join(input_folder_path, car))[0]\n",
    "        flag = True\n",
    "        back_date_flag = False\n",
    "        # Test for correct folder structure\n",
    "        try:\n",
    "            datetime.datetime.strptime(date, \"%y%m%d\")\n",
    "            #print(\"Correct date string format.\")\n",
    "        except ValueError:\n",
    "            print(\"Inorrect date string format. Example 200116 %y%m%d. This might result in process error.\")\n",
    "            flag = False    \n",
    "        \n",
    "        # OMAP Processing\n",
    "        \n",
    "        # Look for ATO, ATP, COM and TDMS folder\n",
    "        if not os.path.exists(os.path.join(input_folder_path, car, date, 'OMAP_ATO')):\n",
    "            print(\"ATO folder does not exist. Incorrect folder structure\")\n",
    "            flag = False\n",
    "        else:\n",
    "            ATO_file_list = os.listdir(os.path.join(input_folder_path, car, date, 'OMAP_ATO'))       \n",
    "            sorted(ATO_file_list)\n",
    "            # Find dataframe/append for the given start and end time\n",
    "            ATO_dataframe = pd.DataFrame()\n",
    "            ATP_dataframe = pd.DataFrame()\n",
    "            COM_dataframe = pd.DataFrame()\n",
    "            TDMS_dataframe = pd.DataFrame()\n",
    "            for index in range (0, len(ATO_file_list)):\n",
    "            #for file in ATO_file_list:\n",
    "                file = ATO_file_list[index]\n",
    "                process_flag = True\n",
    "                # For each ATO file, finding corresponding ATP, COM and TDMS file\n",
    "                ATP_file_list = os.listdir(os.path.join(input_folder_path, car, date, 'OMAP_ATP'))\n",
    "                if not file.replace('ATO','ATP') in ATP_file_list:\n",
    "                    print(file + \" does not have a corresponding ATP file. Hence will not be processed. \")\n",
    "                    process_flag = False\n",
    "                \n",
    "                COM_file_list = os.listdir(os.path.join(input_folder_path, car, date, 'OMAP_COM'))\n",
    "                if not file.replace('ATO','COM') in COM_file_list:\n",
    "                    print(file + \" does not have a corresponding COM file. Hence will not be processed. \")\n",
    "                    process_flag = False\n",
    "                    \n",
    "                TDMS_file_list = os.listdir(os.path.join(input_folder_path, car, date, 'OMAP_TDMS'))\n",
    "                if not file.replace('ATO','TDMS') in TDMS_file_list:\n",
    "                    print(file + \" does not have a corresponding TDMS file. Hence will not be processed. \")\n",
    "                    process_flag = False               \n",
    "                \n",
    "                if process_flag:\n",
    "                    # Read ATO log file and find relevant rows based on start and end time)\n",
    "                    df_ATO = pd.read_csv(os.path.join(input_folder_path, car, date, 'OMAP_ATO', file), sep=\"\\t\")\n",
    "                    df_ATO['Date'] = pd.to_datetime(df_ATO['Date'], format='%m/%d/%Y %H:%M:%S')\n",
    "                    # Read ATP log file and find relevant rows based on start and end time)\n",
    "                    df_ATP = pd.read_csv(os.path.join(input_folder_path, car, date, 'OMAP_ATP', file.replace('ATO','ATP')), sep=\"\\t\")\n",
    "                    df_ATP['Date'] = pd.to_datetime(df_ATP['Date'], format='%m/%d/%Y %H:%M:%S')\n",
    "                    # Read COM log file and find relevant rows based on start and end time)\n",
    "                    df_COM = pd.read_csv(os.path.join(input_folder_path, car, date, 'OMAP_COM', file.replace('ATO','COM')), sep=\"\\t\")\n",
    "                    df_COM['Date'] = pd.to_datetime(df_COM['Date'], format='%m/%d/%Y %H:%M:%S')\n",
    "                    # Read TDMS log file and find relevant rows based on start and end time)\n",
    "                    df_TDMS = pd.read_csv(os.path.join(input_folder_path, car, date, 'OMAP_TDMS', file.replace('ATO','TDMS')), sep=\"\\t\")\n",
    "                    df_TDMS['Date'] = pd.to_datetime(df_TDMS['Date'], format='%m/%d/%Y %H:%M:%S')\n",
    "                                        \n",
    "                    if hours == 1: # Process only 1 log file\n",
    "                        # Compare between cuurent index and next index (to be removed as it doesnt max sense to do it this way)\n",
    "                        # For the sake of being the same as ATLAS\n",
    "                        if (index+1 < len(ATO_file_list)):\n",
    "                            df_ATO_next = pd.read_csv(os.path.join(input_folder_path, car, date, 'OMAP_ATO', ATO_file_list[index+1]), sep=\"\\t\")\n",
    "                            df_ATO_next['Date'] = pd.to_datetime(df_ATO_next['Date'], format='%m/%d/%Y %H:%M:%S')\n",
    "                            next_length = len((df_ATO_next.loc[(df_ATO_next['Date'] == start_time_new)]))\n",
    "                            current_length = len((df_ATO.loc[(df_ATO['Date'] == start_time_new)]))\n",
    "                            \n",
    "                            if next_length > current_length:\n",
    "                                # Read logs from next index\n",
    "                                df_ATP_next = pd.read_csv(os.path.join(input_folder_path, car, date, 'OMAP_ATP', ATO_file_list[index+1].replace('ATO','ATP')), sep=\"\\t\")\n",
    "                                df_ATP_next['Date'] = pd.to_datetime(df_ATP_next['Date'], format='%m/%d/%Y %H:%M:%S')\n",
    "                                df_COM_next= pd.read_csv(os.path.join(input_folder_path, car, date, 'OMAP_COM', ATO_file_list[index+1].replace('ATO','COM')), sep=\"\\t\")\n",
    "                                df_COM_next['Date'] = pd.to_datetime(df_COM_next['Date'], format='%m/%d/%Y %H:%M:%S')\n",
    "                                df_TDMS_next = pd.read_csv(os.path.join(input_folder_path, car, date, 'OMAP_TDMS', ATO_file_list[index+1].replace('ATO','TDMS')), sep=\"\\t\")\n",
    "                                df_TDMS_next['Date'] = pd.to_datetime(df_TDMS_next['Date'], format='%m/%d/%Y %H:%M:%S') \n",
    "                                \n",
    "                                ATO_dataframe = ATO_dataframe.append(df_ATO_next.loc[(df_ATO_next['Date'] >= start_time_new) & (df_ATO_next['Date'] <= end_time_new)])\n",
    "                                ATP_dataframe = ATP_dataframe.append(df_ATP_next.loc[(df_ATP_next['Date'] >= start_time_new) & (df_ATP_next['Date'] <= end_time_new)])\n",
    "                                COM_dataframe = COM_dataframe.append(df_COM_next.loc[(df_COM_next['Date'] >= start_time_new) & (df_COM_next['Date'] <= end_time_new)])\n",
    "                                TDMS_dataframe = TDMS_dataframe.append(df_TDMS_next.loc[(df_TDMS_next['Date'] >= start_time_new) & (df_TDMS_next['Date'] <= end_time_new)])\n",
    "                                break\n",
    "                            else:\n",
    "                                # Read logs from current index\n",
    "                                ATO_dataframe = ATO_dataframe.append(df_ATO.loc[(df_ATO['Date'] >= start_time_new) & (df_ATO['Date'] <= end_time_new)])\n",
    "                                ATP_dataframe = ATP_dataframe.append(df_ATP.loc[(df_ATP['Date'] >= start_time_new) & (df_ATP['Date'] <= end_time_new)])\n",
    "                                COM_dataframe = COM_dataframe.append(df_COM.loc[(df_COM['Date'] >= start_time_new) & (df_COM['Date'] <= end_time_new)])\n",
    "                                TDMS_dataframe = TDMS_dataframe.append(df_TDMS.loc[(df_TDMS['Date'] >= start_time_new) & (df_TDMS['Date'] <= end_time_new)])\n",
    "                                break\n",
    "                        else:\n",
    "                            # Read logs from only index\n",
    "                            ATO_dataframe = ATO_dataframe.append(df_ATO.loc[(df_ATO['Date'] >= start_time_new) & (df_ATO['Date'] <= end_time_new)])\n",
    "                            ATP_dataframe = ATP_dataframe.append(df_ATP.loc[(df_ATP['Date'] >= start_time_new) & (df_ATP['Date'] <= end_time_new)])\n",
    "                            COM_dataframe = COM_dataframe.append(df_COM.loc[(df_COM['Date'] >= start_time_new) & (df_COM['Date'] <= end_time_new)])\n",
    "                            TDMS_dataframe = TDMS_dataframe.append(df_TDMS.loc[(df_TDMS['Date'] >= start_time_new) & (df_TDMS['Date'] <= end_time_new)])\n",
    "                            break\n",
    "                    else:\n",
    "                        # Need to fix for whole number hours (probably not because dont have to copy how atlas process which files)\n",
    "                        ATO_dataframe = ATO_dataframe.append(df_ATO.loc[(df_ATO['Date'] >= start_time_new) & (df_ATO['Date'] < end_time_new)])\n",
    "                        ATP_dataframe = ATP_dataframe.append(df_ATP.loc[(df_ATP['Date'] >= start_time_new) & (df_ATP['Date'] < end_time_new)])\n",
    "                        COM_dataframe = COM_dataframe.append(df_COM.loc[(df_COM['Date'] >= start_time_new) & (df_COM['Date'] < end_time_new)])\n",
    "                        TDMS_dataframe = TDMS_dataframe.append(df_TDMS.loc[(df_TDMS['Date'] >= start_time_new) & (df_TDMS['Date'] < end_time_new)])\n",
    "                        # Check if log is '00'_00_00 time and within time period\n",
    "                        if re.findall(r'%s(\\d+)' %'_', file)[0] == '00' and len(df_ATO.loc[(df_ATO['Date'] >= start_time_new) & (df_ATO['Date'] < end_time_new)])>0:\n",
    "                            back_date_flag = True\n",
    "                else:\n",
    "                    sys.exit(car + \" data folder is incorrect / missing corresponding log files. Cannot proceed with processing! Kindly check whether ATO, ATP COM and TDMS have their corresponding log files.\")\n",
    "            \n",
    "            \n",
    "            # Check if log is '00'_00_00 time and within time period\n",
    "            if re.findall(r'%s(\\d+)' %'_', file)[0] == '00' and len(ATO_dataframe.loc[(ATO_dataframe['Date'] >= start_time_new) & (ATO_dataframe['Date'] <= end_time_new)])>0:\n",
    "                back_date_flag = True\n",
    "                print(\"back_date_flag\")\n",
    "                        \n",
    "            ATO_dataframe = ATO_dataframe.reset_index(drop=True)\n",
    "            ATP_dataframe = ATP_dataframe.reset_index(drop=True)\n",
    "            COM_dataframe = COM_dataframe.reset_index(drop=True)\n",
    "            TDMS_dataframe = TDMS_dataframe.reset_index(drop=True)\n",
    "            \n",
    "            if(ATO_dataframe.shape[0] == 0):\n",
    "                sys.exit(\"No matching time window from start to end time! Kindly check if you have input the correct start and end time!\")\n",
    "            \n",
    "            #print(ATO_dataframe.tail(30))\n",
    "            # Process ATO, ATP, COM, TDMS individually\n",
    "            ATO_dataframe = omap.process_ato(ATO_dataframe, back_date_flag)\n",
    "            ATP_dataframe = omap.process_others(ATP_dataframe, 'ATP')\n",
    "            COM_dataframe = omap.process_others(COM_dataframe, 'COM')\n",
    "            TDMS_dataframe = omap.process_others(TDMS_dataframe, 'TDMS')\n",
    "\n",
    "            # Merge ATO-ATP, ATO-COM, ATO-TDMS\n",
    "            ATO_ATP_result = omap.merge_ato_n_others(ATO_dataframe, ATP_dataframe)\n",
    "            ATO_COM_result = omap.merge_ato_n_others(ATO_dataframe, COM_dataframe)\n",
    "            ATO_TDMS_result = omap.merge_ato_n_others(ATO_dataframe, TDMS_dataframe)\n",
    "\n",
    "            # Merge all results dataframes\n",
    "            df_result = omap.merge_all(ATO_ATP_result, ATO_COM_result, ATO_TDMS_result, train_number, car_number, start_time, end_time)\n",
    "            \n",
    "            # Append results for Unit Test\n",
    "            result_list.append([ATO_ATP_result, ATO_COM_result, ATO_TDMS_result, df_result])\n",
    "            \n",
    "    return result_list\n",
    "                          \n",
    "            \n",
    "input_folder_path =  './T20 OMAP DATA/Train 20 CSV/'\n",
    "#input_folder_path =  './test_folder/Train 19 CSV/'\n",
    "#Machine Time of interest ('%Y/%m/%d %H:%M:%S')\n",
    "start_time = '2020/01/16 05:00:00' \n",
    "end_time = '2020/01/16 08:00:00'\n",
    "\n",
    "# Machine Time of interest ('%m/%d/%Y %H:%M:%S')\n",
    "# start_time = '01/16/2020 07:00:00' \n",
    "# end_time = '01/16/2020 08:00:00'\n",
    "#sample_output_filename = ['./OMAP_Train_20_Car_39_20200116_0600_to_20200116_0700.csv']\n",
    "#sample_output_filename = ['./OMAP_Train_20_Car_39_20200116_0700_to_20200116_0800.csv']\n",
    "sample_output_filename = ['./OMAP_Train_20_Car_39_20200116_0500_to_20200116_0800.csv']\n",
    "\n",
    "# input_folder_path = './Train 23 14NOV2019 Work Folder/raw logs/OMAP/Train 23 Copy'\n",
    "# start_time = '2019/11/14 18:00:00' \n",
    "# end_time = '2019/11/14 19:00:00'\n",
    "# # sample_output_filename = ['./Train 23 14NOV2019 Work Folder/preprocessing_output/OMAP_Train_23_Car_45_20191114_1700_to_20191114_1800.csv',\n",
    "# #                           './Train 23 14NOV2019 Work Folder/preprocessing_output/OMAP_Train_23_Car_46_20191114_1700_to_20191114_1800.csv']\n",
    "# sample_output_filename = ['./Train 23 14NOV2019 Work Folder/preprocessing_output/OMAP_Train_23_Car_46_20191114_1800_to_20191114_1900.csv']\n",
    "\n",
    "\n",
    "# input_folder_path = './Train 10 26FEB2018 Work Folder/raw_logs/OMAP/Train 10'\n",
    "# start_time = '2018/02/26 01:00:00' \n",
    "# end_time = '2018/02/26 02:00:00' \n",
    "# sample_output_filename = ['./Train 10 26FEB2018 Work Folder/preprocessed_output/OMAP_Train_10_Car_19_20180226_0100_to_20180226_0200.csv']\n",
    "\n",
    "# input_folder_path = './Train 10 22FEB2018 Work Folder/raw_logs/OMAP/Train 10'\n",
    "# start_time = '2018/02/22 01:00:00' \n",
    "# end_time = '2018/02/22 02:00:00' \n",
    "# sample_output_filename = ['./Train 10 22FEB2018 Work Folder/preprocessed_output/OMAP_Train_10_Car_19_20180222_0100_to_20180222_0200.csv']\n",
    "\n",
    "\n",
    "result_list = test_folder_path(input_folder_path, start_time, end_time)\n",
    "print(\"Time Taken:\", timeit.default_timer() - starttime)\n",
    "\n",
    "for index in range (0, len(result_list)):\n",
    "    # Unit Test\n",
    "    omap.unit_test(sample_output_filename[index], 'ATP', result_list[index][0])\n",
    "    omap.unit_test(sample_output_filename[index], 'COM', result_list[index][1])\n",
    "    omap.unit_test(sample_output_filename[index], 'TDMS', result_list[index][2])\n",
    "    omap.unit_test_all(sample_output_filename[index], result_list[index][3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-14 17:05:00\n",
      "2019-11-14 17:05:17\n",
      "2018-02-26 00:00:00\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "ts = int(\"1573751100\")\n",
    "\n",
    "# if you encounter a \"year is out of range\" error the timestamp\n",
    "# may be in milliseconds, try `ts /= 1000` in that case\n",
    "print(datetime.utcfromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "ts = int(\"1573751117\")\n",
    "\n",
    "# if you encounter a \"year is out of range\" error the timestamp\n",
    "# may be in milliseconds, try `ts /= 1000` in that case\n",
    "print(datetime.utcfromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "ts = int(\"1519603200\")\n",
    "\n",
    "# if you encounter a \"year is out of range\" error the timestamp\n",
    "# may be in milliseconds, try `ts /= 1000` in that case\n",
    "print(datetime.utcfromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(r'%s(\\d+)' %'_', '200215_00_00_00_329_OMAP_ATO.txt')[0])\n",
    "if re.findall(r'%s(\\d+)' %'_', '200215_00_00_00_329_OMAP_ATO.txt')[0] == '00':\n",
    "    print('True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import itertools \n",
    "#interval_list = [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19]\n",
    "interval_list = [20, 20, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19]\n",
    "#interval_list = [20, 20, 19, 19]\n",
    "#interval_list = [20, 20, 20, 20, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19]\n",
    "\n",
    "\n",
    "def distribute_equal (interval_list):\n",
    "    # Get count of each unique interval\n",
    "    unique_value_collection = collections.Counter(interval_list)\n",
    "    unique_value = unique_value_collection.keys()\n",
    "    unique_value_count = unique_value_collection.values()\n",
    "    unique_value, unique_value_count = list(unique_value), list(unique_value_count)\n",
    "    #print(unique_value, unique_value_count)\n",
    "\n",
    "    remain_list = unique_value.copy()\n",
    "    min_index = unique_value_count.index(min(unique_value_count))\n",
    "    max_index = unique_value_count.index(max(unique_value_count))\n",
    "    remain_list.remove(unique_value[min_index])\n",
    "\n",
    "    master_list = []\n",
    "    counter = 0\n",
    "    for x in range (0, min(unique_value_count)):\n",
    "        unique_value_copy = unique_value.copy()\n",
    "        for y in remain_list:\n",
    "            #print(unique_value_collection[y])\n",
    "            #print(unique_value_collection[y]//min(unique_value_count))\n",
    "            if(unique_value_collection[y]//min(unique_value_count)!=1):\n",
    "                for z in range (0, unique_value_collection[y]//min(unique_value_count)-1):\n",
    "                    unique_value_copy.append(y)\n",
    "        counter = counter + len(unique_value_copy)\n",
    "        master_list.append(unique_value_copy)\n",
    "\n",
    "    # Flattened_list\n",
    "    master_list = list(itertools.chain.from_iterable(master_list))\n",
    "\n",
    "    if counter != len(interval_list):\n",
    "        if unique_value[max_index] > unique_value[min_index]:\n",
    "            master_list.insert(0, unique_value[max_index])\n",
    "        else:\n",
    "            master_list.append(unique_value[max_index])\n",
    "    print(master_list)\n",
    "    return master_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
