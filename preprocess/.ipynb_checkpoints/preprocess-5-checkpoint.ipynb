{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import timeit\n",
    "tqdm.pandas()\n",
    "import omap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Processing Train 23 Car 45---\n",
      "---Processing ATO Dataframe---\n",
      "Number of null value entry =  0\n",
      "Found non-monotonic sequence at index:  Int64Index([25122, 41971, 51783, 57456, 85700, 89530, 95312], dtype='int64')\n",
      "[Timestamp('2019-11-14 16:37:01'), Timestamp('2019-11-14 16:37:02')]\n",
      "[17, 16]\n",
      "[17, 16]\n",
      "25109 2019-11-14 16:37:01\n",
      "25110 2019-11-14 16:37:01\n",
      "25111 2019-11-14 16:37:01\n",
      "25112 2019-11-14 16:37:01\n",
      "25113 2019-11-14 16:37:01\n",
      "25114 2019-11-14 16:37:01\n",
      "25115 2019-11-14 16:37:01\n",
      "25116 2019-11-14 16:37:01\n",
      "25117 2019-11-14 16:37:01\n",
      "25118 2019-11-14 16:37:01\n",
      "25119 2019-11-14 16:37:01\n",
      "25120 2019-11-14 16:37:01\n",
      "25121 2019-11-14 16:37:01\n",
      "25122 2019-11-14 16:37:01\n",
      "25123 2019-11-14 16:37:01\n",
      "25124 2019-11-14 16:37:01\n",
      "25125 2019-11-14 16:37:01\n",
      "25126 2019-11-14 16:37:02\n",
      "25127 2019-11-14 16:37:02\n",
      "25128 2019-11-14 16:37:02\n",
      "25129 2019-11-14 16:37:02\n",
      "25130 2019-11-14 16:37:02\n",
      "25131 2019-11-14 16:37:02\n",
      "25132 2019-11-14 16:37:02\n",
      "25133 2019-11-14 16:37:02\n",
      "25134 2019-11-14 16:37:02\n",
      "25135 2019-11-14 16:37:02\n",
      "25136 2019-11-14 16:37:02\n",
      "25137 2019-11-14 16:37:02\n",
      "25138 2019-11-14 16:37:02\n",
      "25139 2019-11-14 16:37:02\n",
      "25140 2019-11-14 16:37:02\n",
      "25141 2019-11-14 16:37:02\n",
      "[Timestamp('2019-11-14 17:05:12'), Timestamp('2019-11-14 17:05:13'), Timestamp('2019-11-14 17:05:14'), Timestamp('2019-11-14 17:05:15'), Timestamp('2019-11-14 17:05:16'), Timestamp('2019-11-14 17:05:17'), Timestamp('2019-11-14 17:05:18'), Timestamp('2019-11-14 17:05:19'), Timestamp('2019-11-14 17:05:20'), Timestamp('2019-11-14 17:05:21'), Timestamp('2019-11-14 17:05:22'), Timestamp('2019-11-14 17:05:23'), Timestamp('2019-11-14 17:05:24'), Timestamp('2019-11-14 17:05:25'), Timestamp('2019-11-14 17:05:26'), Timestamp('2019-11-14 17:05:27'), Timestamp('2019-11-14 17:05:28'), Timestamp('2019-11-14 17:05:29'), Timestamp('2019-11-14 17:05:30'), Timestamp('2019-11-14 17:05:31'), Timestamp('2019-11-14 17:05:32'), Timestamp('2019-11-14 17:05:33'), Timestamp('2019-11-14 17:05:34'), Timestamp('2019-11-14 17:05:35'), Timestamp('2019-11-14 17:05:36'), Timestamp('2019-11-14 17:05:37'), Timestamp('2019-11-14 17:05:38'), Timestamp('2019-11-14 17:05:39')]\n",
      "[20, 20, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19]\n",
      "[20, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 20, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19]\n",
      "41701 2019-11-14 17:05:12\n",
      "41702 2019-11-14 17:05:12\n",
      "41703 2019-11-14 17:05:12\n",
      "41704 2019-11-14 17:05:12\n",
      "41705 2019-11-14 17:05:12\n",
      "41706 2019-11-14 17:05:12\n",
      "41707 2019-11-14 17:05:12\n",
      "41708 2019-11-14 17:05:12\n",
      "41709 2019-11-14 17:05:12\n",
      "41710 2019-11-14 17:05:12\n",
      "41711 2019-11-14 17:05:12\n",
      "41712 2019-11-14 17:05:12\n",
      "41713 2019-11-14 17:05:12\n",
      "41714 2019-11-14 17:05:12\n",
      "41715 2019-11-14 17:05:12\n",
      "41716 2019-11-14 17:05:12\n",
      "41717 2019-11-14 17:05:12\n",
      "41718 2019-11-14 17:05:12\n",
      "41719 2019-11-14 17:05:12\n",
      "41720 2019-11-14 17:05:12\n",
      "41721 2019-11-14 17:05:13\n",
      "41722 2019-11-14 17:05:13\n",
      "41723 2019-11-14 17:05:13\n",
      "41724 2019-11-14 17:05:13\n",
      "41725 2019-11-14 17:05:13\n",
      "41726 2019-11-14 17:05:13\n",
      "41727 2019-11-14 17:05:13\n",
      "41728 2019-11-14 17:05:13\n",
      "41729 2019-11-14 17:05:13\n",
      "41730 2019-11-14 17:05:13\n",
      "41731 2019-11-14 17:05:13\n",
      "41732 2019-11-14 17:05:13\n",
      "41733 2019-11-14 17:05:13\n",
      "41734 2019-11-14 17:05:13\n",
      "41735 2019-11-14 17:05:13\n",
      "41736 2019-11-14 17:05:13\n",
      "41737 2019-11-14 17:05:13\n",
      "41738 2019-11-14 17:05:13\n",
      "41739 2019-11-14 17:05:13\n",
      "41740 2019-11-14 17:05:14\n",
      "41741 2019-11-14 17:05:14\n",
      "41742 2019-11-14 17:05:14\n",
      "41743 2019-11-14 17:05:14\n",
      "41744 2019-11-14 17:05:14\n",
      "41745 2019-11-14 17:05:14\n",
      "41746 2019-11-14 17:05:14\n",
      "41747 2019-11-14 17:05:14\n",
      "41748 2019-11-14 17:05:14\n",
      "41749 2019-11-14 17:05:14\n",
      "41750 2019-11-14 17:05:14\n",
      "41751 2019-11-14 17:05:14\n",
      "41752 2019-11-14 17:05:14\n",
      "41753 2019-11-14 17:05:14\n",
      "41754 2019-11-14 17:05:14\n",
      "41755 2019-11-14 17:05:14\n",
      "41756 2019-11-14 17:05:14\n",
      "41757 2019-11-14 17:05:14\n",
      "41758 2019-11-14 17:05:14\n",
      "41759 2019-11-14 17:05:15\n",
      "41760 2019-11-14 17:05:15\n",
      "41761 2019-11-14 17:05:15\n",
      "41762 2019-11-14 17:05:15\n",
      "41763 2019-11-14 17:05:15\n",
      "41764 2019-11-14 17:05:15\n",
      "41765 2019-11-14 17:05:15\n",
      "41766 2019-11-14 17:05:15\n",
      "41767 2019-11-14 17:05:15\n",
      "41768 2019-11-14 17:05:15\n",
      "41769 2019-11-14 17:05:15\n",
      "41770 2019-11-14 17:05:15\n",
      "41771 2019-11-14 17:05:15\n",
      "41772 2019-11-14 17:05:15\n",
      "41773 2019-11-14 17:05:15\n",
      "41774 2019-11-14 17:05:15\n",
      "41775 2019-11-14 17:05:15\n",
      "41776 2019-11-14 17:05:15\n",
      "41777 2019-11-14 17:05:15\n",
      "41778 2019-11-14 17:05:16\n",
      "41779 2019-11-14 17:05:16\n",
      "41780 2019-11-14 17:05:16\n",
      "41781 2019-11-14 17:05:16\n",
      "41782 2019-11-14 17:05:16\n",
      "41783 2019-11-14 17:05:16\n",
      "41784 2019-11-14 17:05:16\n",
      "41785 2019-11-14 17:05:16\n",
      "41786 2019-11-14 17:05:16\n",
      "41787 2019-11-14 17:05:16\n",
      "41788 2019-11-14 17:05:16\n",
      "41789 2019-11-14 17:05:16\n",
      "41790 2019-11-14 17:05:16\n",
      "41791 2019-11-14 17:05:16\n",
      "41792 2019-11-14 17:05:16\n",
      "41793 2019-11-14 17:05:16\n",
      "41794 2019-11-14 17:05:16\n",
      "41795 2019-11-14 17:05:16\n",
      "41796 2019-11-14 17:05:16\n",
      "41797 2019-11-14 17:05:17\n",
      "41798 2019-11-14 17:05:17\n",
      "41799 2019-11-14 17:05:17\n",
      "41800 2019-11-14 17:05:17\n",
      "41801 2019-11-14 17:05:17\n",
      "41802 2019-11-14 17:05:17\n",
      "41803 2019-11-14 17:05:17\n",
      "41804 2019-11-14 17:05:17\n",
      "41805 2019-11-14 17:05:17\n",
      "41806 2019-11-14 17:05:17\n",
      "41807 2019-11-14 17:05:17\n",
      "41808 2019-11-14 17:05:17\n",
      "41809 2019-11-14 17:05:17\n",
      "41810 2019-11-14 17:05:17\n",
      "41811 2019-11-14 17:05:17\n",
      "41812 2019-11-14 17:05:17\n",
      "41813 2019-11-14 17:05:17\n",
      "41814 2019-11-14 17:05:17\n",
      "41815 2019-11-14 17:05:17\n",
      "41816 2019-11-14 17:05:18\n",
      "41817 2019-11-14 17:05:18\n",
      "41818 2019-11-14 17:05:18\n",
      "41819 2019-11-14 17:05:18\n",
      "41820 2019-11-14 17:05:18\n",
      "41821 2019-11-14 17:05:18\n",
      "41822 2019-11-14 17:05:18\n",
      "41823 2019-11-14 17:05:18\n",
      "41824 2019-11-14 17:05:18\n",
      "41825 2019-11-14 17:05:18\n",
      "41826 2019-11-14 17:05:18\n",
      "41827 2019-11-14 17:05:18\n",
      "41828 2019-11-14 17:05:18\n",
      "41829 2019-11-14 17:05:18\n",
      "41830 2019-11-14 17:05:18\n",
      "41831 2019-11-14 17:05:18\n",
      "41832 2019-11-14 17:05:18\n",
      "41833 2019-11-14 17:05:18\n",
      "41834 2019-11-14 17:05:18\n",
      "41835 2019-11-14 17:05:19\n",
      "41836 2019-11-14 17:05:19\n",
      "41837 2019-11-14 17:05:19\n",
      "41838 2019-11-14 17:05:19\n",
      "41839 2019-11-14 17:05:19\n",
      "41840 2019-11-14 17:05:19\n",
      "41841 2019-11-14 17:05:19\n",
      "41842 2019-11-14 17:05:19\n",
      "41843 2019-11-14 17:05:19\n",
      "41844 2019-11-14 17:05:19\n",
      "41845 2019-11-14 17:05:19\n",
      "41846 2019-11-14 17:05:19\n",
      "41847 2019-11-14 17:05:19\n",
      "41848 2019-11-14 17:05:19\n",
      "41849 2019-11-14 17:05:19\n",
      "41850 2019-11-14 17:05:19\n",
      "41851 2019-11-14 17:05:19\n",
      "41852 2019-11-14 17:05:19\n",
      "41853 2019-11-14 17:05:19\n",
      "41854 2019-11-14 17:05:20\n",
      "41855 2019-11-14 17:05:20\n",
      "41856 2019-11-14 17:05:20\n",
      "41857 2019-11-14 17:05:20\n",
      "41858 2019-11-14 17:05:20\n",
      "41859 2019-11-14 17:05:20\n",
      "41860 2019-11-14 17:05:20\n",
      "41861 2019-11-14 17:05:20\n",
      "41862 2019-11-14 17:05:20\n",
      "41863 2019-11-14 17:05:20\n",
      "41864 2019-11-14 17:05:20\n",
      "41865 2019-11-14 17:05:20\n",
      "41866 2019-11-14 17:05:20\n",
      "41867 2019-11-14 17:05:20\n",
      "41868 2019-11-14 17:05:20\n",
      "41869 2019-11-14 17:05:20\n",
      "41870 2019-11-14 17:05:20\n",
      "41871 2019-11-14 17:05:20\n",
      "41872 2019-11-14 17:05:20\n",
      "41873 2019-11-14 17:05:21\n",
      "41874 2019-11-14 17:05:21\n",
      "41875 2019-11-14 17:05:21\n",
      "41876 2019-11-14 17:05:21\n",
      "41877 2019-11-14 17:05:21\n",
      "41878 2019-11-14 17:05:21\n",
      "41879 2019-11-14 17:05:21\n",
      "41880 2019-11-14 17:05:21\n",
      "41881 2019-11-14 17:05:21\n",
      "41882 2019-11-14 17:05:21\n",
      "41883 2019-11-14 17:05:21\n",
      "41884 2019-11-14 17:05:21\n",
      "41885 2019-11-14 17:05:21\n",
      "41886 2019-11-14 17:05:21\n",
      "41887 2019-11-14 17:05:21\n",
      "41888 2019-11-14 17:05:21\n",
      "41889 2019-11-14 17:05:21\n",
      "41890 2019-11-14 17:05:21\n",
      "41891 2019-11-14 17:05:21\n",
      "41892 2019-11-14 17:05:22\n",
      "41893 2019-11-14 17:05:22\n",
      "41894 2019-11-14 17:05:22\n",
      "41895 2019-11-14 17:05:22\n",
      "41896 2019-11-14 17:05:22\n",
      "41897 2019-11-14 17:05:22\n",
      "41898 2019-11-14 17:05:22\n",
      "41899 2019-11-14 17:05:22\n",
      "41900 2019-11-14 17:05:22\n",
      "41901 2019-11-14 17:05:22\n",
      "41902 2019-11-14 17:05:22\n",
      "41903 2019-11-14 17:05:22\n",
      "41904 2019-11-14 17:05:22\n",
      "41905 2019-11-14 17:05:22\n",
      "41906 2019-11-14 17:05:22\n",
      "41907 2019-11-14 17:05:22\n",
      "41908 2019-11-14 17:05:22\n",
      "41909 2019-11-14 17:05:22\n",
      "41910 2019-11-14 17:05:22\n",
      "41911 2019-11-14 17:05:23\n",
      "41912 2019-11-14 17:05:23\n",
      "41913 2019-11-14 17:05:23\n",
      "41914 2019-11-14 17:05:23\n",
      "41915 2019-11-14 17:05:23\n",
      "41916 2019-11-14 17:05:23\n",
      "41917 2019-11-14 17:05:23\n",
      "41918 2019-11-14 17:05:23\n",
      "41919 2019-11-14 17:05:23\n",
      "41920 2019-11-14 17:05:23\n",
      "41921 2019-11-14 17:05:23\n",
      "41922 2019-11-14 17:05:23\n",
      "41923 2019-11-14 17:05:23\n",
      "41924 2019-11-14 17:05:23\n",
      "41925 2019-11-14 17:05:23\n",
      "41926 2019-11-14 17:05:23\n",
      "41927 2019-11-14 17:05:23\n",
      "41928 2019-11-14 17:05:23\n",
      "41929 2019-11-14 17:05:23\n",
      "41930 2019-11-14 17:05:24\n",
      "41931 2019-11-14 17:05:24\n",
      "41932 2019-11-14 17:05:24\n",
      "41933 2019-11-14 17:05:24\n",
      "41934 2019-11-14 17:05:24\n",
      "41935 2019-11-14 17:05:24\n",
      "41936 2019-11-14 17:05:24\n",
      "41937 2019-11-14 17:05:24\n",
      "41938 2019-11-14 17:05:24\n",
      "41939 2019-11-14 17:05:24\n",
      "41940 2019-11-14 17:05:24\n",
      "41941 2019-11-14 17:05:24\n",
      "41942 2019-11-14 17:05:24\n",
      "41943 2019-11-14 17:05:24\n",
      "41944 2019-11-14 17:05:24\n",
      "41945 2019-11-14 17:05:24\n",
      "41946 2019-11-14 17:05:24\n",
      "41947 2019-11-14 17:05:24\n",
      "41948 2019-11-14 17:05:24\n",
      "41949 2019-11-14 17:05:25\n",
      "41950 2019-11-14 17:05:25\n",
      "41951 2019-11-14 17:05:25\n",
      "41952 2019-11-14 17:05:25\n",
      "41953 2019-11-14 17:05:25\n",
      "41954 2019-11-14 17:05:25\n",
      "41955 2019-11-14 17:05:25\n",
      "41956 2019-11-14 17:05:25\n",
      "41957 2019-11-14 17:05:25\n",
      "41958 2019-11-14 17:05:25\n",
      "41959 2019-11-14 17:05:25\n",
      "41960 2019-11-14 17:05:25\n",
      "41961 2019-11-14 17:05:25\n",
      "41962 2019-11-14 17:05:25\n",
      "41963 2019-11-14 17:05:25\n",
      "41964 2019-11-14 17:05:25\n",
      "41965 2019-11-14 17:05:25\n",
      "41966 2019-11-14 17:05:25\n",
      "41967 2019-11-14 17:05:25\n",
      "41968 2019-11-14 17:05:26\n",
      "41969 2019-11-14 17:05:26\n",
      "41970 2019-11-14 17:05:26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41971 2019-11-14 17:05:26\n",
      "41972 2019-11-14 17:05:26\n",
      "41973 2019-11-14 17:05:26\n",
      "41974 2019-11-14 17:05:26\n",
      "41975 2019-11-14 17:05:26\n",
      "41976 2019-11-14 17:05:26\n",
      "41977 2019-11-14 17:05:26\n",
      "41978 2019-11-14 17:05:26\n",
      "41979 2019-11-14 17:05:26\n",
      "41980 2019-11-14 17:05:26\n",
      "41981 2019-11-14 17:05:26\n",
      "41982 2019-11-14 17:05:26\n",
      "41983 2019-11-14 17:05:26\n",
      "41984 2019-11-14 17:05:26\n",
      "41985 2019-11-14 17:05:26\n",
      "41986 2019-11-14 17:05:26\n",
      "41987 2019-11-14 17:05:26\n",
      "41988 2019-11-14 17:05:27\n",
      "41989 2019-11-14 17:05:27\n",
      "41990 2019-11-14 17:05:27\n",
      "41991 2019-11-14 17:05:27\n",
      "41992 2019-11-14 17:05:27\n",
      "41993 2019-11-14 17:05:27\n",
      "41994 2019-11-14 17:05:27\n",
      "41995 2019-11-14 17:05:27\n",
      "41996 2019-11-14 17:05:27\n",
      "41997 2019-11-14 17:05:27\n",
      "41998 2019-11-14 17:05:27\n",
      "41999 2019-11-14 17:05:27\n",
      "42000 2019-11-14 17:05:27\n",
      "42001 2019-11-14 17:05:27\n",
      "42002 2019-11-14 17:05:27\n",
      "42003 2019-11-14 17:05:27\n",
      "42004 2019-11-14 17:05:27\n",
      "42005 2019-11-14 17:05:27\n",
      "42006 2019-11-14 17:05:27\n",
      "42007 2019-11-14 17:05:28\n",
      "42008 2019-11-14 17:05:28\n",
      "42009 2019-11-14 17:05:28\n",
      "42010 2019-11-14 17:05:28\n",
      "42011 2019-11-14 17:05:28\n",
      "42012 2019-11-14 17:05:28\n",
      "42013 2019-11-14 17:05:28\n",
      "42014 2019-11-14 17:05:28\n",
      "42015 2019-11-14 17:05:28\n",
      "42016 2019-11-14 17:05:28\n",
      "42017 2019-11-14 17:05:28\n",
      "42018 2019-11-14 17:05:28\n",
      "42019 2019-11-14 17:05:28\n",
      "42020 2019-11-14 17:05:28\n",
      "42021 2019-11-14 17:05:28\n",
      "42022 2019-11-14 17:05:28\n",
      "42023 2019-11-14 17:05:28\n",
      "42024 2019-11-14 17:05:28\n",
      "42025 2019-11-14 17:05:28\n",
      "42026 2019-11-14 17:05:29\n",
      "42027 2019-11-14 17:05:29\n",
      "42028 2019-11-14 17:05:29\n",
      "42029 2019-11-14 17:05:29\n",
      "42030 2019-11-14 17:05:29\n",
      "42031 2019-11-14 17:05:29\n",
      "42032 2019-11-14 17:05:29\n",
      "42033 2019-11-14 17:05:29\n",
      "42034 2019-11-14 17:05:29\n",
      "42035 2019-11-14 17:05:29\n",
      "42036 2019-11-14 17:05:29\n",
      "42037 2019-11-14 17:05:29\n",
      "42038 2019-11-14 17:05:29\n",
      "42039 2019-11-14 17:05:29\n",
      "42040 2019-11-14 17:05:29\n",
      "42041 2019-11-14 17:05:29\n",
      "42042 2019-11-14 17:05:29\n",
      "42043 2019-11-14 17:05:29\n",
      "42044 2019-11-14 17:05:29\n",
      "42045 2019-11-14 17:05:30\n",
      "42046 2019-11-14 17:05:30\n",
      "42047 2019-11-14 17:05:30\n",
      "42048 2019-11-14 17:05:30\n",
      "42049 2019-11-14 17:05:30\n",
      "42050 2019-11-14 17:05:30\n",
      "42051 2019-11-14 17:05:30\n",
      "42052 2019-11-14 17:05:30\n",
      "42053 2019-11-14 17:05:30\n",
      "42054 2019-11-14 17:05:30\n",
      "42055 2019-11-14 17:05:30\n",
      "42056 2019-11-14 17:05:30\n",
      "42057 2019-11-14 17:05:30\n",
      "42058 2019-11-14 17:05:30\n",
      "42059 2019-11-14 17:05:30\n",
      "42060 2019-11-14 17:05:30\n",
      "42061 2019-11-14 17:05:30\n",
      "42062 2019-11-14 17:05:30\n",
      "42063 2019-11-14 17:05:30\n",
      "42064 2019-11-14 17:05:31\n",
      "42065 2019-11-14 17:05:31\n",
      "42066 2019-11-14 17:05:31\n",
      "42067 2019-11-14 17:05:31\n",
      "42068 2019-11-14 17:05:31\n",
      "42069 2019-11-14 17:05:31\n",
      "42070 2019-11-14 17:05:31\n",
      "42071 2019-11-14 17:05:31\n",
      "42072 2019-11-14 17:05:31\n",
      "42073 2019-11-14 17:05:31\n",
      "42074 2019-11-14 17:05:31\n",
      "42075 2019-11-14 17:05:31\n",
      "42076 2019-11-14 17:05:31\n",
      "42077 2019-11-14 17:05:31\n",
      "42078 2019-11-14 17:05:31\n",
      "42079 2019-11-14 17:05:31\n",
      "42080 2019-11-14 17:05:31\n",
      "42081 2019-11-14 17:05:31\n",
      "42082 2019-11-14 17:05:31\n",
      "42083 2019-11-14 17:05:32\n",
      "42084 2019-11-14 17:05:32\n",
      "42085 2019-11-14 17:05:32\n",
      "42086 2019-11-14 17:05:32\n",
      "42087 2019-11-14 17:05:32\n",
      "42088 2019-11-14 17:05:32\n",
      "42089 2019-11-14 17:05:32\n",
      "42090 2019-11-14 17:05:32\n",
      "42091 2019-11-14 17:05:32\n",
      "42092 2019-11-14 17:05:32\n",
      "42093 2019-11-14 17:05:32\n",
      "42094 2019-11-14 17:05:32\n",
      "42095 2019-11-14 17:05:32\n",
      "42096 2019-11-14 17:05:32\n",
      "42097 2019-11-14 17:05:32\n",
      "42098 2019-11-14 17:05:32\n",
      "42099 2019-11-14 17:05:32\n",
      "42100 2019-11-14 17:05:32\n",
      "42101 2019-11-14 17:05:32\n",
      "42102 2019-11-14 17:05:33\n",
      "42103 2019-11-14 17:05:33\n",
      "42104 2019-11-14 17:05:33\n",
      "42105 2019-11-14 17:05:33\n",
      "42106 2019-11-14 17:05:33\n",
      "42107 2019-11-14 17:05:33\n",
      "42108 2019-11-14 17:05:33\n",
      "42109 2019-11-14 17:05:33\n",
      "42110 2019-11-14 17:05:33\n",
      "42111 2019-11-14 17:05:33\n",
      "42112 2019-11-14 17:05:33\n",
      "42113 2019-11-14 17:05:33\n",
      "42114 2019-11-14 17:05:33\n",
      "42115 2019-11-14 17:05:33\n",
      "42116 2019-11-14 17:05:33\n",
      "42117 2019-11-14 17:05:33\n",
      "42118 2019-11-14 17:05:33\n",
      "42119 2019-11-14 17:05:33\n",
      "42120 2019-11-14 17:05:33\n",
      "42121 2019-11-14 17:05:34\n",
      "42122 2019-11-14 17:05:34\n",
      "42123 2019-11-14 17:05:34\n",
      "42124 2019-11-14 17:05:34\n",
      "42125 2019-11-14 17:05:34\n",
      "42126 2019-11-14 17:05:34\n",
      "42127 2019-11-14 17:05:34\n",
      "42128 2019-11-14 17:05:34\n",
      "42129 2019-11-14 17:05:34\n",
      "42130 2019-11-14 17:05:34\n",
      "42131 2019-11-14 17:05:34\n",
      "42132 2019-11-14 17:05:34\n",
      "42133 2019-11-14 17:05:34\n",
      "42134 2019-11-14 17:05:34\n",
      "42135 2019-11-14 17:05:34\n",
      "42136 2019-11-14 17:05:34\n",
      "42137 2019-11-14 17:05:34\n",
      "42138 2019-11-14 17:05:34\n",
      "42139 2019-11-14 17:05:34\n",
      "42140 2019-11-14 17:05:35\n",
      "42141 2019-11-14 17:05:35\n",
      "42142 2019-11-14 17:05:35\n",
      "42143 2019-11-14 17:05:35\n",
      "42144 2019-11-14 17:05:35\n",
      "42145 2019-11-14 17:05:35\n",
      "42146 2019-11-14 17:05:35\n",
      "42147 2019-11-14 17:05:35\n",
      "42148 2019-11-14 17:05:35\n",
      "42149 2019-11-14 17:05:35\n",
      "42150 2019-11-14 17:05:35\n",
      "42151 2019-11-14 17:05:35\n",
      "42152 2019-11-14 17:05:35\n",
      "42153 2019-11-14 17:05:35\n",
      "42154 2019-11-14 17:05:35\n",
      "42155 2019-11-14 17:05:35\n",
      "42156 2019-11-14 17:05:35\n",
      "42157 2019-11-14 17:05:35\n",
      "42158 2019-11-14 17:05:35\n",
      "42159 2019-11-14 17:05:36\n",
      "42160 2019-11-14 17:05:36\n",
      "42161 2019-11-14 17:05:36\n",
      "42162 2019-11-14 17:05:36\n",
      "42163 2019-11-14 17:05:36\n",
      "42164 2019-11-14 17:05:36\n",
      "42165 2019-11-14 17:05:36\n",
      "42166 2019-11-14 17:05:36\n",
      "42167 2019-11-14 17:05:36\n",
      "42168 2019-11-14 17:05:36\n",
      "42169 2019-11-14 17:05:36\n",
      "42170 2019-11-14 17:05:36\n",
      "42171 2019-11-14 17:05:36\n",
      "42172 2019-11-14 17:05:36\n",
      "42173 2019-11-14 17:05:36\n",
      "42174 2019-11-14 17:05:36\n",
      "42175 2019-11-14 17:05:36\n",
      "42176 2019-11-14 17:05:36\n",
      "42177 2019-11-14 17:05:36\n",
      "42178 2019-11-14 17:05:37\n",
      "42179 2019-11-14 17:05:37\n",
      "42180 2019-11-14 17:05:37\n",
      "42181 2019-11-14 17:05:37\n",
      "42182 2019-11-14 17:05:37\n",
      "42183 2019-11-14 17:05:37\n",
      "42184 2019-11-14 17:05:37\n",
      "42185 2019-11-14 17:05:37\n",
      "42186 2019-11-14 17:05:37\n",
      "42187 2019-11-14 17:05:37\n",
      "42188 2019-11-14 17:05:37\n",
      "42189 2019-11-14 17:05:37\n",
      "42190 2019-11-14 17:05:37\n",
      "42191 2019-11-14 17:05:37\n",
      "42192 2019-11-14 17:05:37\n",
      "42193 2019-11-14 17:05:37\n",
      "42194 2019-11-14 17:05:37\n",
      "42195 2019-11-14 17:05:37\n",
      "42196 2019-11-14 17:05:37\n",
      "42197 2019-11-14 17:05:38\n",
      "42198 2019-11-14 17:05:38\n",
      "42199 2019-11-14 17:05:38\n",
      "42200 2019-11-14 17:05:38\n",
      "42201 2019-11-14 17:05:38\n",
      "42202 2019-11-14 17:05:38\n",
      "42203 2019-11-14 17:05:38\n",
      "42204 2019-11-14 17:05:38\n",
      "42205 2019-11-14 17:05:38\n",
      "42206 2019-11-14 17:05:38\n",
      "42207 2019-11-14 17:05:38\n",
      "42208 2019-11-14 17:05:38\n",
      "42209 2019-11-14 17:05:38\n",
      "42210 2019-11-14 17:05:38\n",
      "42211 2019-11-14 17:05:38\n",
      "42212 2019-11-14 17:05:38\n",
      "42213 2019-11-14 17:05:38\n",
      "42214 2019-11-14 17:05:38\n",
      "42215 2019-11-14 17:05:38\n",
      "42216 2019-11-14 17:05:39\n",
      "42217 2019-11-14 17:05:39\n",
      "42218 2019-11-14 17:05:39\n",
      "42219 2019-11-14 17:05:39\n",
      "42220 2019-11-14 17:05:39\n",
      "42221 2019-11-14 17:05:39\n",
      "42222 2019-11-14 17:05:39\n",
      "42223 2019-11-14 17:05:39\n",
      "42224 2019-11-14 17:05:39\n",
      "42225 2019-11-14 17:05:39\n",
      "42226 2019-11-14 17:05:39\n",
      "42227 2019-11-14 17:05:39\n",
      "42228 2019-11-14 17:05:39\n",
      "42229 2019-11-14 17:05:39\n",
      "42230 2019-11-14 17:05:39\n",
      "42231 2019-11-14 17:05:39\n",
      "42232 2019-11-14 17:05:39\n",
      "42233 2019-11-14 17:05:39\n",
      "42234 2019-11-14 17:05:39\n",
      "[Timestamp('2019-11-14 17:21:50'), Timestamp('2019-11-14 17:21:51')]\n",
      "[16, 15]\n",
      "[16, 15]\n",
      "51772 2019-11-14 17:21:50\n",
      "51773 2019-11-14 17:21:50\n",
      "51774 2019-11-14 17:21:50\n",
      "51775 2019-11-14 17:21:50\n",
      "51776 2019-11-14 17:21:50\n",
      "51777 2019-11-14 17:21:50\n",
      "51778 2019-11-14 17:21:50\n",
      "51779 2019-11-14 17:21:50\n",
      "51780 2019-11-14 17:21:50\n",
      "51781 2019-11-14 17:21:50\n",
      "51782 2019-11-14 17:21:50\n",
      "51783 2019-11-14 17:21:50\n",
      "51784 2019-11-14 17:21:50\n",
      "51785 2019-11-14 17:21:50\n",
      "51786 2019-11-14 17:21:50\n",
      "51787 2019-11-14 17:21:50\n",
      "51788 2019-11-14 17:21:51\n",
      "51789 2019-11-14 17:21:51\n",
      "51790 2019-11-14 17:21:51\n",
      "51791 2019-11-14 17:21:51\n",
      "51792 2019-11-14 17:21:51\n",
      "51793 2019-11-14 17:21:51\n",
      "51794 2019-11-14 17:21:51\n",
      "51795 2019-11-14 17:21:51\n",
      "51796 2019-11-14 17:21:51\n",
      "51797 2019-11-14 17:21:51\n",
      "51798 2019-11-14 17:21:51\n",
      "51799 2019-11-14 17:21:51\n",
      "51800 2019-11-14 17:21:51\n",
      "51801 2019-11-14 17:21:51\n",
      "51802 2019-11-14 17:21:51\n",
      "[Timestamp('2019-11-14 17:31:29'), Timestamp('2019-11-14 17:31:30')]\n",
      "[16, 16]\n",
      "57443 2019-11-14 17:31:29\n",
      "57444 2019-11-14 17:31:29\n",
      "57445 2019-11-14 17:31:29\n",
      "57446 2019-11-14 17:31:29\n",
      "57447 2019-11-14 17:31:29\n",
      "57448 2019-11-14 17:31:29\n",
      "57449 2019-11-14 17:31:29\n",
      "57450 2019-11-14 17:31:29\n",
      "57451 2019-11-14 17:31:29\n",
      "57452 2019-11-14 17:31:29\n",
      "57453 2019-11-14 17:31:29\n",
      "57454 2019-11-14 17:31:29\n",
      "57455 2019-11-14 17:31:29\n",
      "57456 2019-11-14 17:31:29\n",
      "57457 2019-11-14 17:31:29\n",
      "57458 2019-11-14 17:31:29\n",
      "57459 2019-11-14 17:31:30\n",
      "57460 2019-11-14 17:31:30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57461 2019-11-14 17:31:30\n",
      "57462 2019-11-14 17:31:30\n",
      "57463 2019-11-14 17:31:30\n",
      "57464 2019-11-14 17:31:30\n",
      "57465 2019-11-14 17:31:30\n",
      "57466 2019-11-14 17:31:30\n",
      "57467 2019-11-14 17:31:30\n",
      "57468 2019-11-14 17:31:30\n",
      "57469 2019-11-14 17:31:30\n",
      "57470 2019-11-14 17:31:30\n",
      "57471 2019-11-14 17:31:30\n",
      "57472 2019-11-14 17:31:30\n",
      "57473 2019-11-14 17:31:30\n",
      "57474 2019-11-14 17:31:30\n",
      "[Timestamp('2019-11-14 18:19:37'), Timestamp('2019-11-14 18:19:38')]\n",
      "[17, 16]\n",
      "[17, 16]\n",
      "85687 2019-11-14 18:19:37\n",
      "85688 2019-11-14 18:19:37\n",
      "85689 2019-11-14 18:19:37\n",
      "85690 2019-11-14 18:19:37\n",
      "85691 2019-11-14 18:19:37\n",
      "85692 2019-11-14 18:19:37\n",
      "85693 2019-11-14 18:19:37\n",
      "85694 2019-11-14 18:19:37\n",
      "85695 2019-11-14 18:19:37\n",
      "85696 2019-11-14 18:19:37\n",
      "85697 2019-11-14 18:19:37\n",
      "85698 2019-11-14 18:19:37\n",
      "85699 2019-11-14 18:19:37\n",
      "85700 2019-11-14 18:19:37\n",
      "85701 2019-11-14 18:19:37\n",
      "85702 2019-11-14 18:19:37\n",
      "85703 2019-11-14 18:19:37\n",
      "85704 2019-11-14 18:19:38\n",
      "85705 2019-11-14 18:19:38\n",
      "85706 2019-11-14 18:19:38\n",
      "85707 2019-11-14 18:19:38\n",
      "85708 2019-11-14 18:19:38\n",
      "85709 2019-11-14 18:19:38\n",
      "85710 2019-11-14 18:19:38\n",
      "85711 2019-11-14 18:19:38\n",
      "85712 2019-11-14 18:19:38\n",
      "85713 2019-11-14 18:19:38\n",
      "85714 2019-11-14 18:19:38\n",
      "85715 2019-11-14 18:19:38\n",
      "85716 2019-11-14 18:19:38\n",
      "85717 2019-11-14 18:19:38\n",
      "85718 2019-11-14 18:19:38\n",
      "85719 2019-11-14 18:19:38\n",
      "[Timestamp('2019-11-14 18:26:05'), Timestamp('2019-11-14 18:26:06')]\n",
      "[18, 18]\n",
      "89514 2019-11-14 18:26:05\n",
      "89515 2019-11-14 18:26:05\n",
      "89516 2019-11-14 18:26:05\n",
      "89517 2019-11-14 18:26:05\n",
      "89518 2019-11-14 18:26:05\n",
      "89519 2019-11-14 18:26:05\n",
      "89520 2019-11-14 18:26:05\n",
      "89521 2019-11-14 18:26:05\n",
      "89522 2019-11-14 18:26:05\n",
      "89523 2019-11-14 18:26:05\n",
      "89524 2019-11-14 18:26:05\n",
      "89525 2019-11-14 18:26:05\n",
      "89526 2019-11-14 18:26:05\n",
      "89527 2019-11-14 18:26:05\n",
      "89528 2019-11-14 18:26:05\n",
      "89529 2019-11-14 18:26:05\n",
      "89530 2019-11-14 18:26:05\n",
      "89531 2019-11-14 18:26:05\n",
      "89532 2019-11-14 18:26:06\n",
      "89533 2019-11-14 18:26:06\n",
      "89534 2019-11-14 18:26:06\n",
      "89535 2019-11-14 18:26:06\n",
      "89536 2019-11-14 18:26:06\n",
      "89537 2019-11-14 18:26:06\n",
      "89538 2019-11-14 18:26:06\n",
      "89539 2019-11-14 18:26:06\n",
      "89540 2019-11-14 18:26:06\n",
      "89541 2019-11-14 18:26:06\n",
      "89542 2019-11-14 18:26:06\n",
      "89543 2019-11-14 18:26:06\n",
      "89544 2019-11-14 18:26:06\n",
      "89545 2019-11-14 18:26:06\n",
      "89546 2019-11-14 18:26:06\n",
      "89547 2019-11-14 18:26:06\n",
      "89548 2019-11-14 18:26:06\n",
      "89549 2019-11-14 18:26:06\n",
      "[Timestamp('2019-11-14 18:35:51'), Timestamp('2019-11-14 18:35:52')]\n",
      "[16, 15]\n",
      "[16, 15]\n",
      "95300 2019-11-14 18:35:51\n",
      "95301 2019-11-14 18:35:51\n",
      "95302 2019-11-14 18:35:51\n",
      "95303 2019-11-14 18:35:51\n",
      "95304 2019-11-14 18:35:51\n",
      "95305 2019-11-14 18:35:51\n",
      "95306 2019-11-14 18:35:51\n",
      "95307 2019-11-14 18:35:51\n",
      "95308 2019-11-14 18:35:51\n",
      "95309 2019-11-14 18:35:51\n",
      "95310 2019-11-14 18:35:51\n",
      "95311 2019-11-14 18:35:51\n",
      "95312 2019-11-14 18:35:51\n",
      "95313 2019-11-14 18:35:51\n",
      "95314 2019-11-14 18:35:51\n",
      "95315 2019-11-14 18:35:51\n",
      "95316 2019-11-14 18:35:52\n",
      "95317 2019-11-14 18:35:52\n",
      "95318 2019-11-14 18:35:52\n",
      "95319 2019-11-14 18:35:52\n",
      "95320 2019-11-14 18:35:52\n",
      "95321 2019-11-14 18:35:52\n",
      "95322 2019-11-14 18:35:52\n",
      "95323 2019-11-14 18:35:52\n",
      "95324 2019-11-14 18:35:52\n",
      "95325 2019-11-14 18:35:52\n",
      "95326 2019-11-14 18:35:52\n",
      "95327 2019-11-14 18:35:52\n",
      "95328 2019-11-14 18:35:52\n",
      "95329 2019-11-14 18:35:52\n",
      "95330 2019-11-14 18:35:52\n",
      "Int64Index([], dtype='int64')\n",
      "---Processing ATP Dataframe---\n",
      "Number of null value entry =  0\n",
      "---Processing COM Dataframe---\n",
      "Number of null value entry =  0\n",
      "---Processing TDMS Dataframe---\n",
      "Number of null value entry =  0\n",
      "---Merging Dataframes---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 124322/124322 [00:53<00:00, 2345.50it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 124322/124322 [00:07<00:00, 16773.08it/s]\n",
      " 60%|████████████████████████████████████████████                             | 74988/124322 [00:43<00:43, 1140.03it/s]"
     ]
    }
   ],
   "source": [
    "starttime = timeit.default_timer()\n",
    "\n",
    "def test_folder_path (input_folder_path, start_time, end_time):\n",
    "    # Retrieve Train number\n",
    "    train_number = re.findall(r'%s(\\d+)' %'Train ', input_folder_path)[0]\n",
    "    \n",
    "    # Determine time difference between start and end time\n",
    "    datetime_format = '%Y/%m/%d %H:%M:%S'\n",
    "    start_time = datetime.datetime.strptime(start_time, datetime_format)\n",
    "    end_time = datetime.datetime.strptime(end_time, datetime_format)\n",
    "    diff = end_time - start_time\n",
    "    hours = diff.total_seconds() /3600\n",
    "\n",
    "    start_time_new = start_time.strftime('%m/%d/%Y %H:%M:%S')\n",
    "    end_time_new = end_time.strftime('%m/%d/%Y %H:%M:%S')\n",
    "    result_list = []\n",
    "    for car in os.listdir(input_folder_path):\n",
    "        car_number = re.findall(r'%s(\\d+)' %'Car ', car)[0]\n",
    "        print(\"---Processing Train \" + train_number + \" Car \" + car_number + \"---\")\n",
    "        date = os.listdir(os.path.join(input_folder_path, car))[0]\n",
    "        flag = True\n",
    "        back_date_flag = False\n",
    "        # Test for correct folder structure\n",
    "        try:\n",
    "            datetime.datetime.strptime(date, \"%y%m%d\")\n",
    "            #print(\"Correct date string format.\")\n",
    "        except ValueError:\n",
    "            print(\"Inorrect date string format. Example 200116 %y%m%d. This might result in process error.\")\n",
    "            flag = False    \n",
    "        \n",
    "        # OMAP Processing\n",
    "        \n",
    "        # Look for ATO, ATP, COM and TDMS folder\n",
    "        if not os.path.exists(os.path.join(input_folder_path, car, date, 'OMAP_ATO')):\n",
    "            print(\"ATO folder does not exist. Incorrect folder structure\")\n",
    "            flag = False\n",
    "        else:\n",
    "            ATO_file_list = os.listdir(os.path.join(input_folder_path, car, date, 'OMAP_ATO'))       \n",
    "            sorted(ATO_file_list)\n",
    "            # Find dataframe/append for the given start and end time\n",
    "            ATO_dataframe = pd.DataFrame()\n",
    "            ATP_dataframe = pd.DataFrame()\n",
    "            COM_dataframe = pd.DataFrame()\n",
    "            TDMS_dataframe = pd.DataFrame()\n",
    "            for index in range (0, len(ATO_file_list)):\n",
    "            #for file in ATO_file_list:\n",
    "                file = ATO_file_list[index]\n",
    "                process_flag = True\n",
    "                # For each ATO file, finding corresponding ATP, COM and TDMS file\n",
    "                ATP_file_list = os.listdir(os.path.join(input_folder_path, car, date, 'OMAP_ATP'))\n",
    "                if not file.replace('ATO','ATP') in ATP_file_list:\n",
    "                    print(file + \" does not have a corresponding ATP file. Hence will not be processed. \")\n",
    "                    process_flag = False\n",
    "                \n",
    "                COM_file_list = os.listdir(os.path.join(input_folder_path, car, date, 'OMAP_COM'))\n",
    "                if not file.replace('ATO','COM') in COM_file_list:\n",
    "                    print(file + \" does not have a corresponding COM file. Hence will not be processed. \")\n",
    "                    process_flag = False\n",
    "                    \n",
    "                TDMS_file_list = os.listdir(os.path.join(input_folder_path, car, date, 'OMAP_TDMS'))\n",
    "                if not file.replace('ATO','TDMS') in TDMS_file_list:\n",
    "                    print(file + \" does not have a corresponding TDMS file. Hence will not be processed. \")\n",
    "                    process_flag = False               \n",
    "                \n",
    "                if process_flag:\n",
    "                    # Read ATO log file and find relevant rows based on start and end time)\n",
    "                    df_ATO = pd.read_csv(os.path.join(input_folder_path, car, date, 'OMAP_ATO', file), sep=\"\\t\")\n",
    "                    df_ATO['Date'] = pd.to_datetime(df_ATO['Date'], format='%m/%d/%Y %H:%M:%S')\n",
    "                    # Read ATP log file and find relevant rows based on start and end time)\n",
    "                    df_ATP = pd.read_csv(os.path.join(input_folder_path, car, date, 'OMAP_ATP', file.replace('ATO','ATP')), sep=\"\\t\")\n",
    "                    df_ATP['Date'] = pd.to_datetime(df_ATP['Date'], format='%m/%d/%Y %H:%M:%S')\n",
    "                    # Read COM log file and find relevant rows based on start and end time)\n",
    "                    df_COM = pd.read_csv(os.path.join(input_folder_path, car, date, 'OMAP_COM', file.replace('ATO','COM')), sep=\"\\t\")\n",
    "                    df_COM['Date'] = pd.to_datetime(df_COM['Date'], format='%m/%d/%Y %H:%M:%S')\n",
    "                    # Read TDMS log file and find relevant rows based on start and end time)\n",
    "                    df_TDMS = pd.read_csv(os.path.join(input_folder_path, car, date, 'OMAP_TDMS', file.replace('ATO','TDMS')), sep=\"\\t\")\n",
    "                    df_TDMS['Date'] = pd.to_datetime(df_TDMS['Date'], format='%m/%d/%Y %H:%M:%S')\n",
    "                                        \n",
    "                    if hours == 1: # Process only 1 log file\n",
    "                        # Compare between cuurent index and next index (to be removed as it doesnt max sense to do it this way)\n",
    "                        # For the sake of being the same as ATLAS\n",
    "                        if (index+1 < len(ATO_file_list)):\n",
    "                            df_ATO_next = pd.read_csv(os.path.join(input_folder_path, car, date, 'OMAP_ATO', ATO_file_list[index+1]), sep=\"\\t\")\n",
    "                            df_ATO_next['Date'] = pd.to_datetime(df_ATO_next['Date'], format='%m/%d/%Y %H:%M:%S')\n",
    "                            next_length = len((df_ATO_next.loc[(df_ATO_next['Date'] == start_time_new)]))\n",
    "                            current_length = len((df_ATO.loc[(df_ATO['Date'] == start_time_new)]))\n",
    "                            \n",
    "                            if next_length > current_length:\n",
    "                                # Read logs from next index\n",
    "                                df_ATP_next = pd.read_csv(os.path.join(input_folder_path, car, date, 'OMAP_ATP', ATO_file_list[index+1].replace('ATO','ATP')), sep=\"\\t\")\n",
    "                                df_ATP_next['Date'] = pd.to_datetime(df_ATP_next['Date'], format='%m/%d/%Y %H:%M:%S')\n",
    "                                df_COM_next= pd.read_csv(os.path.join(input_folder_path, car, date, 'OMAP_COM', ATO_file_list[index+1].replace('ATO','COM')), sep=\"\\t\")\n",
    "                                df_COM_next['Date'] = pd.to_datetime(df_COM_next['Date'], format='%m/%d/%Y %H:%M:%S')\n",
    "                                df_TDMS_next = pd.read_csv(os.path.join(input_folder_path, car, date, 'OMAP_TDMS', ATO_file_list[index+1].replace('ATO','TDMS')), sep=\"\\t\")\n",
    "                                df_TDMS_next['Date'] = pd.to_datetime(df_TDMS_next['Date'], format='%m/%d/%Y %H:%M:%S') \n",
    "                                \n",
    "                                ATO_dataframe = ATO_dataframe.append(df_ATO_next.loc[(df_ATO_next['Date'] >= start_time_new) & (df_ATO_next['Date'] <= end_time_new)])\n",
    "                                ATP_dataframe = ATP_dataframe.append(df_ATP_next.loc[(df_ATP_next['Date'] >= start_time_new) & (df_ATP_next['Date'] <= end_time_new)])\n",
    "                                COM_dataframe = COM_dataframe.append(df_COM_next.loc[(df_COM_next['Date'] >= start_time_new) & (df_COM_next['Date'] <= end_time_new)])\n",
    "                                TDMS_dataframe = TDMS_dataframe.append(df_TDMS_next.loc[(df_TDMS_next['Date'] >= start_time_new) & (df_TDMS_next['Date'] <= end_time_new)])\n",
    "                                break\n",
    "                            else:\n",
    "                                # Read logs from current index\n",
    "                                ATO_dataframe = ATO_dataframe.append(df_ATO.loc[(df_ATO['Date'] >= start_time_new) & (df_ATO['Date'] <= end_time_new)])\n",
    "                                ATP_dataframe = ATP_dataframe.append(df_ATP.loc[(df_ATP['Date'] >= start_time_new) & (df_ATP['Date'] <= end_time_new)])\n",
    "                                COM_dataframe = COM_dataframe.append(df_COM.loc[(df_COM['Date'] >= start_time_new) & (df_COM['Date'] <= end_time_new)])\n",
    "                                TDMS_dataframe = TDMS_dataframe.append(df_TDMS.loc[(df_TDMS['Date'] >= start_time_new) & (df_TDMS['Date'] <= end_time_new)])\n",
    "                                break\n",
    "                        else:\n",
    "                            # Read logs from only index\n",
    "                            ATO_dataframe = ATO_dataframe.append(df_ATO.loc[(df_ATO['Date'] >= start_time_new) & (df_ATO['Date'] <= end_time_new)])\n",
    "                            ATP_dataframe = ATP_dataframe.append(df_ATP.loc[(df_ATP['Date'] >= start_time_new) & (df_ATP['Date'] <= end_time_new)])\n",
    "                            COM_dataframe = COM_dataframe.append(df_COM.loc[(df_COM['Date'] >= start_time_new) & (df_COM['Date'] <= end_time_new)])\n",
    "                            TDMS_dataframe = TDMS_dataframe.append(df_TDMS.loc[(df_TDMS['Date'] >= start_time_new) & (df_TDMS['Date'] <= end_time_new)])\n",
    "                            break\n",
    "                    else:\n",
    "                        # Need to fix for whole number hours (probably not because dont have to copy how atlas process which files)\n",
    "                        ATO_dataframe = ATO_dataframe.append(df_ATO.loc[(df_ATO['Date'] >= start_time_new) & (df_ATO['Date'] < end_time_new)])\n",
    "                        ATP_dataframe = ATP_dataframe.append(df_ATP.loc[(df_ATP['Date'] >= start_time_new) & (df_ATP['Date'] < end_time_new)])\n",
    "                        COM_dataframe = COM_dataframe.append(df_COM.loc[(df_COM['Date'] >= start_time_new) & (df_COM['Date'] < end_time_new)])\n",
    "                        TDMS_dataframe = TDMS_dataframe.append(df_TDMS.loc[(df_TDMS['Date'] >= start_time_new) & (df_TDMS['Date'] < end_time_new)])\n",
    "                        # Check if log is '00'_00_00 time and within time period\n",
    "                        if re.findall(r'%s(\\d+)' %'_', file)[0] == '00' and len(df_ATO.loc[(df_ATO['Date'] >= start_time_new) & (df_ATO['Date'] < end_time_new)])>0:\n",
    "                            back_date_flag = True\n",
    "                else:\n",
    "                    sys.exit(car + \" data folder is incorrect / missing corresponding log files. Cannot proceed with processing! Kindly check whether ATO, ATP COM and TDMS have their corresponding log files.\")\n",
    "            \n",
    "            \n",
    "            # Check if log is '00'_00_00 time and within time period\n",
    "            if re.findall(r'%s(\\d+)' %'_', file)[0] == '00' and len(ATO_dataframe.loc[(ATO_dataframe['Date'] >= start_time_new) & (ATO_dataframe['Date'] <= end_time_new)])>0:\n",
    "                back_date_flag = True\n",
    "                print(\"back_date_flag\")\n",
    "                        \n",
    "            ATO_dataframe = ATO_dataframe.reset_index(drop=True)\n",
    "            ATP_dataframe = ATP_dataframe.reset_index(drop=True)\n",
    "            COM_dataframe = COM_dataframe.reset_index(drop=True)\n",
    "            TDMS_dataframe = TDMS_dataframe.reset_index(drop=True)\n",
    "            \n",
    "            if(ATO_dataframe.shape[0] == 0):\n",
    "                sys.exit(\"No matching time window from start to end time! Kindly check if you have input the correct start and end time!\")\n",
    "            \n",
    "            #print(ATO_dataframe.tail(30))\n",
    "            # Process ATO, ATP, COM, TDMS individually\n",
    "            ATO_dataframe = omap.process_ato(ATO_dataframe, back_date_flag)\n",
    "            ATP_dataframe = omap.process_others(ATP_dataframe, 'ATP')\n",
    "            COM_dataframe = omap.process_others(COM_dataframe, 'COM')\n",
    "            TDMS_dataframe = omap.process_others(TDMS_dataframe, 'TDMS')\n",
    "\n",
    "            # Merge ATO-ATP, ATO-COM, ATO-TDMS\n",
    "            ATO_ATP_result = omap.merge_ato_n_others(ATO_dataframe, ATP_dataframe)\n",
    "            ATO_COM_result = omap.merge_ato_n_others(ATO_dataframe, COM_dataframe)\n",
    "            ATO_TDMS_result = omap.merge_ato_n_others(ATO_dataframe, TDMS_dataframe)\n",
    "\n",
    "            # Merge all results dataframes\n",
    "            df_result = omap.merge_all(ATO_ATP_result, ATO_COM_result, ATO_TDMS_result, train_number, car_number, start_time, end_time)\n",
    "            \n",
    "            # Append results for Unit Test\n",
    "            result_list.append([ATO_ATP_result, ATO_COM_result, ATO_TDMS_result, df_result])\n",
    "            \n",
    "    return result_list\n",
    "                          \n",
    "            \n",
    "# input_folder_path =  './T20 OMAP DATA/Train 20 CSV/'\n",
    "# #input_folder_path =  './test_folder/Train 19 CSV/'\n",
    "# #Machine Time of interest ('%Y/%m/%d %H:%M:%S')\n",
    "# start_time = '2020/01/16 05:00:00' \n",
    "# end_time = '2020/01/16 08:00:00'\n",
    "\n",
    "# Machine Time of interest ('%m/%d/%Y %H:%M:%S')\n",
    "# start_time = '01/16/2020 07:00:00' \n",
    "# end_time = '01/16/2020 08:00:00'\n",
    "#sample_output_filename = ['./OMAP_Train_20_Car_39_20200116_0600_to_20200116_0700.csv']\n",
    "#sample_output_filename = ['./OMAP_Train_20_Car_39_20200116_0700_to_20200116_0800.csv']\n",
    "#sample_output_filename = ['./OMAP_Train_20_Car_39_20200116_0500_to_20200116_0800.csv']\n",
    "\n",
    "input_folder_path = './Train 23 14NOV2019 Work Folder/raw logs/OMAP/Train 23 Copy'\n",
    "start_time = '2019/11/14 17:00:00' \n",
    "end_time = '2019/11/14 20:00:00'\n",
    "# sample_output_filename = ['./Train 23 14NOV2019 Work Folder/preprocessing_output/OMAP_Train_23_Car_45_20191114_1700_to_20191114_1800.csv',\n",
    "#                           './Train 23 14NOV2019 Work Folder/preprocessing_output/OMAP_Train_23_Car_46_20191114_1700_to_20191114_1800.csv']\n",
    "# sample_output_filename = ['./Train 23 14NOV2019 Work Folder/preprocessing_output/OMAP_Train_23_Car_46_20191114_1800_to_20191114_1900.csv']\n",
    "sample_output_filename = ['./Train 23 14NOV2019 Work Folder/preprocessing_output/OMAP_Train_23_Car_45_20191114_1700_to_20191114_2000.csv']\n",
    "\n",
    "\n",
    "# input_folder_path = './Train 10 26FEB2018 Work Folder/raw_logs/OMAP/Train 10'\n",
    "# start_time = '2018/02/26 01:00:00' \n",
    "# end_time = '2018/02/26 02:00:00' \n",
    "# sample_output_filename = ['./Train 10 26FEB2018 Work Folder/preprocessed_output/OMAP_Train_10_Car_19_20180226_0100_to_20180226_0200.csv']\n",
    "\n",
    "# input_folder_path = './Train 10 22FEB2018 Work Folder/raw_logs/OMAP/Train 10'\n",
    "# start_time = '2018/02/22 01:00:00' \n",
    "# end_time = '2018/02/22 02:00:00' \n",
    "# sample_output_filename = ['./Train 10 22FEB2018 Work Folder/preprocessed_output/OMAP_Train_10_Car_19_20180222_0100_to_20180222_0200.csv']\n",
    "\n",
    "\n",
    "result_list = test_folder_path(input_folder_path, start_time, end_time)\n",
    "print(\"Time Taken:\", timeit.default_timer() - starttime)\n",
    "\n",
    "for index in range (0, len(result_list)):\n",
    "    # Unit Test\n",
    "    omap.unit_test(sample_output_filename[index], 'ATP', result_list[index][0])\n",
    "    omap.unit_test(sample_output_filename[index], 'COM', result_list[index][1])\n",
    "    omap.unit_test(sample_output_filename[index], 'TDMS', result_list[index][2])\n",
    "    omap.unit_test_all(sample_output_filename[index], result_list[index][3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-26 16:32:25\n",
      "2020-10-26 16:26:04\n",
      "2018-02-26 00:00:00\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "ts = int(\"1603729945\")\n",
    "\n",
    "# if you encounter a \"year is out of range\" error the timestamp\n",
    "# may be in milliseconds, try `ts /= 1000` in that case\n",
    "print(datetime.utcfromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "ts = int(\"1603729564\")\n",
    "\n",
    "# if you encounter a \"year is out of range\" error the timestamp\n",
    "# may be in milliseconds, try `ts /= 1000` in that case\n",
    "print(datetime.utcfromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "ts = int(\"1519603200\")\n",
    "\n",
    "# if you encounter a \"year is out of range\" error the timestamp\n",
    "# may be in milliseconds, try `ts /= 1000` in that case\n",
    "print(datetime.utcfromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(r'%s(\\d+)' %'_', '200215_00_00_00_329_OMAP_ATO.txt')[0])\n",
    "if re.findall(r'%s(\\d+)' %'_', '200215_00_00_00_329_OMAP_ATO.txt')[0] == '00':\n",
    "    print('True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVR folder exist!\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32239 2020-10-26 15:58:14 1603727894000\n",
      "32240 2020-10-26 15:58:14 1603727894500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32248 2020-10-26 15:58:22 1603727902000\n",
      "32249 2020-10-26 15:58:22 1603727902333\n",
      "32250 2020-10-26 15:58:22 1603727902666\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32259 2020-10-26 15:58:31 1603727911000\n",
      "32260 2020-10-26 15:58:31 1603727911500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32283 2020-10-26 15:58:54 1603727934000\n",
      "32284 2020-10-26 15:58:54 1603727934333\n",
      "32285 2020-10-26 15:58:54 1603727934666\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32291 2020-10-26 15:58:56 1603727936000\n",
      "32290 2020-10-26 15:58:56 1603727936200\n",
      "32288 2020-10-26 15:58:56 1603727936400\n",
      "32287 2020-10-26 15:58:56 1603727936600\n",
      "32289 2020-10-26 15:58:56 1603727936800\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32292 2020-10-26 15:58:57 1603727937000\n",
      "32293 2020-10-26 15:58:57 1603727937200\n",
      "32294 2020-10-26 15:58:57 1603727937400\n",
      "32295 2020-10-26 15:58:57 1603727937600\n",
      "32296 2020-10-26 15:58:57 1603727937800\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32300 2020-10-26 15:58:58 1603727938000\n",
      "32299 2020-10-26 15:58:58 1603727938250\n",
      "32298 2020-10-26 15:58:58 1603727938500\n",
      "32297 2020-10-26 15:58:58 1603727938750\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32301 2020-10-26 15:58:59 1603727939000\n",
      "32302 2020-10-26 15:58:59 1603727939500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32303 2020-10-26 15:59:00 1603727940000\n",
      "32304 2020-10-26 15:59:00 1603727940500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32310 2020-10-26 15:59:06 1603727946000\n",
      "32311 2020-10-26 15:59:06 1603727946333\n",
      "32312 2020-10-26 15:59:06 1603727946666\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32521 2020-10-26 16:02:35 1603728155000\n",
      "32522 2020-10-26 16:02:35 1603728155333\n",
      "32523 2020-10-26 16:02:35 1603728155666\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32527 2020-10-26 16:02:36 1603728156000\n",
      "32526 2020-10-26 16:02:36 1603728156250\n",
      "32525 2020-10-26 16:02:36 1603728156500\n",
      "32524 2020-10-26 16:02:36 1603728156750\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32528 2020-10-26 16:02:37 1603728157000\n",
      "32529 2020-10-26 16:02:37 1603728157500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32531 2020-10-26 16:02:39 1603728159000\n",
      "32532 2020-10-26 16:02:39 1603728159500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32538 2020-10-26 16:02:45 1603728165000\n",
      "32539 2020-10-26 16:02:45 1603728165333\n",
      "32540 2020-10-26 16:02:45 1603728165666\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33229 2020-10-26 16:14:14 1603728854000\n",
      "33230 2020-10-26 16:14:14 1603728854200\n",
      "33231 2020-10-26 16:14:14 1603728854400\n",
      "33232 2020-10-26 16:14:14 1603728854600\n",
      "33233 2020-10-26 16:14:14 1603728854800\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33236 2020-10-26 16:14:15 1603728855000\n",
      "33234 2020-10-26 16:14:15 1603728855333\n",
      "33235 2020-10-26 16:14:15 1603728855666\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33237 2020-10-26 16:14:16 1603728856000\n",
      "33238 2020-10-26 16:14:16 1603728856250\n",
      "33239 2020-10-26 16:14:16 1603728856500\n",
      "33240 2020-10-26 16:14:16 1603728856750\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33241 2020-10-26 16:14:17 1603728857000\n",
      "33242 2020-10-26 16:14:17 1603728857500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33246 2020-10-26 16:14:19 1603728859000\n",
      "33244 2020-10-26 16:14:19 1603728859333\n",
      "33245 2020-10-26 16:14:19 1603728859666\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33340 2020-10-26 16:15:53 1603728953000\n",
      "33341 2020-10-26 16:15:53 1603728953500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33349 2020-10-26 16:16:01 1603728961000\n",
      "33350 2020-10-26 16:16:01 1603728961333\n",
      "33351 2020-10-26 16:16:01 1603728961666\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33352 2020-10-26 16:16:02 1603728962000\n",
      "33353 2020-10-26 16:16:02 1603728962500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33354 2020-10-26 16:16:03 1603728963000\n",
      "33355 2020-10-26 16:16:03 1603728963500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33359 2020-10-26 16:16:04 1603728964000\n",
      "33358 2020-10-26 16:16:04 1603728964250\n",
      "33357 2020-10-26 16:16:04 1603728964500\n",
      "33356 2020-10-26 16:16:04 1603728964750\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33360 2020-10-26 16:16:05 1603728965000\n",
      "33361 2020-10-26 16:16:05 1603728965500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33383 2020-10-26 16:16:27 1603728987000\n",
      "33384 2020-10-26 16:16:27 1603728987500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33385 2020-10-26 16:16:28 1603728988000\n",
      "33386 2020-10-26 16:16:28 1603728988500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33388 2020-10-26 16:16:29 1603728989000\n",
      "33389 2020-10-26 16:16:29 1603728989333\n",
      "33387 2020-10-26 16:16:29 1603728989666\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33390 2020-10-26 16:16:30 1603728990000\n",
      "33391 2020-10-26 16:16:30 1603728990333\n",
      "33392 2020-10-26 16:16:30 1603728990666\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33393 2020-10-26 16:16:31 1603728991000\n",
      "33394 2020-10-26 16:16:31 1603728991500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33396 2020-10-26 16:16:33 1603728993000\n",
      "33397 2020-10-26 16:16:33 1603728993500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33399 2020-10-26 16:16:34 1603728994000\n",
      "33398 2020-10-26 16:16:34 1603728994500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33402 2020-10-26 16:16:37 1603728997000\n",
      "33403 2020-10-26 16:16:37 1603728997500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33415 2020-10-26 16:16:49 1603729009000\n",
      "33416 2020-10-26 16:16:49 1603729009500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33418 2020-10-26 16:16:50 1603729010000\n",
      "33417 2020-10-26 16:16:50 1603729010500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33423 2020-10-26 16:16:55 1603729015000\n",
      "33424 2020-10-26 16:16:55 1603729015500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33427 2020-10-26 16:16:57 1603729017000\n",
      "33426 2020-10-26 16:16:57 1603729017500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33433 2020-10-26 16:17:03 1603729023000\n",
      "33434 2020-10-26 16:17:03 1603729023500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33437 2020-10-26 16:17:06 1603729026000\n",
      "33438 2020-10-26 16:17:06 1603729026500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33454 2020-10-26 16:17:22 1603729042000\n",
      "33455 2020-10-26 16:17:22 1603729042500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33457 2020-10-26 16:17:24 1603729044000\n",
      "33458 2020-10-26 16:17:24 1603729044500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33468 2020-10-26 16:17:34 1603729054000\n",
      "33469 2020-10-26 16:17:34 1603729054500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33486 2020-10-26 16:17:51 1603729071000\n",
      "33487 2020-10-26 16:17:51 1603729071500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33491 2020-10-26 16:17:54 1603729074000\n",
      "33490 2020-10-26 16:17:54 1603729074500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33496 2020-10-26 16:17:59 1603729079000\n",
      "33497 2020-10-26 16:17:59 1603729079500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33517 2020-10-26 16:18:19 1603729099000\n",
      "33518 2020-10-26 16:18:19 1603729099500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33519 2020-10-26 16:18:20 1603729100000\n",
      "33520 2020-10-26 16:18:20 1603729100500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33533 2020-10-26 16:18:33 1603729113000\n",
      "33534 2020-10-26 16:18:33 1603729113333\n",
      "33535 2020-10-26 16:18:33 1603729113666\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33543 2020-10-26 16:18:41 1603729121000\n",
      "33544 2020-10-26 16:18:41 1603729121500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33546 2020-10-26 16:18:43 1603729123000\n",
      "33547 2020-10-26 16:18:43 1603729123500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33555 2020-10-26 16:18:51 1603729131000\n",
      "33556 2020-10-26 16:18:51 1603729131500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33558 2020-10-26 16:18:53 1603729133000\n",
      "33559 2020-10-26 16:18:53 1603729133500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33574 2020-10-26 16:19:08 1603729148000\n",
      "33575 2020-10-26 16:19:08 1603729148500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33576 2020-10-26 16:19:09 1603729149000\n",
      "33577 2020-10-26 16:19:09 1603729149500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33583 2020-10-26 16:19:15 1603729155000\n",
      "33584 2020-10-26 16:19:15 1603729155500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33585 2020-10-26 16:19:16 1603729156000\n",
      "33586 2020-10-26 16:19:16 1603729156500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33588 2020-10-26 16:19:18 1603729158000\n",
      "33589 2020-10-26 16:19:18 1603729158500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33590 2020-10-26 16:19:19 1603729159000\n",
      "33591 2020-10-26 16:19:19 1603729159500\n",
      "---Handling Duplicates, Split duplicate evenly---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33593 2020-10-26 16:19:20 1603729160000\n",
      "33592 2020-10-26 16:19:20 1603729160500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33594 2020-10-26 16:19:21 1603729161000\n",
      "33595 2020-10-26 16:19:21 1603729161250\n",
      "33596 2020-10-26 16:19:21 1603729161500\n",
      "33597 2020-10-26 16:19:21 1603729161750\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33598 2020-10-26 16:19:22 1603729162000\n",
      "33599 2020-10-26 16:19:22 1603729162250\n",
      "33600 2020-10-26 16:19:22 1603729162500\n",
      "33601 2020-10-26 16:19:22 1603729162750\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33604 2020-10-26 16:19:23 1603729163000\n",
      "33602 2020-10-26 16:19:23 1603729163333\n",
      "33603 2020-10-26 16:19:23 1603729163666\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33605 2020-10-26 16:19:24 1603729164000\n",
      "33606 2020-10-26 16:19:24 1603729164200\n",
      "33607 2020-10-26 16:19:24 1603729164400\n",
      "33608 2020-10-26 16:19:24 1603729164600\n",
      "33609 2020-10-26 16:19:24 1603729164800\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33611 2020-10-26 16:19:25 1603729165000\n",
      "33610 2020-10-26 16:19:25 1603729165333\n",
      "33612 2020-10-26 16:19:25 1603729165666\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33622 2020-10-26 16:19:34 1603729174000\n",
      "33621 2020-10-26 16:19:34 1603729174500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33723 2020-10-26 16:21:15 1603729275000\n",
      "33724 2020-10-26 16:21:15 1603729275500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33725 2020-10-26 16:21:16 1603729276000\n",
      "33726 2020-10-26 16:21:16 1603729276333\n",
      "33727 2020-10-26 16:21:16 1603729276666\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33731 2020-10-26 16:21:17 1603729277000\n",
      "33730 2020-10-26 16:21:17 1603729277250\n",
      "33728 2020-10-26 16:21:17 1603729277500\n",
      "33729 2020-10-26 16:21:17 1603729277750\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33732 2020-10-26 16:21:18 1603729278000\n",
      "33733 2020-10-26 16:21:18 1603729278250\n",
      "33734 2020-10-26 16:21:18 1603729278500\n",
      "33735 2020-10-26 16:21:18 1603729278750\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33737 2020-10-26 16:21:19 1603729279000\n",
      "33736 2020-10-26 16:21:19 1603729279500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33755 2020-10-26 16:21:37 1603729297000\n",
      "33756 2020-10-26 16:21:37 1603729297500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33757 2020-10-26 16:21:38 1603729298000\n",
      "33758 2020-10-26 16:21:38 1603729298500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33761 2020-10-26 16:21:39 1603729299000\n",
      "33759 2020-10-26 16:21:39 1603729299333\n",
      "33760 2020-10-26 16:21:39 1603729299666\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33762 2020-10-26 16:21:40 1603729300000\n",
      "33763 2020-10-26 16:21:40 1603729300333\n",
      "33764 2020-10-26 16:21:40 1603729300666\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33765 2020-10-26 16:21:41 1603729301000\n",
      "33766 2020-10-26 16:21:41 1603729301500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33768 2020-10-26 16:21:43 1603729303000\n",
      "33769 2020-10-26 16:21:43 1603729303500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33771 2020-10-26 16:21:44 1603729304000\n",
      "33770 2020-10-26 16:21:44 1603729304500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33774 2020-10-26 16:21:47 1603729307000\n",
      "33775 2020-10-26 16:21:47 1603729307500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33778 2020-10-26 16:21:50 1603729310000\n",
      "33779 2020-10-26 16:21:50 1603729310500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33782 2020-10-26 16:21:53 1603729313000\n",
      "33783 2020-10-26 16:21:53 1603729313500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33810 2020-10-26 16:22:19 1603729339000\n",
      "33809 2020-10-26 16:22:19 1603729339500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33813 2020-10-26 16:22:22 1603729342000\n",
      "33814 2020-10-26 16:22:22 1603729342500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33823 2020-10-26 16:22:31 1603729351000\n",
      "33824 2020-10-26 16:22:31 1603729351500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33825 2020-10-26 16:22:32 1603729352000\n",
      "33826 2020-10-26 16:22:32 1603729352500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33832 2020-10-26 16:22:38 1603729358000\n",
      "33833 2020-10-26 16:22:38 1603729358500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33834 2020-10-26 16:22:39 1603729359000\n",
      "33835 2020-10-26 16:22:39 1603729359500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33854 2020-10-26 16:22:58 1603729378000\n",
      "33855 2020-10-26 16:22:58 1603729378500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33861 2020-10-26 16:23:03 1603729383000\n",
      "33860 2020-10-26 16:23:03 1603729383500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33900 2020-10-26 16:23:42 1603729422000\n",
      "33901 2020-10-26 16:23:42 1603729422500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33903 2020-10-26 16:23:44 1603729424000\n",
      "33904 2020-10-26 16:23:44 1603729424500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33909 2020-10-26 16:23:49 1603729429000\n",
      "33910 2020-10-26 16:23:49 1603729429500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33913 2020-10-26 16:23:51 1603729431000\n",
      "33912 2020-10-26 16:23:51 1603729431500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33929 2020-10-26 16:24:07 1603729447000\n",
      "33930 2020-10-26 16:24:07 1603729447500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33933 2020-10-26 16:24:09 1603729449000\n",
      "33932 2020-10-26 16:24:09 1603729449500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33936 2020-10-26 16:24:12 1603729452000\n",
      "33937 2020-10-26 16:24:12 1603729452500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33939 2020-10-26 16:24:14 1603729454000\n",
      "33940 2020-10-26 16:24:14 1603729454500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33949 2020-10-26 16:24:23 1603729463000\n",
      "33950 2020-10-26 16:24:23 1603729463500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33953 2020-10-26 16:24:25 1603729465000\n",
      "33952 2020-10-26 16:24:25 1603729465500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33955 2020-10-26 16:24:27 1603729467000\n",
      "33956 2020-10-26 16:24:27 1603729467333\n",
      "33957 2020-10-26 16:24:27 1603729467666\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33959 2020-10-26 16:24:29 1603729469000\n",
      "33960 2020-10-26 16:24:29 1603729469500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33963 2020-10-26 16:24:31 1603729471000\n",
      "33964 2020-10-26 16:24:31 1603729471333\n",
      "33962 2020-10-26 16:24:31 1603729471666\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33965 2020-10-26 16:24:32 1603729472000\n",
      "33966 2020-10-26 16:24:32 1603729472500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33968 2020-10-26 16:24:34 1603729474000\n",
      "33969 2020-10-26 16:24:34 1603729474500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33970 2020-10-26 16:24:35 1603729475000\n",
      "33971 2020-10-26 16:24:35 1603729475500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33972 2020-10-26 16:24:36 1603729476000\n",
      "33973 2020-10-26 16:24:36 1603729476500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33975 2020-10-26 16:24:37 1603729477000\n",
      "33974 2020-10-26 16:24:37 1603729477500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33976 2020-10-26 16:24:38 1603729478000\n",
      "33977 2020-10-26 16:24:38 1603729478500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33978 2020-10-26 16:24:39 1603729479000\n",
      "33979 2020-10-26 16:24:39 1603729479500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33980 2020-10-26 16:24:40 1603729480000\n",
      "33981 2020-10-26 16:24:40 1603729480250\n",
      "33982 2020-10-26 16:24:40 1603729480500\n",
      "33983 2020-10-26 16:24:40 1603729480750\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33986 2020-10-26 16:24:41 1603729481000\n",
      "33984 2020-10-26 16:24:41 1603729481333\n",
      "33985 2020-10-26 16:24:41 1603729481666\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33987 2020-10-26 16:24:42 1603729482000\n",
      "33988 2020-10-26 16:24:42 1603729482200\n",
      "33989 2020-10-26 16:24:42 1603729482400\n",
      "33990 2020-10-26 16:24:42 1603729482600\n",
      "33991 2020-10-26 16:24:42 1603729482800\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33992 2020-10-26 16:24:43 1603729483000\n",
      "33993 2020-10-26 16:24:43 1603729483500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "34001 2020-10-26 16:24:51 1603729491000\n",
      "34002 2020-10-26 16:24:51 1603729491500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "34006 2020-10-26 16:24:52 1603729492000\n",
      "34007 2020-10-26 16:24:52 1603729492200\n",
      "34005 2020-10-26 16:24:52 1603729492400\n",
      "34004 2020-10-26 16:24:52 1603729492600\n",
      "34003 2020-10-26 16:24:52 1603729492800\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "34008 2020-10-26 16:24:53 1603729493000\n",
      "34009 2020-10-26 16:24:53 1603729493500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "34010 2020-10-26 16:24:54 1603729494000\n",
      "34011 2020-10-26 16:24:54 1603729494250\n",
      "34012 2020-10-26 16:24:54 1603729494500\n",
      "34013 2020-10-26 16:24:54 1603729494750\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "34016 2020-10-26 16:24:55 1603729495000\n",
      "34015 2020-10-26 16:24:55 1603729495333\n",
      "34014 2020-10-26 16:24:55 1603729495666\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "34017 2020-10-26 16:24:56 1603729496000\n",
      "34018 2020-10-26 16:24:56 1603729496333\n",
      "34019 2020-10-26 16:24:56 1603729496666\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "34224 2020-10-26 16:34:41 1603730081000\n",
      "34225 2020-10-26 16:34:41 1603730081500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "34274 2020-10-26 16:35:28 1603730128000\n",
      "34272 2020-10-26 16:35:28 1603730128333\n",
      "34273 2020-10-26 16:35:28 1603730128666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Handling Duplicates, Split duplicate evenly---\n",
      "8186 2020-10-26 15:50:11 1603727411000\n",
      "8185 2020-10-26 15:50:11 1603727411500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "48 2020-10-26 15:51:05 1603727465000\n",
      "47 2020-10-26 15:51:05 1603727465500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "67 2020-10-26 15:51:25 1603727485000\n",
      "68 2020-10-26 15:51:25 1603727485500\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward---\n",
      "85 2020-10-26 15:51:43 1603727503000\n",
      "86 2020-10-26 15:51:43 1603727504000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "88 2020-10-26 15:51:45 1603727505000\n",
      "87 2020-10-26 15:51:45 1603727505500\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward---\n",
      "127 2020-10-26 15:52:25 1603727545000\n",
      "128 2020-10-26 15:52:25 1603727546000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "129 2020-10-26 15:52:27 1603727547000\n",
      "130 2020-10-26 15:52:27 1603727547500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "148 2020-10-26 15:52:46 1603727566000\n",
      "149 2020-10-26 15:52:46 1603727566500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "191 2020-10-26 15:53:28 1603727608000\n",
      "190 2020-10-26 15:53:28 1603727608500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "223 2020-10-26 15:54:00 1603727640000\n",
      "222 2020-10-26 15:54:00 1603727640500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "260 2020-10-26 15:54:37 1603727677000\n",
      "259 2020-10-26 15:54:37 1603727677500\n",
      "---Handling Duplicates, Fill gap with duplicate, Backward---\n",
      "271 2020-10-26 15:54:48 1603727688000\n",
      "270 2020-10-26 15:54:48 1603727687000\n",
      "---Handling Duplicates, Fill gap with duplicate, Backward---\n",
      "274 2020-10-26 15:54:51 1603727691000\n",
      "273 2020-10-26 15:54:51 1603727690000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "282 2020-10-26 15:55:00 1603727700000\n",
      "283 2020-10-26 15:55:00 1603727700500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "296 2020-10-26 15:55:14 1603727714000\n",
      "297 2020-10-26 15:55:14 1603727714500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "300 2020-10-26 15:55:18 1603727718000\n",
      "301 2020-10-26 15:55:18 1603727718500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "317 2020-10-26 15:55:34 1603727734000\n",
      "316 2020-10-26 15:55:34 1603727734500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "325 2020-10-26 15:55:42 1603727742000\n",
      "324 2020-10-26 15:55:42 1603727742500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "334 2020-10-26 15:55:51 1603727751000\n",
      "333 2020-10-26 15:55:51 1603727751500\n",
      "---Handling Duplicates, Fill gap with duplicate, Backward---\n",
      "368 2020-10-26 15:56:25 1603727785000\n",
      "367 2020-10-26 15:56:25 1603727784000\n",
      "---Handling Duplicates, Fill gap with duplicate, Backward---\n",
      "385 2020-10-26 15:56:42 1603727802000\n",
      "384 2020-10-26 15:56:42 1603727801000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "391 2020-10-26 15:56:48 1603727808000\n",
      "390 2020-10-26 15:56:48 1603727808500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "397 2020-10-26 15:56:54 1603727814000\n",
      "396 2020-10-26 15:56:54 1603727814500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "403 2020-10-26 15:57:00 1603727820000\n",
      "402 2020-10-26 15:57:00 1603727820500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "444 2020-10-26 15:57:42 1603727862000\n",
      "445 2020-10-26 15:57:42 1603727862500\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward---\n",
      "457 2020-10-26 15:57:55 1603727875000\n",
      "458 2020-10-26 15:57:55 1603727876000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "459 2020-10-26 15:57:57 1603727877000\n",
      "460 2020-10-26 15:57:57 1603727877500\n",
      "---Handling Duplicates, Fill gap with duplicate, Backward---\n",
      "500 2020-10-26 15:58:37 1603727917000\n",
      "499 2020-10-26 15:58:37 1603727916000\n",
      "---Handling Duplicates, Fill gap with duplicate, Backward---\n",
      "505 2020-10-26 15:58:42 1603727922000\n",
      "504 2020-10-26 15:58:42 1603727921000\n",
      "---Handling Duplicates, Fill gap with duplicate, Backward---\n",
      "511 2020-10-26 15:58:48 1603727928000\n",
      "510 2020-10-26 15:58:48 1603727927000\n",
      "---Handling Duplicates, Fill gap with duplicate, Backward---\n",
      "514 2020-10-26 15:58:51 1603727931000\n",
      "513 2020-10-26 15:58:51 1603727930000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "614 2020-10-26 16:00:31 1603728031000\n",
      "613 2020-10-26 16:00:31 1603728031500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "718 2020-10-26 16:02:15 1603728135000\n",
      "717 2020-10-26 16:02:15 1603728135500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "866 2020-10-26 16:04:43 1603728283000\n",
      "865 2020-10-26 16:04:43 1603728283500\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward---\n",
      "893 2020-10-26 16:05:11 1603728311000\n",
      "894 2020-10-26 16:05:11 1603728312000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "895 2020-10-26 16:05:13 1603728313000\n",
      "896 2020-10-26 16:05:13 1603728313500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "899 2020-10-26 16:05:16 1603728316000\n",
      "898 2020-10-26 16:05:16 1603728316500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "933 2020-10-26 16:05:50 1603728350000\n",
      "932 2020-10-26 16:05:50 1603728350500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "991 2020-10-26 16:06:48 1603728408000\n",
      "990 2020-10-26 16:06:48 1603728408500\n",
      "---Handling Duplicates, Fill gap with duplicate, Backward---\n",
      "1013 2020-10-26 16:07:10 1603728430000\n",
      "1012 2020-10-26 16:07:10 1603728429000\n",
      "---Handling Duplicates, Fill gap with duplicate, Backward---\n",
      "1016 2020-10-26 16:07:13 1603728433000\n",
      "1015 2020-10-26 16:07:13 1603728432000\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward and Backward---\n",
      "1018 2020-10-26 16:07:16 1603728435000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1025 2020-10-26 16:07:22 1603728442000\n",
      "1024 2020-10-26 16:07:22 1603728442500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1036 2020-10-26 16:07:34 1603728454000\n",
      "1037 2020-10-26 16:07:34 1603728454500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1098 2020-10-26 16:08:36 1603728516000\n",
      "1099 2020-10-26 16:08:36 1603728516500\n",
      "---Handling Duplicates, Fill gap with duplicate, Backward---\n",
      "1134 2020-10-26 16:09:11 1603728551000\n",
      "1133 2020-10-26 16:09:11 1603728550000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1137 2020-10-26 16:09:14 1603728554000\n",
      "1136 2020-10-26 16:09:14 1603728554500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1140 2020-10-26 16:09:17 1603728557000\n",
      "1139 2020-10-26 16:09:17 1603728557500\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward---\n",
      "1145 2020-10-26 16:09:23 1603728563000\n",
      "1146 2020-10-26 16:09:23 1603728564000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1148 2020-10-26 16:09:25 1603728565000\n",
      "1147 2020-10-26 16:09:25 1603728565500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1206 2020-10-26 16:10:24 1603728624000\n",
      "1207 2020-10-26 16:10:24 1603728624500\n",
      "---Handling Duplicates, Fill gap with duplicate, Backward---\n",
      "1257 2020-10-26 16:11:14 1603728674000\n",
      "1256 2020-10-26 16:11:14 1603728673000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1263 2020-10-26 16:11:20 1603728680000\n",
      "1262 2020-10-26 16:11:20 1603728680500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1280 2020-10-26 16:11:37 1603728697000\n",
      "1279 2020-10-26 16:11:37 1603728697500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1317 2020-10-26 16:12:14 1603728734000\n",
      "1316 2020-10-26 16:12:14 1603728734500\n",
      "---Handling Duplicates, Fill gap with duplicate, Backward---\n",
      "1376 2020-10-26 16:13:13 1603728793000\n",
      "1375 2020-10-26 16:13:13 1603728792000\n",
      "---Handling Duplicates, Fill gap with duplicate, Backward---\n",
      "1379 2020-10-26 16:13:16 1603728796000\n",
      "1378 2020-10-26 16:13:16 1603728795000\n",
      "---Handling Duplicates, Fill gap with duplicate, Backward---\n",
      "1382 2020-10-26 16:13:19 1603728799000\n",
      "1381 2020-10-26 16:13:19 1603728798000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1393 2020-10-26 16:13:30 1603728810000\n",
      "1392 2020-10-26 16:13:30 1603728810500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1424 2020-10-26 16:14:02 1603728842000\n",
      "1425 2020-10-26 16:14:02 1603728842500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1438 2020-10-26 16:14:16 1603728856000\n",
      "1439 2020-10-26 16:14:16 1603728856500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1442 2020-10-26 16:14:19 1603728859000\n",
      "1441 2020-10-26 16:14:19 1603728859500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1447 2020-10-26 16:14:24 1603728864000\n",
      "1446 2020-10-26 16:14:24 1603728864500\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward---\n",
      "1503 2020-10-26 16:15:21 1603728921000\n",
      "1504 2020-10-26 16:15:21 1603728922000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1506 2020-10-26 16:15:23 1603728923000\n",
      "1505 2020-10-26 16:15:23 1603728923500\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward and Backward---\n",
      "1508 2020-10-26 16:15:26 1603728925000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1530 2020-10-26 16:15:48 1603728948000\n",
      "1531 2020-10-26 16:15:48 1603728948500\n",
      "---Handling Duplicates, Fill gap with duplicate, Backward---\n",
      "1571 2020-10-26 16:16:28 1603728988000\n",
      "1570 2020-10-26 16:16:28 1603728987000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1664 2020-10-26 16:18:02 1603729082000\n",
      "1665 2020-10-26 16:18:02 1603729082500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "31753.0 2020-10-26 15:50:08 1603727408000\n",
      "31753.0 2020-10-26 15:50:08 1603727408500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "31766.0 2020-10-26 15:50:21 1603727421000\n",
      "31766.0 2020-10-26 15:50:21 1603727421500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "31769.0 2020-10-26 15:50:24 1603727424000\n",
      "31769.0 2020-10-26 15:50:24 1603727424500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "31771.0 2020-10-26 15:50:26 1603727426000\n",
      "31771.0 2020-10-26 15:50:26 1603727426500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "31788.0 2020-10-26 15:50:43 1603727443000\n",
      "31788.0 2020-10-26 15:50:43 1603727443500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "31790.0 2020-10-26 15:50:45 1603727445000\n",
      "31790.0 2020-10-26 15:50:45 1603727445500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "31808.0 2020-10-26 15:51:03 1603727463000\n",
      "31808.0 2020-10-26 15:51:03 1603727463500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "31812.0 2020-10-26 15:51:07 1603727467000\n",
      "31812.0 2020-10-26 15:51:07 1603727467500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "31833.0 2020-10-26 15:51:28 1603727488000\n",
      "31833.0 2020-10-26 15:51:28 1603727488500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "31870.0 2020-10-26 15:52:05 1603727525000\n",
      "31870.0 2020-10-26 15:52:05 1603727525500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "31872.0 2020-10-26 15:52:07 1603727527000\n",
      "31872.0 2020-10-26 15:52:07 1603727527500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "31877.0 2020-10-26 15:52:12 1603727532000\n",
      "31877.0 2020-10-26 15:52:12 1603727532500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "31907.0 2020-10-26 15:52:42 1603727562000\n",
      "31907.0 2020-10-26 15:52:42 1603727562500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "31918.0 2020-10-26 15:52:53 1603727573000\n",
      "31918.0 2020-10-26 15:52:53 1603727573500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "31927.0 2020-10-26 15:53:02 1603727582000\n",
      "31927.0 2020-10-26 15:53:02 1603727582500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32027.0 2020-10-26 15:54:42 1603727682000\n",
      "32027.0 2020-10-26 15:54:42 1603727682500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32030.0 2020-10-26 15:54:45 1603727685000\n",
      "32030.0 2020-10-26 15:54:45 1603727685500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32039.0 2020-10-26 15:54:54 1603727694000\n",
      "32039.0 2020-10-26 15:54:54 1603727694500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32054.0 2020-10-26 15:55:09 1603727709000\n",
      "32054.0 2020-10-26 15:55:09 1603727709500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32070.0 2020-10-26 15:55:25 1603727725000\n",
      "32070.0 2020-10-26 15:55:25 1603727725500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32072.0 2020-10-26 15:55:27 1603727727000\n",
      "32072.0 2020-10-26 15:55:27 1603727727500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32142.0 2020-10-26 15:56:37 1603727797000\n",
      "32142.0 2020-10-26 15:56:37 1603727797500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32150.0 2020-10-26 15:56:45 1603727805000\n",
      "32150.0 2020-10-26 15:56:45 1603727805500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32156.0 2020-10-26 15:56:51 1603727811000\n",
      "32156.0 2020-10-26 15:56:51 1603727811500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32162.0 2020-10-26 15:56:57 1603727817000\n",
      "32162.0 2020-10-26 15:56:57 1603727817500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32168.0 2020-10-26 15:57:03 1603727823000\n",
      "32168.0 2020-10-26 15:57:03 1603727823500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32171.0 2020-10-26 15:57:06 1603727826000\n",
      "32171.0 2020-10-26 15:57:06 1603727826500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32176.0 2020-10-26 15:57:11 1603727831000\n",
      "32176.0 2020-10-26 15:57:11 1603727831500\n",
      "500 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32241.0 2020-10-26 15:58:15 1603727895000\n",
      "32241.0 2020-10-26 15:58:15 1603727895500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32243.0 2020-10-26 15:58:17 1603727897000\n",
      "32243.0 2020-10-26 15:58:17 1603727897500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32264.0 2020-10-26 15:58:35 1603727915000\n",
      "32264.0 2020-10-26 15:58:35 1603727915500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32446.0 2020-10-26 16:01:20 1603728080000\n",
      "32446.0 2020-10-26 16:01:20 1603728080500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32506.0 2020-10-26 16:02:20 1603728140000\n",
      "32506.0 2020-10-26 16:02:20 1603728140500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32520.0 2020-10-26 16:02:34 1603728154000\n",
      "32520.0 2020-10-26 16:02:34 1603728154500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32553.0 2020-10-26 16:02:58 1603728178000\n",
      "32553.0 2020-10-26 16:02:58 1603728178500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32573.0 2020-10-26 16:03:18 1603728198000\n",
      "32573.0 2020-10-26 16:03:18 1603728198500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32593.0 2020-10-26 16:03:38 1603728218000\n",
      "32593.0 2020-10-26 16:03:38 1603728218500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32596.0 2020-10-26 16:03:41 1603728221000\n",
      "32596.0 2020-10-26 16:03:41 1603728221500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32613.0 2020-10-26 16:03:58 1603728238000\n",
      "32613.0 2020-10-26 16:03:58 1603728238500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32633.0 2020-10-26 16:04:18 1603728258000\n",
      "32633.0 2020-10-26 16:04:18 1603728258500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32653.0 2020-10-26 16:04:38 1603728278000\n",
      "32653.0 2020-10-26 16:04:38 1603728278500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32675.0 2020-10-26 16:05:00 1603728300000\n",
      "32675.0 2020-10-26 16:05:00 1603728300500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32677.0 2020-10-26 16:05:02 1603728302000\n",
      "32677.0 2020-10-26 16:05:02 1603728302500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32680.0 2020-10-26 16:05:05 1603728305000\n",
      "32680.0 2020-10-26 16:05:05 1603728305500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32694.0 2020-10-26 16:05:19 1603728319000\n",
      "32694.0 2020-10-26 16:05:19 1603728319500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32700.0 2020-10-26 16:05:25 1603728325000\n",
      "32700.0 2020-10-26 16:05:25 1603728325500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32703.0 2020-10-26 16:05:28 1603728328000\n",
      "32703.0 2020-10-26 16:05:28 1603728328500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32708.0 2020-10-26 16:05:33 1603728333000\n",
      "32708.0 2020-10-26 16:05:33 1603728333500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32796.0 2020-10-26 16:07:01 1603728421000\n",
      "32796.0 2020-10-26 16:07:01 1603728421500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32814.0 2020-10-26 16:07:19 1603728439000\n",
      "32814.0 2020-10-26 16:07:19 1603728439500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32822.0 2020-10-26 16:07:27 1603728447000\n",
      "32822.0 2020-10-26 16:07:27 1603728447500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32825.0 2020-10-26 16:07:30 1603728450000\n",
      "32825.0 2020-10-26 16:07:30 1603728450500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32918.0 2020-10-26 16:09:03 1603728543000\n",
      "32918.0 2020-10-26 16:09:03 1603728543500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32923.0 2020-10-26 16:09:08 1603728548000\n",
      "32923.0 2020-10-26 16:09:08 1603728548500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32952.0 2020-10-26 16:09:37 1603728577000\n",
      "32952.0 2020-10-26 16:09:37 1603728577500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33038.0 2020-10-26 16:11:03 1603728663000\n",
      "33038.0 2020-10-26 16:11:03 1603728663500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33052.0 2020-10-26 16:11:17 1603728677000\n",
      "33052.0 2020-10-26 16:11:17 1603728677500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33058.0 2020-10-26 16:11:23 1603728683000\n",
      "33058.0 2020-10-26 16:11:23 1603728683500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33061.0 2020-10-26 16:11:26 1603728686000\n",
      "33061.0 2020-10-26 16:11:26 1603728686500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33064.0 2020-10-26 16:11:29 1603728689000\n",
      "33064.0 2020-10-26 16:11:29 1603728689500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33067.0 2020-10-26 16:11:32 1603728692000\n",
      "33067.0 2020-10-26 16:11:32 1603728692500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33070.0 2020-10-26 16:11:35 1603728695000\n",
      "33070.0 2020-10-26 16:11:35 1603728695500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33165.0 2020-10-26 16:13:10 1603728790000\n",
      "33165.0 2020-10-26 16:13:10 1603728790500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33180.0 2020-10-26 16:13:25 1603728805000\n",
      "33180.0 2020-10-26 16:13:25 1603728805500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33182.0 2020-10-26 16:13:27 1603728807000\n",
      "33182.0 2020-10-26 16:13:27 1603728807500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33188.0 2020-10-26 16:13:33 1603728813000\n",
      "33188.0 2020-10-26 16:13:33 1603728813500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33191.0 2020-10-26 16:13:36 1603728816000\n",
      "33191.0 2020-10-26 16:13:36 1603728816500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33196.0 2020-10-26 16:13:41 1603728821000\n",
      "33196.0 2020-10-26 16:13:41 1603728821500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33199.0 2020-10-26 16:13:44 1603728824000\n",
      "33199.0 2020-10-26 16:13:44 1603728824500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33249.0 2020-10-26 16:14:22 1603728862000\n",
      "33249.0 2020-10-26 16:14:22 1603728862500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33253.0 2020-10-26 16:14:26 1603728866000\n",
      "33253.0 2020-10-26 16:14:26 1603728866500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33299.0 2020-10-26 16:15:12 1603728912000\n",
      "33299.0 2020-10-26 16:15:12 1603728912500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33302.0 2020-10-26 16:15:15 1603728915000\n",
      "33302.0 2020-10-26 16:15:15 1603728915500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33316.0 2020-10-26 16:15:29 1603728929000\n",
      "33316.0 2020-10-26 16:15:29 1603728929500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33319.0 2020-10-26 16:15:32 1603728932000\n",
      "33319.0 2020-10-26 16:15:32 1603728932500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33322.0 2020-10-26 16:15:35 1603728935000\n",
      "33322.0 2020-10-26 16:15:35 1603728935500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33325.0 2020-10-26 16:15:38 1603728938000\n",
      "33325.0 2020-10-26 16:15:38 1603728938500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33330.0 2020-10-26 16:15:43 1603728943000\n",
      "33330.0 2020-10-26 16:15:43 1603728943500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33443.0 2020-10-26 16:17:11 1603729031000\n",
      "33443.0 2020-10-26 16:17:11 1603729031500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33446.0 2020-10-26 16:17:14 1603729034000\n",
      "33446.0 2020-10-26 16:17:14 1603729034500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33450.0 2020-10-26 16:17:18 1603729038000\n",
      "33450.0 2020-10-26 16:17:18 1603729038500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33453.0 2020-10-26 16:17:21 1603729041000\n",
      "33453.0 2020-10-26 16:17:21 1603729041500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33471.0 2020-10-26 16:17:36 1603729056000\n",
      "33471.0 2020-10-26 16:17:36 1603729056500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33476.0 2020-10-26 16:17:41 1603729061000\n",
      "33476.0 2020-10-26 16:17:41 1603729061500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33552.0 2020-10-26 16:18:48 1603729128000\n",
      "33552.0 2020-10-26 16:18:48 1603729128500\n",
      "1000 500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33558.0 2020-10-26 16:18:53 1603729133000\n",
      "33558.0 2020-10-26 16:18:53 1603729133500\n",
      "1000 1000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33580.0 2020-10-26 16:19:12 1603729152000\n",
      "33580.0 2020-10-26 16:19:12 1603729152500\n",
      "1000 500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33583.0 2020-10-26 16:19:15 1603729155000\n",
      "33583.0 2020-10-26 16:19:15 1603729155500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Unit Test for entire Dataframe---\n",
      "(2736, 1047) (2761, 1051)\n",
      "Equality Between Sample Output and Self Processed:  False\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Can only compare identically-labeled Series objects",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-172-a9fbd40a791a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;31m#unit_test(sample_output_filename[0], 'C2', df_c2_clean)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;31m#unit_test(sample_output_filename[0], 'OD', df_od_clean)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m \u001b[0munit_test_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_output_filename\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-170-ad5df4c40657>\u001b[0m in \u001b[0;36munit_test_all\u001b[1;34m(sample_output_filename, df_test)\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[0massert_equal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0momap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan_equal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Timestampms'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_output\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'epoch'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Equality Between Sample Output and Self Processed: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0massert_equal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m     \u001b[0mdf_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'result'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Timestampms'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mdf_output\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'epoch'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'True'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'False'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m     \u001b[1;31m#df_test.to_csv('./df_test.csv', index=False, header=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\ops\\common.py\u001b[0m in \u001b[0;36mnew_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\arraylike.py\u001b[0m in \u001b[0;36m__eq__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"__eq__\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cmp_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"__ne__\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_cmp_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   4932\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4933\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_indexed_same\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4934\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Can only compare identically-labeled Series objects\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4935\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4936\u001b[0m         \u001b[0mlvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextract_numpy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Can only compare identically-labeled Series objects"
     ]
    }
   ],
   "source": [
    "input_folder_path =  './EVR_T23_Car46_261020/'\n",
    "#input_folder_path = './Train 23 14NOV2019 Work Folder/raw logs'\n",
    "start_time = '2020/10/26 15:50:00' \n",
    "end_time = '2020/10/26 16:50:00'\n",
    "#start_time = '2020/10/26 04:36:45'\n",
    "#end_time = '2020/10/26 04:37:09'\n",
    "sample_output_filename = ['./EVR_T23_Car46_261020/preprocessing_output/EVR_20201026_1550_to_20201026_1650.csv']\n",
    "\n",
    "\n",
    "# input_folder_path = './Train 23 14NOV2019 Work Folder/raw logs'\n",
    "# #input_folder_path = './Train 23 14NOV2019 Work Folder/raw logs'\n",
    "# start_time = '2019/11/14 17:00:00' \n",
    "# end_time = '2019/11/14 18:00:00'\n",
    "# sample_output_filename = ['./Train 23 14NOV2019 Work Folder/preprocessing_output/EVR_20191114_1700_to_20191114_1800.csv']\n",
    "\n",
    "def process_evr (input_folder_path, start_time, end_time):\n",
    "    # Determine time difference between start and end time\n",
    "    datetime_format = '%Y/%m/%d %H:%M:%S'\n",
    "    start_time = datetime.datetime.strptime(start_time, datetime_format)\n",
    "    end_time = datetime.datetime.strptime(end_time, datetime_format)\n",
    "    start_time_new = start_time.strftime('%m/%d/%Y %H:%M:%S')\n",
    "    end_time_new = end_time.strftime('%m/%d/%Y %H:%M:%S')\n",
    "    # Check if EVR folder exist\n",
    "    if not os.path.exists(os.path.join(input_folder_path, 'EVR')):\n",
    "        print(\"EVR folder does not exist. Kindly check the folder strucutre. No EVR processing will be performed\")\n",
    "    else:\n",
    "        print(\"EVR folder exist!\")\n",
    "        # Check if C2 file exist\n",
    "        c2_pattern = 'EVR_Car\\d+_\\d+_C2.txt'\n",
    "        norm_pattern = 'EVR_Car\\d+_\\d+.txt'\n",
    "        c2_flag = False\n",
    "        norm_flag = False\n",
    "        for evr_file in os.listdir(os.path.join(input_folder_path, 'EVR')):\n",
    "            if re.search(c2_pattern, evr_file):\n",
    "                c2_filename = evr_file\n",
    "                c2_flag = True\n",
    "            if re.search(norm_pattern,evr_file):\n",
    "                norm_filename = evr_file\n",
    "                norm_flag = True\n",
    "\n",
    "        if not (c2_flag and norm_flag):\n",
    "            sys.exit(\"Missing EVR files! Kindly check EVR folder!\")\n",
    "        else:\n",
    "            # Process C2 file\n",
    "            df_c2 = pd.read_csv(os.path.join(input_folder_path, 'EVR', c2_filename), sep=\";\")\n",
    "            df_c2['Date'] = pd.to_datetime(df_c2['Date'], format='%m/%d/%Y %H:%M:%S')\n",
    "            df_c2_period = df_c2.loc[(df_c2['Date'] >= start_time_new) & (df_c2['Date'] <= end_time_new)]\n",
    "            #df_c2_clean_timestamp = preprocess_timestamp_old(df_c2_period)\n",
    "            df_c2_clean = process_dataframe(df_c2_period, 'C2')\n",
    "            #df_c2_clean.to_csv('./result_c2.csv', index=False, header=True)\n",
    "            \n",
    "            # Processs Operating Data file\n",
    "            df_od = pd.read_csv(os.path.join(input_folder_path, 'EVR', norm_filename), sep=\";\")\n",
    "            df_od['Date'] = pd.to_datetime(df_od['Date'], format='%m/%d/%Y %H:%M:%S')\n",
    "            df_od_period = df_od.loc[(df_od['Date'] >= start_time_new) & (df_od['Date'] <= end_time_new)]\n",
    "            #df_od_clean_timestamp = preprocess_timestamp_old(df_od_period)\n",
    "            df_od_clean = process_dataframe(df_od_period, 'OD')\n",
    "            #df_od_clean.to_csv('./result_od.csv', index=False, header=True)\n",
    "            \n",
    "            # Merge other Operating Data to C2 Dataframe based on Timestampms (ordered manner)\n",
    "            df_result = pd.merge_ordered(df_c2_clean, df_od_clean, how='outer', on='Timestampms')\n",
    "            df_result.to_csv('./result_4.csv', index=False, header=True)\n",
    "            # Merge C2 and OD files first\n",
    "            \n",
    "            # Clean Timestamp\n",
    "            df_result = preprocess_timestamp(df_result)\n",
    "            \n",
    "            \n",
    "    return df_c2_clean, df_od_clean, df_result\n",
    "\n",
    "\n",
    "df_c2_clean, df_od_clean, df_result = process_evr(input_folder_path, start_time, end_time)\n",
    "\n",
    "#unit_test(sample_output_filename[0], 'C2', df_c2_clean)  \n",
    "#unit_test(sample_output_filename[0], 'OD', df_od_clean)  \n",
    "unit_test_all(sample_output_filename[0], df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe(df, name):\n",
    "    # Drop columns with NaNs\n",
    "    df = df.dropna(how='any',axis=1)\n",
    "            \n",
    "    # Sort df based on column 'Date'\n",
    "    df = df.sort_values(by='Date',ascending=True).reset_index(drop=True)\n",
    "    \n",
    "    # Convert datetime to timestamp (Unix) in milisecond resolution\n",
    "    df['Timestamp'] = df.Date.values.astype(np.int64) // 10 ** 9 *1000\n",
    "    # Add column for Real_Time Timestampms\n",
    "    df['Timestampms'] = df['Timestamp']\n",
    "    \n",
    "     # Find duplicates datetime in column 'Date'\n",
    "    df_duplicate = df[df.duplicated('Timestampms',keep=False)]\n",
    "    for index, unique_datetime in enumerate (df_duplicate['Timestampms'].unique()):\n",
    "        timestamp_df = df_duplicate.loc[df_duplicate['Timestampms'] == unique_datetime]\n",
    "        if index < len(df_duplicate['Timestampms'].unique())-1:\n",
    "            timestamp_df_next = df_duplicate.loc[df_duplicate['Timestampms'] == df_duplicate['Timestampms'].unique()[index+1]]\n",
    "        if index > 0:\n",
    "             timestamp_df_previous = df_duplicate.loc[df_duplicate['Timestampms'] == df_duplicate['Timestampms'].unique()[index-1]]\n",
    "        difference_previous, difference_next = 0 , 0\n",
    "        if timestamp_df.index[0]-1 >=0 and timestamp_df.index[-1]+1 <= len(df)-1:\n",
    "            difference_previous = unique_datetime - df.loc[timestamp_df.index[0]-1,'Timestampms']\n",
    "            difference_next = df.loc[timestamp_df.index[-1]+1,'Timestampms'] - unique_datetime\n",
    "        elif timestamp_df.index[-1] == len(df)-1:\n",
    "            difference_previous = unique_datetime - df.loc[timestamp_df.index[0]-1,'Timestampms']\n",
    "        elif timestamp_df.index[0] == 0:\n",
    "            difference_next = df.loc[timestamp_df.index[-1]+1,'Timestampms'] - unique_datetime\n",
    "            \n",
    "        #print(timestamp_df.index)\n",
    "        if df.loc[timestamp_df.index[0],'Record Number'] < df.loc[timestamp_df.index[-1],'Record Number']:\n",
    "            if (difference_previous == 1000 and difference_next >= 2000) and (timestamp_df_next.index[0]-timestamp_df.index[-1]==1):\n",
    "                print(\"---Handling Duplicates, Fill gap with duplicate, Forward---\")\n",
    "                for counter in range (0, len(timestamp_df.index)):\n",
    "                    df.loc[timestamp_df.index[counter],'Timestampms'] = df.loc[timestamp_df.index[counter],'Timestampms'] + counter*1000\n",
    "                    print(df.loc[timestamp_df.index[counter],'Record Number'], df.loc[timestamp_df.index[counter],'Date'], df.loc[timestamp_df.index[counter],'Timestampms'])\n",
    "            elif(difference_previous >= 2000 and difference_next == 1000):\n",
    "                # Loop backwards\n",
    "                print(\"---Handling Duplicates, Fill gap with duplicate, Backward---\")\n",
    "                count = 0\n",
    "                for counter in range (len(timestamp_df.index), 0, -1):\n",
    "                    df.loc[timestamp_df.index[counter-1],'Timestampms'] = df.loc[timestamp_df.index[counter-1],'Timestampms'] - count*1000\n",
    "                    count +=1\n",
    "                    print(df.loc[timestamp_df.index[counter-1],'Record Number'], df.loc[timestamp_df.index[counter-1],'Date'], df.loc[timestamp_df.index[counter-1],'Timestampms'])\n",
    "            #Implement fill preceding gap first\n",
    "            elif(difference_previous == difference_next) and (difference_previous>1000 and difference_next>1000):\n",
    "                print(\"---Handling Duplicates, Fill gap with duplicate, Forward and Backward---\")\n",
    "                mid_point = len(timestamp_df.index) // 2\n",
    "                left_count, right_count = 0, 0\n",
    "                for counter in range (1, len(timestamp_df.index)):\n",
    "                    if counter % 2 == 0:\n",
    "                        right_count +=1\n",
    "                        df.loc[timestamp_df.index[mid_point+right_count],'Timestampms'] = df.loc[timestamp_df.index[mid_point],'Timestampms'] + right_count*1000\n",
    "                        print(df.loc[timestamp_df.index[mid_point+right_count],'Record Number'], df.loc[timestamp_df.index[mid_point+right_count],'Date'], df.loc[timestamp_df.index[mid_point+right_count],'Timestampms'])\n",
    "                    else:\n",
    "                        left_count +=1\n",
    "                        df.loc[timestamp_df.index[mid_point-left_count],'Timestampms'] = df.loc[timestamp_df.index[mid_point],'Timestampms'] - left_count*1000\n",
    "                        print(df.loc[timestamp_df.index[mid_point-left_count],'Record Number'], df.loc[timestamp_df.index[mid_point-left_count],'Date'], df.loc[timestamp_df.index[mid_point-left_count],'Timestampms'])\n",
    "            elif not(difference_previous == 0 and difference_next == 0) and ((difference_previous == difference_next) or \n",
    "                                                                       (difference_previous <= 1000 and difference_next <= 1000) or\n",
    "                                                                       (difference_previous == 1000 and difference_next == 0) or\n",
    "                                                                       (difference_previous == 0 and difference_next == 1000)):\n",
    "                print(\"---Handling Duplicates, Split duplicate evenly---\")\n",
    "                # For every Timestamp (1000 millisecond), find the interval\n",
    "                interval_ms = round(1000 / len(timestamp_df.index))\n",
    "                for counter in range (0, len(timestamp_df.index)):\n",
    "                    df.loc[timestamp_df.index[counter],'Timestampms'] = df.loc[timestamp_df.index[counter],'Timestampms'] + counter*interval_ms\n",
    "                    print(df.loc[timestamp_df.index[counter],'Record Number'], df.loc[timestamp_df.index[counter],'Date'], df.loc[timestamp_df.index[counter],'Timestampms'])            \n",
    "        else:\n",
    "            if not(difference_previous == 0 and difference_next == 0) and ((difference_previous == difference_next) or \n",
    "                                                                       (difference_previous <= 1000 and difference_next <= 1000) or\n",
    "                                                                       (difference_previous == 1000 and difference_next == 0) or\n",
    "                                                                       (difference_previous == 0 and difference_next == 1000)):\n",
    "                print(\"---Handling Duplicates, Split duplicate evenly---\")\n",
    "                # For every Timestamp (1000 millisecond), find the interval\n",
    "                interval_ms = round(1000 / len(timestamp_df.index))\n",
    "                for counter in range (0, len(timestamp_df.index)):\n",
    "                    df.loc[timestamp_df.index[counter],'Timestampms'] = df.loc[timestamp_df.index[counter],'Timestampms'] + counter*interval_ms\n",
    "                    print(df.loc[timestamp_df.index[counter],'Record Number'], df.loc[timestamp_df.index[counter],'Date'], df.loc[timestamp_df.index[counter],'Timestampms'])\n",
    "        \n",
    "    # Add Prefix to Columns name (e.g. EC2/EOD_***) except Timestampms\n",
    "    if name == 'C2':\n",
    "        df.columns = df.columns.map(lambda x : 'EC2_'+x if x !='Timestampms' else x)\n",
    "    elif name == 'OD':\n",
    "        df.columns = df.columns.map(lambda x : 'EOD_'+x if x !='Timestampms' else x)\n",
    "        \n",
    "    # Shift Timestampms to first column\n",
    "    df = df[ ['Timestampms'] + [ col for col in df.columns if col != 'Timestampms' ] ]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def preprocess_timestamp(df):\n",
    "    # Find duplicates datetime in colum 'Date'\n",
    "    df_duplicate = df[df.duplicated('Timestampms',keep=False)]\n",
    "    for unique_datetime in df_duplicate['Timestampms'].unique():\n",
    "        timestamp_df = df_duplicate.loc[df_duplicate['Timestampms'] == unique_datetime]\n",
    "        difference_previous, difference_next = 0 , 0\n",
    "        #print(unique_datetime)\n",
    "        if timestamp_df.index[0]-1 >=0 and timestamp_df.index[-1]+1 <= len(df)-1:\n",
    "            difference_previous = unique_datetime - df.loc[timestamp_df.index[0]-1,'Timestampms']\n",
    "            difference_next = df.loc[timestamp_df.index[-1]+1,'Timestampms'] - unique_datetime\n",
    "            #print(difference_previous, difference_next)\n",
    "        \n",
    "        elif timestamp_df.index[-1] == len(df)-1:\n",
    "            difference_previous = unique_datetime - df.loc[timestamp_df.index[0]-1,'Timestampms']\n",
    "        elif timestamp_df.index[0] == 0:\n",
    "            difference_next = df.loc[timestamp_df.index[-1]+1,'Timestampms'] - unique_datetime\n",
    "        # Check if both (index[0]-1) and (index[-1]+1) are only difference of 1000 millisecond apart respectively\n",
    "        print(difference_previous, difference_next)\n",
    "        if not(difference_previous == 0 and difference_next == 0) and ((difference_previous == difference_next) or \n",
    "                                                                       (difference_previous <= 1000 and difference_next <= 1000) or\n",
    "                                                                       (difference_previous == 1000 and difference_next == 0) or\n",
    "                                                                       (difference_previous == 0 and difference_next == 1000)):\n",
    "            print(\"---Handling Duplicates, Split duplicate evenly---\")\n",
    "            #print(difference_previous, difference_next)\n",
    "            # Handle duplicates - Split duplicate evenly\n",
    "            # For every Timestamp (1000 millisecond), find the interval\n",
    "            interval_ms = round(1000 / len(timestamp_df.index))\n",
    "            for counter in range (0, len(timestamp_df.index)):\n",
    "                df.loc[timestamp_df.index[counter],'Timestampms'] = df.loc[timestamp_df.index[counter],'Timestampms'] + counter*interval_ms\n",
    "                print(df.loc[timestamp_df.index[counter],'EC2_Record Number'], df.loc[timestamp_df.index[counter],'EC2_Date'], df.loc[timestamp_df.index[counter],'Timestampms'])\n",
    "\n",
    "#         elif difference_previous == 1000 and difference_next >= 2000:\n",
    "#             print(\"---Handling Duplicates, Fill gap with duplicate---\")\n",
    "#             for counter in range (0, len(timestamp_df.index)):\n",
    "#                 df.loc[timestamp_df.index[counter],'Timestampms'] = df.loc[timestamp_df.index[counter],'Timestampms'] + counter*1000\n",
    "#                 print(df.loc[timestamp_df.index[counter],'EC2_Record Number'], df.loc[timestamp_df.index[counter],'EC2_Date'], df.loc[timestamp_df.index[counter],'Timestampms'])\n",
    "    \n",
    "    # Group resulting same real timestamp together\n",
    "    df = df.groupby(\"Timestampms\").last().reset_index()\n",
    "    \n",
    "    # Remove similar record numbers for both EC2 and EOD\n",
    "    #df_duplicate = df[df.duplicated('EC2_Record Number',keep='first')]\n",
    "    filter_col = [col for col in df if col.startswith('EC2_')]\n",
    "    df.loc[df.duplicated('EC2_Record Number',keep='first'), filter_col] = np.NaN\n",
    "    filter_col = [col for col in df if col.startswith('EOD_')]\n",
    "    df.loc[df.duplicated('EOD_Record Number',keep='first'), filter_col] = np.NaN\n",
    "    \n",
    "    df.to_csv('./result_2.csv', index=False, header=True)\n",
    "    #print(filter_col)\n",
    "#     print(df.head())\n",
    "#     print(df.loc[8, 'Timestampms'], df.loc[8, 'EC2_Record Number'], df.loc[8, 'EOD_Record Number'])\n",
    "#     print(df.loc[9, 'Timestampms'],df.loc[9, 'EC2_Record Number'], df.loc[9, 'EOD_Record Number'])\n",
    "#     print(df.loc[10, 'Timestampms'],df.loc[10, 'EC2_Record Number'], df.loc[10, 'EOD_Record Number'])\n",
    "        \n",
    "        \n",
    "    return df\n",
    "\n",
    "def unit_test(sample_output_filename, name, df_test):\n",
    "    print(\"---Unit Test for \" + name + \" Dataframe---\")\n",
    "    # Unit Test for C2\n",
    "    # Import Output File\n",
    "    df_output = pd.read_csv(sample_output_filename)\n",
    "    # Only retrieve respective columns\n",
    "    if name == 'C2':\n",
    "        df_output = df_output.drop(df_output[(df_output['EC2_002_Record_Number'].isnull())].index)\n",
    "        #drop column with prefix EOD\n",
    "        df_output = df_output.loc[:, ~df_output.columns.str.startswith('EOD')]\n",
    "    elif name == 'OD':\n",
    "        df_output = df_output.drop(df_output[(df_output['EOD_002_Record_Number'].isnull())].index)\n",
    "        #drop column with prefix EOD\n",
    "        df_output = df_output.loc[:, ~df_output.columns.str.startswith('EC2')]\n",
    "    \n",
    "    df_output['epoch'] = df_output.epoch.values.astype(np.float64)\n",
    "    df_output = df_output.reset_index(drop=True)\n",
    "    print(df_output.shape, df_test.shape)    \n",
    "    #Output to CSV\n",
    "    #df_output.to_csv('./df_output.csv', index=False, header=True)\n",
    "\n",
    "    #Output to CSV\n",
    "    #df_test.to_csv('./df_test.csv', index=False, header=True)\n",
    "    #df_drop_test = pd.read_csv('./df_test.csv')\n",
    "    df_drop_test = df_test\n",
    "    df_test['result'] = np.where(df_test['Timestampms'] == df_output['epoch'], 'True', 'False')\n",
    "    df_test.to_csv('./df_test.csv', index=False, header=True)\n",
    "    #df_drop_test = df_drop_test.sort_values(by='ATO_Real_Timestampms',ascending=True).reset_index(drop=True)\n",
    "    # Assert whether sample output and self processed are equal\n",
    "    assert_equal = omap.nan_equal(df_drop_test['Timestampms'].values, df_output['epoch'].values)\n",
    "    print(\"Equality Between Sample Output and Self Processed: \", assert_equal)\n",
    "    assert_equal = omap.nan_equal(df_drop_test['E'+name+'_Record Number'].values, df_output['E'+name+'_002_Record_Number'].values)\n",
    "    print(\"Equality Between Sample Output and Self Processed: \", assert_equal)\n",
    "    \n",
    "    #df_drop_test.columns = df_output.columns\n",
    "    #print(np.testing.assert_allclose(df_drop_test.values, df_output.values, rtol=1e-10, atol=0))\n",
    "    #print(pd.testing.assert_frame_equal(df_drop_test, df_output, check_dtype=False))\n",
    "    #print(df_drop_test.compare(df_output, align_axis=0))\n",
    "    #assert_equal = nan_equal(df_drop_test.values, df_output.values)\n",
    "    #assert_equal = nan_equal(df_drop['ATO_* General'].values, df_output['ATO_0101__General'].values)\n",
    "    #print(\"Equality Between Sample Output and Self Processed: \", assert_equal)\n",
    "    #print(np.testing.assert_equal(df_drop_test.values, df_output.values))\n",
    "    \n",
    "def unit_test_all(sample_output_filename, df_test):\n",
    "    print(\"---Unit Test for entire Dataframe---\")\n",
    "    # Import Output File\n",
    "    df_output = pd.read_csv(sample_output_filename)\n",
    "    print(df_output.shape, df_test.shape)\n",
    "    assert_equal = omap.nan_equal(df_test['Timestampms'].values, df_output['epoch'].values)\n",
    "    print(\"Equality Between Sample Output and Self Processed: \", assert_equal)\n",
    "    df_test['result'] = np.where(df_test['Timestampms'] == df_output['epoch'], 'True', 'False')\n",
    "    #df_test.to_csv('./df_test.csv', index=False, header=True)\n",
    "        \n",
    "    df_test['result_1'] = np.where(df_test['EC2_Record Number'] == df_output['EC2_002_Record_Number'], 'True', 'False')\n",
    "    df_test.to_csv('./df_test.csv', index=False, header=True)\n",
    "    assert_equal = omap.nan_equal(df_test['EC2_Record Number'].values, df_output['EC2_002_Record_Number'].values)\n",
    "    print(\"Equality Between Sample Output and Self Processed: \", assert_equal)\n",
    "    assert_equal = omap.nan_equal(df_test['EOD_Record Number'].values, df_output['EOD_002_Record_Number'].values)\n",
    "    print(\"Equality Between Sample Output and Self Processed: \", assert_equal)\n",
    "    print(np.testing.assert_equal(df_test['EC2_Record Number'].values, df_output['EC2_002_Record_Number'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_timestamp_old(df):\n",
    "    # Drop columns with NaNs\n",
    "    df = df.dropna(how='any',axis=1)\n",
    "            \n",
    "    # Sort df based on column 'Date'\n",
    "    df = df.sort_values(by='Date',ascending=True).reset_index(drop=True)\n",
    "\n",
    "    # Convert datetime to timestamp (Unix) in milisecond resolution\n",
    "    df['Timestamp'] = df.Date.values.astype(np.int64) // 10 ** 9 *1000\n",
    "    # Add column for Real_Time Timestampms\n",
    "    df['Timestampms'] = df['Timestamp']\n",
    "\n",
    "    # Find duplicates datetime in colum 'Date'\n",
    "    df_duplicate = df[df.duplicated('Timestamp',keep=False)]\n",
    "    for unique_datetime in df_duplicate['Timestamp'].unique():\n",
    "        timestamp_df = df_duplicate.loc[df_duplicate['Timestamp'] == unique_datetime]\n",
    "        difference_previous, difference_next = 0 , 0\n",
    "        #print(unique_datetime)\n",
    "        if timestamp_df.index[0]-1 >=0 and timestamp_df.index[-1]+1 <= len(df)-1:\n",
    "            difference_previous = unique_datetime - df.loc[timestamp_df.index[0]-1,'Timestamp']\n",
    "            difference_next = df.loc[timestamp_df.index[-1]+1,'Timestamp'] - unique_datetime\n",
    "            #print(difference_previous, difference_next)\n",
    "        \n",
    "        elif timestamp_df.index[-1] == len(df)-1:\n",
    "            difference_previous = unique_datetime - df.loc[timestamp_df.index[0]-1,'Timestamp']\n",
    "        elif timestamp_df.index[0] == 0:\n",
    "            difference_next = df.loc[timestamp_df.index[-1]+1,'Timestamp'] - unique_datetime\n",
    "        # Check if both (index[0]-1) and (index[-1]+1) are only difference of 1000 millisecond apart respectively\n",
    "        print(difference_previous, difference_next)\n",
    "        if not(difference_previous == 0 and difference_next == 0) and ((difference_previous == difference_next) or \n",
    "                                                                       (difference_previous == 1000 and difference_next == 1000) or\n",
    "                                                                       (difference_previous == 1000 and difference_next == 0) or\n",
    "                                                                       (difference_previous == 0 and difference_next == 1000)):\n",
    "            print(\"---Handling Duplicates, Split duplicate evenly---\")\n",
    "            #print(difference_previous, difference_next)\n",
    "            # Handle duplicates - Split duplicate evenly\n",
    "            # For every Timestamp (1000 millisecond), find the interval\n",
    "            interval_ms = round(1000 / len(timestamp_df.index))\n",
    "            for counter in range (0, len(timestamp_df.index)):\n",
    "                df.loc[timestamp_df.index[counter],'Timestampms'] = df.loc[timestamp_df.index[counter],'Timestampms'] + counter*interval_ms\n",
    "                print(df.loc[timestamp_df.index[counter],'Record Number'], df.loc[timestamp_df.index[counter],'Date'], df.loc[timestamp_df.index[counter],'Timestampms'])\n",
    "        elif difference_previous == 1000 and difference_next >= 2000:\n",
    "            print(\"---Handling Duplicates, Fill gap with duplicate---\")\n",
    "            for counter in range (0, len(timestamp_df.index)):\n",
    "                df.loc[timestamp_df.index[counter],'Timestampms'] = df.loc[timestamp_df.index[counter],'Timestampms'] + counter*1000\n",
    "                print(df.loc[timestamp_df.index[counter],'Record Number'], df.loc[timestamp_df.index[counter],'Date'], df.loc[timestamp_df.index[counter],'Timestampms'])\n",
    "                \n",
    "    # Add Prefix to Columns name (ATO_***) except Timestampms\n",
    "    #df.columns = df.columns.map(lambda x : 'ATO_'+x if x !='Timestampms' and x!='Real_Timestampms' else x)\n",
    "    # Shift Timestampms to first column\n",
    "    df = df[ ['Timestampms'] + [ col for col in df.columns if col != 'Timestampms' ] ]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Find duplicates datetime in column 'Date'\n",
    "    df_duplicate = df[df.duplicated('Timestampms',keep=False)]\n",
    "    for index in range (0, len(df_duplicate['Timestampms'].unique())-1):\n",
    "        timestamp_df = df_duplicate.loc[df_duplicate['Timestampms'] == df_duplicate['Timestampms'].unique()[index]]\n",
    "        #print(timestamp_df.index)\n",
    "        timestamp_df_next = df_duplicate.loc[df_duplicate['Timestampms'] == df_duplicate['Timestampms'].unique()[index+1]]\n",
    "        # Duplicates are side by side\n",
    "        if(timestamp_df_next.index[0]-timestamp_df.index[-1]==1) and (df.loc[timestamp_df.index[0],'Record Number'] < df.loc[timestamp_df.index[-1],'Record Number']):\n",
    "            print(\"---Handling Duplicates, Fill gap with duplicate---\")\n",
    "            for counter in range (0, len(timestamp_df.index)):\n",
    "                df.loc[timestamp_df.index[counter],'Timestampms'] = df.loc[timestamp_df.index[counter],'Timestampms'] + counter*1000\n",
    "                print(df.loc[timestamp_df.index[counter],'Record Number'], df.loc[timestamp_df.index[counter],'Date'], df.loc[timestamp_df.index[counter],'Timestampms'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
