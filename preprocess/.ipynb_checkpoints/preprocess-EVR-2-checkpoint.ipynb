{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import timeit\n",
    "tqdm.pandas()\n",
    "import omap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-26 16:32:25\n",
      "2020-10-26 16:26:04\n",
      "2018-02-26 00:00:00\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "ts = int(\"1603729945\")\n",
    "\n",
    "# if you encounter a \"year is out of range\" error the timestamp\n",
    "# may be in milliseconds, try `ts /= 1000` in that case\n",
    "print(datetime.utcfromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "ts = int(\"1603729564\")\n",
    "\n",
    "# if you encounter a \"year is out of range\" error the timestamp\n",
    "# may be in milliseconds, try `ts /= 1000` in that case\n",
    "print(datetime.utcfromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "ts = int(\"1519603200\")\n",
    "\n",
    "# if you encounter a \"year is out of range\" error the timestamp\n",
    "# may be in milliseconds, try `ts /= 1000` in that case\n",
    "print(datetime.utcfromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVR folder exist!\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "31753 2020-10-26 15:50:08 1603727408000 8183.0\n",
      "31753 2020-10-26 15:50:08 1603727408500 8182.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "31756 2020-10-26 15:50:11 1603727411000 8186.0\n",
      "31756 2020-10-26 15:50:11 1603727411500 8185.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "31766 2020-10-26 15:50:21 1603727421000 4.0\n",
      "31766 2020-10-26 15:50:21 1603727421500 3.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "31769 2020-10-26 15:50:24 1603727424000 7.0\n",
      "31769 2020-10-26 15:50:24 1603727424500 6.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "31771 2020-10-26 15:50:26 1603727426000 9.0\n",
      "31771 2020-10-26 15:50:26 1603727426500 8.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "31788 2020-10-26 15:50:43 1603727443000 26.0\n",
      "31788 2020-10-26 15:50:43 1603727443500 25.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "31790 2020-10-26 15:50:45 1603727445000 28.0\n",
      "31790 2020-10-26 15:50:45 1603727445500 27.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "31808 2020-10-26 15:51:03 1603727463000 46.0\n",
      "31808 2020-10-26 15:51:03 1603727463500 45.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "31810 2020-10-26 15:51:05 1603727465000 48.0\n",
      "31810 2020-10-26 15:51:05 1603727465500 47.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "31812 2020-10-26 15:51:07 1603727467000 50.0\n",
      "31812 2020-10-26 15:51:07 1603727467500 49.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "31830 2020-10-26 15:51:25 1603727485000 67.0\n",
      "31830 2020-10-26 15:51:25 1603727485500 68.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "31833 2020-10-26 15:51:28 1603727488000 71.0\n",
      "31833 2020-10-26 15:51:28 1603727488500 70.0\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward 2---\n",
      "31848 2020-10-26 15:51:43 1603727503000 85.0\n",
      "31848 2020-10-26 15:51:43 1603727504000 86.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "31850 2020-10-26 15:51:45 1603727505000 88.0\n",
      "31850 2020-10-26 15:51:45 1603727505500 87.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "31870 2020-10-26 15:52:05 1603727525000 108.0\n",
      "31870 2020-10-26 15:52:05 1603727525500 107.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "31872 2020-10-26 15:52:07 1603727527000 110.0\n",
      "31872 2020-10-26 15:52:07 1603727527500 109.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "31877 2020-10-26 15:52:12 1603727532000 115.0\n",
      "31877 2020-10-26 15:52:12 1603727532500 114.0\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward 2---\n",
      "31890 2020-10-26 15:52:25 1603727545000 127.0\n",
      "31890 2020-10-26 15:52:25 1603727546000 128.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "31892 2020-10-26 15:52:27 1603727547000 129.0\n",
      "31892 2020-10-26 15:52:27 1603727547500 130.0\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward 2---\n",
      "31907 2020-10-26 15:52:42 1603727562000 144.0\n",
      "31907 2020-10-26 15:52:42 1603727563000 145.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "31911 2020-10-26 15:52:46 1603727566000 148.0\n",
      "31911 2020-10-26 15:52:46 1603727566500 149.0\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward 2---\n",
      "31918 2020-10-26 15:52:53 1603727573000 155.0\n",
      "31918 2020-10-26 15:52:53 1603727574000 156.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "31927 2020-10-26 15:53:02 1603727582000 165.0\n",
      "31927 2020-10-26 15:53:02 1603727582500 164.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "31953 2020-10-26 15:53:28 1603727608000 191.0\n",
      "31953 2020-10-26 15:53:28 1603727608500 190.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "31985 2020-10-26 15:54:00 1603727640000 223.0\n",
      "31985 2020-10-26 15:54:00 1603727640500 222.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32022 2020-10-26 15:54:37 1603727677000 260.0\n",
      "32022 2020-10-26 15:54:37 1603727677500 259.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32027 2020-10-26 15:54:42 1603727682000 265.0\n",
      "32027 2020-10-26 15:54:42 1603727682500 264.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32030 2020-10-26 15:54:45 1603727685000 268.0\n",
      "32030 2020-10-26 15:54:45 1603727685500 267.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32033 2020-10-26 15:54:48 1603727688000 270.0\n",
      "32033 2020-10-26 15:54:48 1603727688500 271.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32036 2020-10-26 15:54:51 1603727691000 273.0\n",
      "32036 2020-10-26 15:54:51 1603727691500 274.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32039 2020-10-26 15:54:54 1603727694000 277.0\n",
      "32039 2020-10-26 15:54:54 1603727694500 276.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32045 2020-10-26 15:55:00 1603727700000 282.0\n",
      "32045 2020-10-26 15:55:00 1603727700500 283.0\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward 2---\n",
      "32054 2020-10-26 15:55:09 1603727709000 291.0\n",
      "32054 2020-10-26 15:55:09 1603727710000 292.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32059 2020-10-26 15:55:14 1603727714000 296.0\n",
      "32059 2020-10-26 15:55:14 1603727714500 297.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32063 2020-10-26 15:55:18 1603727718000 300.0\n",
      "32063 2020-10-26 15:55:18 1603727718500 301.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32070 2020-10-26 15:55:25 1603727725000 308.0\n",
      "32070 2020-10-26 15:55:25 1603727725500 307.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32072 2020-10-26 15:55:27 1603727727000 310.0\n",
      "32072 2020-10-26 15:55:27 1603727727500 309.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32079 2020-10-26 15:55:34 1603727734000 317.0\n",
      "32079 2020-10-26 15:55:34 1603727734500 316.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32087 2020-10-26 15:55:42 1603727742000 325.0\n",
      "32087 2020-10-26 15:55:42 1603727742500 324.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32096 2020-10-26 15:55:51 1603727751000 334.0\n",
      "32096 2020-10-26 15:55:51 1603727751500 333.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32130 2020-10-26 15:56:25 1603727785000 367.0\n",
      "32130 2020-10-26 15:56:25 1603727785500 368.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32142 2020-10-26 15:56:37 1603727797000 380.0\n",
      "32142 2020-10-26 15:56:37 1603727797500 379.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32147 2020-10-26 15:56:42 1603727802000 384.0\n",
      "32147 2020-10-26 15:56:42 1603727802500 385.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32150 2020-10-26 15:56:45 1603727805000 388.0\n",
      "32150 2020-10-26 15:56:45 1603727805500 387.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32153 2020-10-26 15:56:48 1603727808000 391.0\n",
      "32153 2020-10-26 15:56:48 1603727808500 390.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32156 2020-10-26 15:56:51 1603727811000 394.0\n",
      "32156 2020-10-26 15:56:51 1603727811500 393.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32159 2020-10-26 15:56:54 1603727814000 397.0\n",
      "32159 2020-10-26 15:56:54 1603727814500 396.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32162 2020-10-26 15:56:57 1603727817000 400.0\n",
      "32162 2020-10-26 15:56:57 1603727817500 399.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32165 2020-10-26 15:57:00 1603727820000 403.0\n",
      "32165 2020-10-26 15:57:00 1603727820500 402.0\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward 2---\n",
      "32168 2020-10-26 15:57:03 1603727823000 405.0\n",
      "32168 2020-10-26 15:57:03 1603727824000 406.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32171 2020-10-26 15:57:06 1603727826000 409.0\n",
      "32171 2020-10-26 15:57:06 1603727826500 408.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32176 2020-10-26 15:57:11 1603727831000 414.0\n",
      "32176 2020-10-26 15:57:11 1603727831500 413.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32207 2020-10-26 15:57:42 1603727862000 444.0\n",
      "32207 2020-10-26 15:57:42 1603727862500 445.0\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward 2---\n",
      "32220 2020-10-26 15:57:55 1603727875000 457.0\n",
      "32220 2020-10-26 15:57:55 1603727876000 458.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32222 2020-10-26 15:57:57 1603727877000 459.0\n",
      "32222 2020-10-26 15:57:57 1603727877500 460.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32239 2020-10-26 15:58:14 1603727894000 476.0\n",
      "32240 2020-10-26 15:58:14 1603727894500 476.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32241 2020-10-26 15:58:15 1603727895000 478.0\n",
      "32241 2020-10-26 15:58:15 1603727895500 477.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32243 2020-10-26 15:58:17 1603727897000 480.0\n",
      "32243 2020-10-26 15:58:17 1603727897500 479.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32248 2020-10-26 15:58:22 1603727902000 485.0\n",
      "32249 2020-10-26 15:58:22 1603727902333 485.0\n",
      "32250 2020-10-26 15:58:22 1603727902666 485.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32259 2020-10-26 15:58:31 1603727911000 nan\n",
      "32260 2020-10-26 15:58:31 1603727911500 nan\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32264 2020-10-26 15:58:35 1603727915000 498.0\n",
      "32264 2020-10-26 15:58:35 1603727915500 497.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32266 2020-10-26 15:58:37 1603727917000 499.0\n",
      "32266 2020-10-26 15:58:37 1603727917500 500.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32271 2020-10-26 15:58:42 1603727922000 504.0\n",
      "32271 2020-10-26 15:58:42 1603727922500 505.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32277 2020-10-26 15:58:48 1603727928000 510.0\n",
      "32277 2020-10-26 15:58:48 1603727928500 511.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32280 2020-10-26 15:58:51 1603727931000 513.0\n",
      "32280 2020-10-26 15:58:51 1603727931500 514.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32283 2020-10-26 15:58:54 1603727934000 516.0\n",
      "32284 2020-10-26 15:58:54 1603727934333 516.0\n",
      "32285 2020-10-26 15:58:54 1603727934666 516.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32291 2020-10-26 15:58:56 1603727936000 518.0\n",
      "32290 2020-10-26 15:58:56 1603727936200 518.0\n",
      "32288 2020-10-26 15:58:56 1603727936400 518.0\n",
      "32287 2020-10-26 15:58:56 1603727936600 518.0\n",
      "32289 2020-10-26 15:58:56 1603727936800 518.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32292 2020-10-26 15:58:57 1603727937000 519.0\n",
      "32293 2020-10-26 15:58:57 1603727937200 519.0\n",
      "32294 2020-10-26 15:58:57 1603727937400 519.0\n",
      "32295 2020-10-26 15:58:57 1603727937600 519.0\n",
      "32296 2020-10-26 15:58:57 1603727937800 519.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32300 2020-10-26 15:58:58 1603727938000 520.0\n",
      "32299 2020-10-26 15:58:58 1603727938250 520.0\n",
      "32298 2020-10-26 15:58:58 1603727938500 520.0\n",
      "32297 2020-10-26 15:58:58 1603727938750 520.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32301 2020-10-26 15:58:59 1603727939000 521.0\n",
      "32302 2020-10-26 15:58:59 1603727939500 521.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32303 2020-10-26 15:59:00 1603727940000 522.0\n",
      "32304 2020-10-26 15:59:00 1603727940500 522.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32310 2020-10-26 15:59:06 1603727946000 528.0\n",
      "32311 2020-10-26 15:59:06 1603727946333 528.0\n",
      "32312 2020-10-26 15:59:06 1603727946666 528.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32397 2020-10-26 16:00:31 1603728031000 614.0\n",
      "32397 2020-10-26 16:00:31 1603728031500 613.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32446 2020-10-26 16:01:20 1603728080000 663.0\n",
      "32446 2020-10-26 16:01:20 1603728080500 662.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32501 2020-10-26 16:02:15 1603728135000 718.0\n",
      "32501 2020-10-26 16:02:15 1603728135500 717.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32506 2020-10-26 16:02:20 1603728140000 723.0\n",
      "32506 2020-10-26 16:02:20 1603728140500 722.0\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward 2---\n",
      "32520 2020-10-26 16:02:34 1603728154000 736.0\n",
      "32520 2020-10-26 16:02:34 1603728155000 737.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32521 2020-10-26 16:02:35 1603728155000 nan\n",
      "32522 2020-10-26 16:02:35 1603728155333 nan\n",
      "32523 2020-10-26 16:02:35 1603728155666 nan\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32527 2020-10-26 16:02:36 1603728156000 738.0\n",
      "32526 2020-10-26 16:02:36 1603728156250 738.0\n",
      "32525 2020-10-26 16:02:36 1603728156500 738.0\n",
      "32524 2020-10-26 16:02:36 1603728156750 738.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32528 2020-10-26 16:02:37 1603728157000 739.0\n",
      "32529 2020-10-26 16:02:37 1603728157500 739.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32531 2020-10-26 16:02:39 1603728159000 741.0\n",
      "32532 2020-10-26 16:02:39 1603728159500 741.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32538 2020-10-26 16:02:45 1603728165000 747.0\n",
      "32539 2020-10-26 16:02:45 1603728165333 747.0\n",
      "32540 2020-10-26 16:02:45 1603728165666 747.0\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward 2---\n",
      "32553 2020-10-26 16:02:58 1603728178000 760.0\n",
      "32553 2020-10-26 16:02:58 1603728179000 761.0\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward 2---\n",
      "32573 2020-10-26 16:03:18 1603728198000 780.0\n",
      "32573 2020-10-26 16:03:18 1603728199000 781.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32593 2020-10-26 16:03:38 1603728218000 801.0\n",
      "32593 2020-10-26 16:03:38 1603728218500 800.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32596 2020-10-26 16:03:41 1603728221000 804.0\n",
      "32596 2020-10-26 16:03:41 1603728221500 803.0\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward 2---\n",
      "32613 2020-10-26 16:03:58 1603728238000 820.0\n",
      "32613 2020-10-26 16:03:58 1603728239000 821.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32633 2020-10-26 16:04:18 1603728258000 841.0\n",
      "32633 2020-10-26 16:04:18 1603728258500 840.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32653 2020-10-26 16:04:38 1603728278000 861.0\n",
      "32653 2020-10-26 16:04:38 1603728278500 860.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32658 2020-10-26 16:04:43 1603728283000 866.0\n",
      "32658 2020-10-26 16:04:43 1603728283500 865.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32675 2020-10-26 16:05:00 1603728300000 883.0\n",
      "32675 2020-10-26 16:05:00 1603728300500 882.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32677 2020-10-26 16:05:02 1603728302000 885.0\n",
      "32677 2020-10-26 16:05:02 1603728302500 884.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32680 2020-10-26 16:05:05 1603728305000 888.0\n",
      "32680 2020-10-26 16:05:05 1603728305500 887.0\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward 2---\n",
      "32686 2020-10-26 16:05:11 1603728311000 893.0\n",
      "32686 2020-10-26 16:05:11 1603728312000 894.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32688 2020-10-26 16:05:13 1603728313000 895.0\n",
      "32688 2020-10-26 16:05:13 1603728313500 896.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32691 2020-10-26 16:05:16 1603728316000 899.0\n",
      "32691 2020-10-26 16:05:16 1603728316500 898.0\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward 2---\n",
      "32694 2020-10-26 16:05:19 1603728319000 901.0\n",
      "32694 2020-10-26 16:05:19 1603728320000 902.0\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward 2---\n",
      "32700 2020-10-26 16:05:25 1603728325000 907.0\n",
      "32700 2020-10-26 16:05:25 1603728326000 908.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32703 2020-10-26 16:05:28 1603728328000 911.0\n",
      "32703 2020-10-26 16:05:28 1603728328500 910.0\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward 2---\n",
      "32708 2020-10-26 16:05:33 1603728333000 915.0\n",
      "32708 2020-10-26 16:05:33 1603728334000 916.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32725 2020-10-26 16:05:50 1603728350000 933.0\n",
      "32725 2020-10-26 16:05:50 1603728350500 932.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32783 2020-10-26 16:06:48 1603728408000 991.0\n",
      "32783 2020-10-26 16:06:48 1603728408500 990.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32796 2020-10-26 16:07:01 1603728421000 1004.0\n",
      "32796 2020-10-26 16:07:01 1603728421500 1003.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32805 2020-10-26 16:07:10 1603728430000 1012.0\n",
      "32805 2020-10-26 16:07:10 1603728430500 1013.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32808 2020-10-26 16:07:13 1603728433000 1015.0\n",
      "32808 2020-10-26 16:07:13 1603728433500 1016.0\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward 2---\n",
      "32814 2020-10-26 16:07:19 1603728439000 1021.0\n",
      "32814 2020-10-26 16:07:19 1603728440000 1022.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32817 2020-10-26 16:07:22 1603728442000 1025.0\n",
      "32817 2020-10-26 16:07:22 1603728442500 1024.0\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward 2---\n",
      "32822 2020-10-26 16:07:27 1603728447000 1029.0\n",
      "32822 2020-10-26 16:07:27 1603728448000 1030.0\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward 2---\n",
      "32825 2020-10-26 16:07:30 1603728450000 1032.0\n",
      "32825 2020-10-26 16:07:30 1603728451000 1033.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32829 2020-10-26 16:07:34 1603728454000 1036.0\n",
      "32829 2020-10-26 16:07:34 1603728454500 1037.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32891 2020-10-26 16:08:36 1603728516000 1098.0\n",
      "32891 2020-10-26 16:08:36 1603728516500 1099.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32918 2020-10-26 16:09:03 1603728543000 1126.0\n",
      "32918 2020-10-26 16:09:03 1603728543500 1125.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32923 2020-10-26 16:09:08 1603728548000 1131.0\n",
      "32923 2020-10-26 16:09:08 1603728548500 1130.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32926 2020-10-26 16:09:11 1603728551000 1133.0\n",
      "32926 2020-10-26 16:09:11 1603728551500 1134.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32929 2020-10-26 16:09:14 1603728554000 1137.0\n",
      "32929 2020-10-26 16:09:14 1603728554500 1136.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32932 2020-10-26 16:09:17 1603728557000 1140.0\n",
      "32932 2020-10-26 16:09:17 1603728557500 1139.0\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward 2---\n",
      "32938 2020-10-26 16:09:23 1603728563000 1145.0\n",
      "32938 2020-10-26 16:09:23 1603728564000 1146.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32940 2020-10-26 16:09:25 1603728565000 1148.0\n",
      "32940 2020-10-26 16:09:25 1603728565500 1147.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32952 2020-10-26 16:09:37 1603728577000 1160.0\n",
      "32952 2020-10-26 16:09:37 1603728577500 1159.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32999 2020-10-26 16:10:24 1603728624000 1206.0\n",
      "32999 2020-10-26 16:10:24 1603728624500 1207.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33038 2020-10-26 16:11:03 1603728663000 1246.0\n",
      "33038 2020-10-26 16:11:03 1603728663500 1245.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33049 2020-10-26 16:11:14 1603728674000 1256.0\n",
      "33049 2020-10-26 16:11:14 1603728674500 1257.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33052 2020-10-26 16:11:17 1603728677000 1260.0\n",
      "33052 2020-10-26 16:11:17 1603728677500 1259.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33055 2020-10-26 16:11:20 1603728680000 1263.0\n",
      "33055 2020-10-26 16:11:20 1603728680500 1262.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33058 2020-10-26 16:11:23 1603728683000 1266.0\n",
      "33058 2020-10-26 16:11:23 1603728683500 1265.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33061 2020-10-26 16:11:26 1603728686000 1269.0\n",
      "33061 2020-10-26 16:11:26 1603728686500 1268.0\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward 2---\n",
      "33064 2020-10-26 16:11:29 1603728689000 1271.0\n",
      "33064 2020-10-26 16:11:29 1603728690000 1272.0\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward 2---\n",
      "33067 2020-10-26 16:11:32 1603728692000 1274.0\n",
      "33067 2020-10-26 16:11:32 1603728693000 1275.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33070 2020-10-26 16:11:35 1603728695000 1278.0\n",
      "33070 2020-10-26 16:11:35 1603728695500 1277.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33072 2020-10-26 16:11:37 1603728697000 1280.0\n",
      "33072 2020-10-26 16:11:37 1603728697500 1279.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33109 2020-10-26 16:12:14 1603728734000 1317.0\n",
      "33109 2020-10-26 16:12:14 1603728734500 1316.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33165 2020-10-26 16:13:10 1603728790000 1373.0\n",
      "33165 2020-10-26 16:13:10 1603728790500 1372.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33168 2020-10-26 16:13:13 1603728793000 1375.0\n",
      "33168 2020-10-26 16:13:13 1603728793500 1376.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33171 2020-10-26 16:13:16 1603728796000 1378.0\n",
      "33171 2020-10-26 16:13:16 1603728796500 1379.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33174 2020-10-26 16:13:19 1603728799000 1381.0\n",
      "33174 2020-10-26 16:13:19 1603728799500 1382.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33180 2020-10-26 16:13:25 1603728805000 1388.0\n",
      "33180 2020-10-26 16:13:25 1603728805500 1387.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33182 2020-10-26 16:13:27 1603728807000 1390.0\n",
      "33182 2020-10-26 16:13:27 1603728807500 1389.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33185 2020-10-26 16:13:30 1603728810000 1393.0\n",
      "33185 2020-10-26 16:13:30 1603728810500 1392.0\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward 2---\n",
      "33188 2020-10-26 16:13:33 1603728813000 1395.0\n",
      "33188 2020-10-26 16:13:33 1603728814000 1396.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33191 2020-10-26 16:13:36 1603728816000 1399.0\n",
      "33191 2020-10-26 16:13:36 1603728816500 1398.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33196 2020-10-26 16:13:41 1603728821000 1404.0\n",
      "33196 2020-10-26 16:13:41 1603728821500 1403.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33199 2020-10-26 16:13:44 1603728824000 1407.0\n",
      "33199 2020-10-26 16:13:44 1603728824500 1406.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33217 2020-10-26 16:14:02 1603728842000 1424.0\n",
      "33217 2020-10-26 16:14:02 1603728842500 1425.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33229 2020-10-26 16:14:14 1603728854000 nan\n",
      "33230 2020-10-26 16:14:14 1603728854200 nan\n",
      "33231 2020-10-26 16:14:14 1603728854400 nan\n",
      "33232 2020-10-26 16:14:14 1603728854600 nan\n",
      "33233 2020-10-26 16:14:14 1603728854800 nan\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33236 2020-10-26 16:14:15 1603728855000 1437.0\n",
      "33234 2020-10-26 16:14:15 1603728855333 1437.0\n",
      "33235 2020-10-26 16:14:15 1603728855666 1437.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "4 2\n",
      "0\n",
      "33237 2020-10-26 16:14:16 1603728856000 1438.0\n",
      "0\n",
      "33237 2020-10-26 16:14:16 1603728856000 1439.0\n",
      "1\n",
      "33238 2020-10-26 16:14:16 1603728856250 1438.0\n",
      "1\n",
      "33238 2020-10-26 16:14:16 1603728856250 1439.0\n",
      "2\n",
      "33239 2020-10-26 16:14:16 1603728856500 1438.0\n",
      "2\n",
      "33239 2020-10-26 16:14:16 1603728856500 1439.0\n",
      "3\n",
      "33240 2020-10-26 16:14:16 1603728856750 1438.0\n",
      "3\n",
      "33240 2020-10-26 16:14:16 1603728856750 1439.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33241 2020-10-26 16:14:17 1603728857000 1440.0\n",
      "33242 2020-10-26 16:14:17 1603728857500 1440.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "3 2\n",
      "0\n",
      "33246 2020-10-26 16:14:19 1603728859000 1442.0\n",
      "0\n",
      "33246 2020-10-26 16:14:19 1603728859000 1441.0\n",
      "1\n",
      "33244 2020-10-26 16:14:19 1603728859333 1442.0\n",
      "1\n",
      "33244 2020-10-26 16:14:19 1603728859333 1441.0\n",
      "2\n",
      "33245 2020-10-26 16:14:19 1603728859666 1442.0\n",
      "2\n",
      "33245 2020-10-26 16:14:19 1603728859666 1441.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33249 2020-10-26 16:14:22 1603728862000 1445.0\n",
      "33249 2020-10-26 16:14:22 1603728862500 1444.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33251 2020-10-26 16:14:24 1603728864000 1447.0\n",
      "33251 2020-10-26 16:14:24 1603728864500 1446.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33253 2020-10-26 16:14:26 1603728866000 1449.0\n",
      "33253 2020-10-26 16:14:26 1603728866500 1448.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33299 2020-10-26 16:15:12 1603728912000 1495.0\n",
      "33299 2020-10-26 16:15:12 1603728912500 1494.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33302 2020-10-26 16:15:15 1603728915000 1498.0\n",
      "33302 2020-10-26 16:15:15 1603728915500 1497.0\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward 2---\n",
      "33308 2020-10-26 16:15:21 1603728921000 1503.0\n",
      "33308 2020-10-26 16:15:21 1603728922000 1504.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33310 2020-10-26 16:15:23 1603728923000 1506.0\n",
      "33310 2020-10-26 16:15:23 1603728923500 1505.0\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward 2---\n",
      "33316 2020-10-26 16:15:29 1603728929000 1511.0\n",
      "33316 2020-10-26 16:15:29 1603728930000 1512.0\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward 2---\n",
      "33319 2020-10-26 16:15:32 1603728932000 1514.0\n",
      "33319 2020-10-26 16:15:32 1603728933000 1515.0\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward 2---\n",
      "33322 2020-10-26 16:15:35 1603728935000 1517.0\n",
      "33322 2020-10-26 16:15:35 1603728936000 1518.0\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward 2---\n",
      "33325 2020-10-26 16:15:38 1603728938000 1520.0\n",
      "33325 2020-10-26 16:15:38 1603728939000 1521.0\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward 2---\n",
      "33330 2020-10-26 16:15:43 1603728943000 1525.0\n",
      "33330 2020-10-26 16:15:43 1603728944000 1526.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33335 2020-10-26 16:15:48 1603728948000 1530.0\n",
      "33335 2020-10-26 16:15:48 1603728948500 1531.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33340 2020-10-26 16:15:53 1603728953000 1536.0\n",
      "33341 2020-10-26 16:15:53 1603728953500 1536.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33349 2020-10-26 16:16:01 1603728961000 1544.0\n",
      "33350 2020-10-26 16:16:01 1603728961333 1544.0\n",
      "33351 2020-10-26 16:16:01 1603728961666 1544.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33352 2020-10-26 16:16:02 1603728962000 1545.0\n",
      "33353 2020-10-26 16:16:02 1603728962500 1545.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33354 2020-10-26 16:16:03 1603728963000 1546.0\n",
      "33355 2020-10-26 16:16:03 1603728963500 1546.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33359 2020-10-26 16:16:04 1603728964000 1547.0\n",
      "33358 2020-10-26 16:16:04 1603728964250 1547.0\n",
      "33357 2020-10-26 16:16:04 1603728964500 1547.0\n",
      "33356 2020-10-26 16:16:04 1603728964750 1547.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33360 2020-10-26 16:16:05 1603728965000 1548.0\n",
      "33361 2020-10-26 16:16:05 1603728965500 1548.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33383 2020-10-26 16:16:27 1603728987000 nan\n",
      "33384 2020-10-26 16:16:27 1603728987500 nan\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "2 2\n",
      "0\n",
      "33385 2020-10-26 16:16:28 1603728988000 1570.0\n",
      "0\n",
      "33385 2020-10-26 16:16:28 1603728988000 1571.0\n",
      "1\n",
      "33386 2020-10-26 16:16:28 1603728988500 1570.0\n",
      "1\n",
      "33386 2020-10-26 16:16:28 1603728988500 1571.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33388 2020-10-26 16:16:29 1603728989000 1572.0\n",
      "33389 2020-10-26 16:16:29 1603728989333 1572.0\n",
      "33387 2020-10-26 16:16:29 1603728989666 1572.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33390 2020-10-26 16:16:30 1603728990000 1573.0\n",
      "33391 2020-10-26 16:16:30 1603728990333 1573.0\n",
      "33392 2020-10-26 16:16:30 1603728990666 1573.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33393 2020-10-26 16:16:31 1603728991000 1574.0\n",
      "33394 2020-10-26 16:16:31 1603728991500 1574.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33396 2020-10-26 16:16:33 1603728993000 1576.0\n",
      "33397 2020-10-26 16:16:33 1603728993500 1576.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33399 2020-10-26 16:16:34 1603728994000 1577.0\n",
      "33398 2020-10-26 16:16:34 1603728994500 1577.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33402 2020-10-26 16:16:37 1603728997000 1580.0\n",
      "33403 2020-10-26 16:16:37 1603728997500 1580.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33415 2020-10-26 16:16:49 1603729009000 1592.0\n",
      "33416 2020-10-26 16:16:49 1603729009500 1592.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33418 2020-10-26 16:16:50 1603729010000 1593.0\n",
      "33417 2020-10-26 16:16:50 1603729010500 1593.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33423 2020-10-26 16:16:55 1603729015000 1598.0\n",
      "33424 2020-10-26 16:16:55 1603729015500 1598.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33427 2020-10-26 16:16:57 1603729017000 1600.0\n",
      "33426 2020-10-26 16:16:57 1603729017500 1600.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33433 2020-10-26 16:17:03 1603729023000 1606.0\n",
      "33434 2020-10-26 16:17:03 1603729023500 1606.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33437 2020-10-26 16:17:06 1603729026000 1609.0\n",
      "33438 2020-10-26 16:17:06 1603729026500 1609.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33443 2020-10-26 16:17:11 1603729031000 1614.0\n",
      "33443 2020-10-26 16:17:11 1603729031500 1613.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33446 2020-10-26 16:17:14 1603729034000 1617.0\n",
      "33446 2020-10-26 16:17:14 1603729034500 1616.0\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward 2---\n",
      "33450 2020-10-26 16:17:18 1603729038000 1620.0\n",
      "33450 2020-10-26 16:17:18 1603729039000 1621.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33453 2020-10-26 16:17:21 1603729041000 1624.0\n",
      "33453 2020-10-26 16:17:21 1603729041500 1623.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33454 2020-10-26 16:17:22 1603729042000 nan\n",
      "33455 2020-10-26 16:17:22 1603729042500 nan\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33457 2020-10-26 16:17:24 1603729044000 1626.0\n",
      "33458 2020-10-26 16:17:24 1603729044500 1626.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33468 2020-10-26 16:17:34 1603729054000 1636.0\n",
      "33469 2020-10-26 16:17:34 1603729054500 1636.0\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward 2---\n",
      "33471 2020-10-26 16:17:36 1603729056000 1638.0\n",
      "33471 2020-10-26 16:17:36 1603729057000 1639.0\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward 2---\n",
      "33476 2020-10-26 16:17:41 1603729061000 1643.0\n",
      "33476 2020-10-26 16:17:41 1603729062000 1644.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33486 2020-10-26 16:17:51 1603729071000 1653.0\n",
      "33487 2020-10-26 16:17:51 1603729071500 1653.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33491 2020-10-26 16:17:54 1603729074000 1656.0\n",
      "33490 2020-10-26 16:17:54 1603729074500 1656.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33496 2020-10-26 16:17:59 1603729079000 1661.0\n",
      "33497 2020-10-26 16:17:59 1603729079500 1661.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33500 2020-10-26 16:18:02 1603729082000 1664.0\n",
      "33500 2020-10-26 16:18:02 1603729082500 1665.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33517 2020-10-26 16:18:19 1603729099000 1681.0\n",
      "33518 2020-10-26 16:18:19 1603729099500 1681.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33519 2020-10-26 16:18:20 1603729100000 1682.0\n",
      "33520 2020-10-26 16:18:20 1603729100500 1682.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33533 2020-10-26 16:18:33 1603729113000 1695.0\n",
      "33534 2020-10-26 16:18:33 1603729113333 1695.0\n",
      "33535 2020-10-26 16:18:33 1603729113666 1695.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33543 2020-10-26 16:18:41 1603729121000 1703.0\n",
      "33544 2020-10-26 16:18:41 1603729121500 1703.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33546 2020-10-26 16:18:43 1603729123000 1705.0\n",
      "33547 2020-10-26 16:18:43 1603729123500 1705.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33552 2020-10-26 16:18:48 1603729128000 1711.0\n",
      "33552 2020-10-26 16:18:48 1603729128500 1710.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33555 2020-10-26 16:18:51 1603729131000 1713.0\n",
      "33556 2020-10-26 16:18:51 1603729131500 1713.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "2 2\n",
      "0\n",
      "33558 2020-10-26 16:18:53 1603729133000 1716.0\n",
      "0\n",
      "33558 2020-10-26 16:18:53 1603729133000 1715.0\n",
      "1\n",
      "33559 2020-10-26 16:18:53 1603729133500 1716.0\n",
      "1\n",
      "33559 2020-10-26 16:18:53 1603729133500 1715.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33574 2020-10-26 16:19:08 1603729148000 1730.0\n",
      "33575 2020-10-26 16:19:08 1603729148500 1730.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33576 2020-10-26 16:19:09 1603729149000 1731.0\n",
      "33577 2020-10-26 16:19:09 1603729149500 1731.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33580 2020-10-26 16:19:12 1603729152000 1735.0\n",
      "33580 2020-10-26 16:19:12 1603729152500 1734.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "2 2\n",
      "0\n",
      "33583 2020-10-26 16:19:15 1603729155000 1738.0\n",
      "0\n",
      "33583 2020-10-26 16:19:15 1603729155000 1737.0\n",
      "1\n",
      "33584 2020-10-26 16:19:15 1603729155500 1738.0\n",
      "1\n",
      "33584 2020-10-26 16:19:15 1603729155500 1737.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33585 2020-10-26 16:19:16 1603729156000 nan\n",
      "33586 2020-10-26 16:19:16 1603729156500 nan\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33588 2020-10-26 16:19:18 1603729158000 1740.0\n",
      "33589 2020-10-26 16:19:18 1603729158500 1740.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33590 2020-10-26 16:19:19 1603729159000 1741.0\n",
      "33591 2020-10-26 16:19:19 1603729159500 1741.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33593 2020-10-26 16:19:20 1603729160000 1742.0\n",
      "33592 2020-10-26 16:19:20 1603729160500 1742.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33594 2020-10-26 16:19:21 1603729161000 1743.0\n",
      "33595 2020-10-26 16:19:21 1603729161250 1743.0\n",
      "33596 2020-10-26 16:19:21 1603729161500 1743.0\n",
      "33597 2020-10-26 16:19:21 1603729161750 1743.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33598 2020-10-26 16:19:22 1603729162000 1744.0\n",
      "33599 2020-10-26 16:19:22 1603729162250 1744.0\n",
      "33600 2020-10-26 16:19:22 1603729162500 1744.0\n",
      "33601 2020-10-26 16:19:22 1603729162750 1744.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33604 2020-10-26 16:19:23 1603729163000 1745.0\n",
      "33602 2020-10-26 16:19:23 1603729163333 1745.0\n",
      "33603 2020-10-26 16:19:23 1603729163666 1745.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33605 2020-10-26 16:19:24 1603729164000 1746.0\n",
      "33606 2020-10-26 16:19:24 1603729164200 1746.0\n",
      "33607 2020-10-26 16:19:24 1603729164400 1746.0\n",
      "33608 2020-10-26 16:19:24 1603729164600 1746.0\n",
      "33609 2020-10-26 16:19:24 1603729164800 1746.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33611 2020-10-26 16:19:25 1603729165000 1747.0\n",
      "33610 2020-10-26 16:19:25 1603729165333 1747.0\n",
      "33612 2020-10-26 16:19:25 1603729165666 1747.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33622 2020-10-26 16:19:34 1603729174000 1756.0\n",
      "33621 2020-10-26 16:19:34 1603729174500 1756.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33723 2020-10-26 16:21:15 1603729275000 1857.0\n",
      "33724 2020-10-26 16:21:15 1603729275500 1857.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33725 2020-10-26 16:21:16 1603729276000 1858.0\n",
      "33726 2020-10-26 16:21:16 1603729276333 1858.0\n",
      "33727 2020-10-26 16:21:16 1603729276666 1858.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33731 2020-10-26 16:21:17 1603729277000 1859.0\n",
      "33730 2020-10-26 16:21:17 1603729277250 1859.0\n",
      "33728 2020-10-26 16:21:17 1603729277500 1859.0\n",
      "33729 2020-10-26 16:21:17 1603729277750 1859.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33732 2020-10-26 16:21:18 1603729278000 1860.0\n",
      "33733 2020-10-26 16:21:18 1603729278250 1860.0\n",
      "33734 2020-10-26 16:21:18 1603729278500 1860.0\n",
      "33735 2020-10-26 16:21:18 1603729278750 1860.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33737 2020-10-26 16:21:19 1603729279000 1861.0\n",
      "33736 2020-10-26 16:21:19 1603729279500 1861.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33755 2020-10-26 16:21:37 1603729297000 1879.0\n",
      "33756 2020-10-26 16:21:37 1603729297500 1879.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33757 2020-10-26 16:21:38 1603729298000 1880.0\n",
      "33758 2020-10-26 16:21:38 1603729298500 1880.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33761 2020-10-26 16:21:39 1603729299000 1881.0\n",
      "33759 2020-10-26 16:21:39 1603729299333 1881.0\n",
      "33760 2020-10-26 16:21:39 1603729299666 1881.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33762 2020-10-26 16:21:40 1603729300000 1882.0\n",
      "33763 2020-10-26 16:21:40 1603729300333 1882.0\n",
      "33764 2020-10-26 16:21:40 1603729300666 1882.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33765 2020-10-26 16:21:41 1603729301000 1883.0\n",
      "33766 2020-10-26 16:21:41 1603729301500 1883.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33768 2020-10-26 16:21:43 1603729303000 1885.0\n",
      "33769 2020-10-26 16:21:43 1603729303500 1885.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33771 2020-10-26 16:21:44 1603729304000 1886.0\n",
      "33770 2020-10-26 16:21:44 1603729304500 1886.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33774 2020-10-26 16:21:47 1603729307000 1889.0\n",
      "33775 2020-10-26 16:21:47 1603729307500 1889.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33778 2020-10-26 16:21:50 1603729310000 1892.0\n",
      "33779 2020-10-26 16:21:50 1603729310500 1892.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33782 2020-10-26 16:21:53 1603729313000 1895.0\n",
      "33783 2020-10-26 16:21:53 1603729313500 1895.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33810 2020-10-26 16:22:19 1603729339000 1921.0\n",
      "33809 2020-10-26 16:22:19 1603729339500 1921.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33813 2020-10-26 16:22:22 1603729342000 1924.0\n",
      "33814 2020-10-26 16:22:22 1603729342500 1924.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33823 2020-10-26 16:22:31 1603729351000 1933.0\n",
      "33824 2020-10-26 16:22:31 1603729351500 1933.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33825 2020-10-26 16:22:32 1603729352000 1934.0\n",
      "33826 2020-10-26 16:22:32 1603729352500 1934.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33832 2020-10-26 16:22:38 1603729358000 1940.0\n",
      "33833 2020-10-26 16:22:38 1603729358500 1940.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33834 2020-10-26 16:22:39 1603729359000 1941.0\n",
      "33835 2020-10-26 16:22:39 1603729359500 1941.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33854 2020-10-26 16:22:58 1603729378000 1960.0\n",
      "33855 2020-10-26 16:22:58 1603729378500 1960.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33861 2020-10-26 16:23:03 1603729383000 1965.0\n",
      "33860 2020-10-26 16:23:03 1603729383500 1965.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33900 2020-10-26 16:23:42 1603729422000 2004.0\n",
      "33901 2020-10-26 16:23:42 1603729422500 2004.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33903 2020-10-26 16:23:44 1603729424000 2006.0\n",
      "33904 2020-10-26 16:23:44 1603729424500 2006.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33909 2020-10-26 16:23:49 1603729429000 2011.0\n",
      "33910 2020-10-26 16:23:49 1603729429500 2011.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33913 2020-10-26 16:23:51 1603729431000 2013.0\n",
      "33912 2020-10-26 16:23:51 1603729431500 2013.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33929 2020-10-26 16:24:07 1603729447000 2029.0\n",
      "33930 2020-10-26 16:24:07 1603729447500 2029.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33933 2020-10-26 16:24:09 1603729449000 2031.0\n",
      "33932 2020-10-26 16:24:09 1603729449500 2031.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33936 2020-10-26 16:24:12 1603729452000 2034.0\n",
      "33937 2020-10-26 16:24:12 1603729452500 2034.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33939 2020-10-26 16:24:14 1603729454000 2036.0\n",
      "33940 2020-10-26 16:24:14 1603729454500 2036.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33949 2020-10-26 16:24:23 1603729463000 2045.0\n",
      "33950 2020-10-26 16:24:23 1603729463500 2045.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33953 2020-10-26 16:24:25 1603729465000 2047.0\n",
      "33952 2020-10-26 16:24:25 1603729465500 2047.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33955 2020-10-26 16:24:27 1603729467000 2049.0\n",
      "33956 2020-10-26 16:24:27 1603729467333 2049.0\n",
      "33957 2020-10-26 16:24:27 1603729467666 2049.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33959 2020-10-26 16:24:29 1603729469000 2051.0\n",
      "33960 2020-10-26 16:24:29 1603729469500 2051.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33963 2020-10-26 16:24:31 1603729471000 2053.0\n",
      "33964 2020-10-26 16:24:31 1603729471333 2053.0\n",
      "33962 2020-10-26 16:24:31 1603729471666 2053.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33965 2020-10-26 16:24:32 1603729472000 2054.0\n",
      "33966 2020-10-26 16:24:32 1603729472500 2054.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33968 2020-10-26 16:24:34 1603729474000 2056.0\n",
      "33969 2020-10-26 16:24:34 1603729474500 2056.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33970 2020-10-26 16:24:35 1603729475000 2057.0\n",
      "33971 2020-10-26 16:24:35 1603729475500 2057.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33972 2020-10-26 16:24:36 1603729476000 2058.0\n",
      "33973 2020-10-26 16:24:36 1603729476500 2058.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33975 2020-10-26 16:24:37 1603729477000 2059.0\n",
      "33974 2020-10-26 16:24:37 1603729477500 2059.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33976 2020-10-26 16:24:38 1603729478000 2060.0\n",
      "33977 2020-10-26 16:24:38 1603729478500 2060.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33978 2020-10-26 16:24:39 1603729479000 2061.0\n",
      "33979 2020-10-26 16:24:39 1603729479500 2061.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33980 2020-10-26 16:24:40 1603729480000 2062.0\n",
      "33981 2020-10-26 16:24:40 1603729480250 2062.0\n",
      "33982 2020-10-26 16:24:40 1603729480500 2062.0\n",
      "33983 2020-10-26 16:24:40 1603729480750 2062.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33986 2020-10-26 16:24:41 1603729481000 2063.0\n",
      "33984 2020-10-26 16:24:41 1603729481333 2063.0\n",
      "33985 2020-10-26 16:24:41 1603729481666 2063.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33987 2020-10-26 16:24:42 1603729482000 2064.0\n",
      "33988 2020-10-26 16:24:42 1603729482200 2064.0\n",
      "33989 2020-10-26 16:24:42 1603729482400 2064.0\n",
      "33990 2020-10-26 16:24:42 1603729482600 2064.0\n",
      "33991 2020-10-26 16:24:42 1603729482800 2064.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33992 2020-10-26 16:24:43 1603729483000 2065.0\n",
      "33993 2020-10-26 16:24:43 1603729483500 2065.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "34001 2020-10-26 16:24:51 1603729491000 2073.0\n",
      "34002 2020-10-26 16:24:51 1603729491500 2073.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "34006 2020-10-26 16:24:52 1603729492000 2074.0\n",
      "34007 2020-10-26 16:24:52 1603729492200 2074.0\n",
      "34005 2020-10-26 16:24:52 1603729492400 2074.0\n",
      "34004 2020-10-26 16:24:52 1603729492600 2074.0\n",
      "34003 2020-10-26 16:24:52 1603729492800 2074.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "34008 2020-10-26 16:24:53 1603729493000 2075.0\n",
      "34009 2020-10-26 16:24:53 1603729493500 2075.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "34010 2020-10-26 16:24:54 1603729494000 2076.0\n",
      "34011 2020-10-26 16:24:54 1603729494250 2076.0\n",
      "34012 2020-10-26 16:24:54 1603729494500 2076.0\n",
      "34013 2020-10-26 16:24:54 1603729494750 2076.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "34016 2020-10-26 16:24:55 1603729495000 2077.0\n",
      "34015 2020-10-26 16:24:55 1603729495333 2077.0\n",
      "34014 2020-10-26 16:24:55 1603729495666 2077.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "34017 2020-10-26 16:24:56 1603729496000 2078.0\n",
      "34018 2020-10-26 16:24:56 1603729496333 2078.0\n",
      "34019 2020-10-26 16:24:56 1603729496666 2078.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "34224 2020-10-26 16:34:41 1603730081000 2283.0\n",
      "34225 2020-10-26 16:34:41 1603730081500 2283.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "34274 2020-10-26 16:35:28 1603730128000 2330.0\n",
      "34272 2020-10-26 16:35:28 1603730128333 2330.0\n",
      "34273 2020-10-26 16:35:28 1603730128666 2330.0\n"
     ]
    }
   ],
   "source": [
    "input_folder_path =  './EVR_T23_Car46_261020/'\n",
    "#input_folder_path = './Train 23 14NOV2019 Work Folder/raw logs'\n",
    "start_time = '2020/10/26 15:50:00' \n",
    "end_time = '2020/10/26 16:50:00'\n",
    "#start_time = '2020/10/26 04:36:45'\n",
    "#end_time = '2020/10/26 04:37:09'\n",
    "sample_output_filename = ['./EVR_T23_Car46_261020/preprocessing_output/EVR_20201026_1550_to_20201026_1650.csv']\n",
    "\n",
    "# input_folder_path = './Train 23 14NOV2019 Work Folder/raw logs'\n",
    "# #input_folder_path = './Train 23 14NOV2019 Work Folder/raw logs'\n",
    "# start_time = '2019/11/14 17:00:00' \n",
    "# end_time = '2019/11/14 18:00:00'\n",
    "# sample_output_filename = ['./Train 23 14NOV2019 Work Folder/preprocessing_output/EVR_20191114_1700_to_20191114_1800.csv']\n",
    "\n",
    "# input_folder_path = './Train 23 14NOV2019 Work Folder/raw logs'\n",
    "# #input_folder_path = './Train 23 14NOV2019 Work Folder/raw logs'\n",
    "# start_time = '2019/11/14 18:00:00' \n",
    "# end_time = '2019/11/14 19:00:00'\n",
    "# sample_output_filename = ['./Train 23 14NOV2019 Work Folder/preprocessing_output/EVR_20191114_1800_to_20191114_1900.csv']\n",
    "\n",
    "def process_evr (input_folder_path, start_time, end_time):\n",
    "    # Determine time difference between start and end time\n",
    "    datetime_format = '%Y/%m/%d %H:%M:%S'\n",
    "    start_time = datetime.datetime.strptime(start_time, datetime_format)\n",
    "    end_time = datetime.datetime.strptime(end_time, datetime_format)\n",
    "    start_time_new = start_time.strftime('%m/%d/%Y %H:%M:%S')\n",
    "    end_time_new = end_time.strftime('%m/%d/%Y %H:%M:%S')\n",
    "    # Check if EVR folder exist\n",
    "    if not os.path.exists(os.path.join(input_folder_path, 'EVR')):\n",
    "        print(\"EVR folder does not exist. Kindly check the folder strucutre. No EVR processing will be performed\")\n",
    "    else:\n",
    "        print(\"EVR folder exist!\")\n",
    "        # Check if C2 file exist\n",
    "        c2_pattern = 'EVR_Car\\d+_\\d+_C2.txt'\n",
    "        norm_pattern = 'EVR_Car\\d+_\\d+.txt'\n",
    "        c2_flag = False\n",
    "        norm_flag = False\n",
    "        for evr_file in os.listdir(os.path.join(input_folder_path, 'EVR')):\n",
    "            if re.search(c2_pattern, evr_file):\n",
    "                c2_filename = evr_file\n",
    "                c2_flag = True\n",
    "            if re.search(norm_pattern,evr_file):\n",
    "                norm_filename = evr_file\n",
    "                norm_flag = True\n",
    "\n",
    "        if not (c2_flag and norm_flag):\n",
    "            sys.exit(\"Missing EVR files! Kindly check EVR folder!\")\n",
    "        else:\n",
    "            # Process C2 file\n",
    "            df_c2 = pd.read_csv(os.path.join(input_folder_path, 'EVR', c2_filename), sep=\";\")\n",
    "            df_c2['Date'] = pd.to_datetime(df_c2['Date'], format='%m/%d/%Y %H:%M:%S')\n",
    "            df_c2_period = df_c2.loc[(df_c2['Date'] >= start_time_new) & (df_c2['Date'] <= end_time_new)]\n",
    "            #df_c2_clean_timestamp = preprocess_timestamp_old(df_c2_period)\n",
    "            df_c2_clean = clean_dataframe(df_c2_period, 'C2')\n",
    "            #df_c2_clean.to_csv('./result_c2.csv', index=False, header=True)\n",
    "            \n",
    "            # Processs Operating Data file\n",
    "            df_od = pd.read_csv(os.path.join(input_folder_path, 'EVR', norm_filename), sep=\";\")\n",
    "            df_od['Date'] = pd.to_datetime(df_od['Date'], format='%m/%d/%Y %H:%M:%S')\n",
    "            df_od_period = df_od.loc[(df_od['Date'] >= start_time_new) & (df_od['Date'] <= end_time_new)]\n",
    "            #df_od_clean_timestamp = preprocess_timestamp_old(df_od_period)\n",
    "            df_od_clean = clean_dataframe(df_od_period, 'OD')\n",
    "            #df_od_clean.to_csv('./result_od.csv', index=False, header=True)\n",
    "            \n",
    "            # Merge other Operating Data to C2 Dataframe based on Timestampms (ordered manner)\n",
    "            df_result = pd.merge_ordered(df_c2_clean, df_od_clean, how='outer', on='Timestampms')\n",
    "            #df_result.to_csv('./result_9.csv', index=False, header=True)\n",
    "            \n",
    "            \n",
    "            df_result = process_timestamp(df_result, df_c2_clean, df_od_clean)\n",
    "            #df_result.to_csv('./result_9.csv', index=False, header=True)\n",
    "            # Merge C2 and OD files first\n",
    "            \n",
    "#             # Group resulting same real timestamp together\n",
    "#             df_result = df_result.groupby(\"Timestampms\").last().reset_index()\n",
    "\n",
    "#             # Remove similar record numbers for both EC2 and EOD\n",
    "#             #df_duplicate = df[df.duplicated('EC2_Record Number',keep='first')]\n",
    "#             filter_col = [col for col in df_result if col.startswith('EC2_')]\n",
    "#             df_result.loc[df_result.duplicated('EC2_Record Number',keep='first'), filter_col] = np.NaN\n",
    "#             filter_col = [col for col in df_result if col.startswith('EOD_')]\n",
    "#             df_result.loc[df_result.duplicated('EOD_Record Number',keep='first'), filter_col] = np.NaN\n",
    "            \n",
    "            #df_result.to_csv('./result_8.csv', index=False, header=True)\n",
    "            # Clean Timestamp\n",
    "            #df_result = preprocess_timestamp(df_result)\n",
    "            \n",
    "            \n",
    "    return df_c2_clean, df_od_clean, df_result\n",
    "\n",
    "\n",
    "df_c2_clean, df_od_clean, df_result = process_evr(input_folder_path, start_time, end_time)\n",
    "\n",
    "output(sample_output_filename[0], df_result)\n",
    "#unit_test(sample_output_filename[0], 'C2', df_c2_clean)  \n",
    "#unit_test(sample_output_filename[0], 'OD', df_od_clean)  \n",
    "#unit_test_all(sample_output_filename[0], df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe(df, name):\n",
    "    # Drop columns with NaNs\n",
    "    df = df.dropna(how='any',axis=1)\n",
    "    # Sort df based on column 'Date'\n",
    "    df = df.sort_values(by='Date',ascending=True).reset_index(drop=True)\n",
    "    # Convert datetime to timestamp (Unix) in milisecond resolution\n",
    "    df['Timestamp'] = df.Date.values.astype(np.int64) // 10 ** 9 *1000\n",
    "    # Add column for Real_Time Timestampms\n",
    "    df['Timestampms'] = df['Timestamp'] \n",
    "    \n",
    "    # Add Prefix to Columns name (e.g. EC2/EOD_***) except Timestampms\n",
    "    if name == 'C2':\n",
    "        df.columns = df.columns.map(lambda x : 'EC2_'+x if x !='Timestampms' else x)\n",
    "    elif name == 'OD':\n",
    "        df.columns = df.columns.map(lambda x : 'EOD_'+x if x !='Timestampms' else x)\n",
    "        \n",
    "    # Shift Timestampms to first column\n",
    "    df = df[ ['Timestampms'] + [ col for col in df.columns if col != 'Timestampms' ] ]\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_timestamp(df, df_c2_clean, df_od_clean):\n",
    "    # Find duplicates datetime in column 'Date'\n",
    "    df_duplicate = df[df.duplicated('Timestampms',keep=False)]\n",
    "    for index, unique_datetime in enumerate (df_duplicate['Timestampms'].unique()):\n",
    "        timestamp_df = df_duplicate.loc[df_duplicate['Timestampms'] == unique_datetime]\n",
    "        timestamp_df_next = pd.DataFrame()\n",
    "        if index + 1 <= len(df_duplicate['Timestampms'].unique())-1:\n",
    "            timestamp_df_next = df_duplicate.loc[df_duplicate['Timestampms'] == df_duplicate['Timestampms'].unique()[index+1]]\n",
    "        \n",
    "        difference_previous,  difference_next = find_difference(timestamp_df, unique_datetime, df)\n",
    "        \n",
    "        #Check if record number is in duplicate\n",
    "        df_ec_record_number = timestamp_df[timestamp_df.duplicated('EC2_Record Number',keep=False)]\n",
    "        df_od_record_number = timestamp_df[timestamp_df.duplicated('EOD_Record Number',keep=False)]\n",
    "        #print(len(df_ec_record_number), len(df_od_record_number))\n",
    "        # EC2 Data has duplciate timestamp\n",
    "        if len(df_ec_record_number) == 0 and len(df_od_record_number) > 0:\n",
    "            print(\"---Handling Duplicates, Split duplicate evenly---\")\n",
    "            # For every Timestamp (1000 millisecond), find the interval\n",
    "            interval_ms = round(1000 / len(timestamp_df.index))\n",
    "            for counter in range (0, len(timestamp_df.index)):\n",
    "                df.loc[timestamp_df.index[counter],'Timestampms'] = df.loc[timestamp_df.index[counter],'Timestampms'] + counter*interval_ms\n",
    "                print(df.loc[timestamp_df.index[counter],'EC2_Record Number'], df.loc[timestamp_df.index[counter],'EC2_Date'],\n",
    "                      df.loc[timestamp_df.index[counter],'Timestampms'],df.loc[timestamp_df.index[counter],'EOD_Record Number'])\n",
    "        \n",
    "        # OD Data has duplicate timestamp\n",
    "        elif len(df_ec_record_number) > 0 and len(df_od_record_number) == 0:\n",
    "            df_od_timestamp = df_od_clean.loc[df_od_clean['EOD_Timestamp'] == unique_datetime]\n",
    "            od_difference_previous,  od_difference_next = find_difference(df_od_timestamp, unique_datetime, df_od_clean)\n",
    "            \n",
    "            if difference_previous < 1000 and (timestamp_df_next.index[0]-timestamp_df.index[-1]==1):\n",
    "                print(\"---Handling Duplicates, Fill gap with duplicate, Forward 1---\")\n",
    "                for counter in range (0, len(timestamp_df.index)):\n",
    "                    df.loc[timestamp_df.index[counter],'Timestampms'] = df.loc[timestamp_df.index[counter],'Timestampms'] + counter*1000\n",
    "                    print(df.loc[timestamp_df.index[counter],'EC2_Record Number'], df.loc[timestamp_df.index[counter],'EC2_Date'],\n",
    "                      df.loc[timestamp_df.index[counter],'Timestampms'],df.loc[timestamp_df.index[counter],'EOD_Record Number'])\n",
    "                    \n",
    "            elif difference_previous <= 1000 and ((od_difference_previous <= 1000 and od_difference_next <=1000) or\n",
    "                                                 df.loc[timestamp_df.index[0],'EOD_Record Number'] > df.loc[timestamp_df.index[-1],'EOD_Record Number'] or\n",
    "                                                 od_difference_next <=1000):\n",
    "                print(\"---Handling Duplicates, Split duplicate evenly---\")\n",
    "                # For every Timestamp (1000 millisecond), find the interval\n",
    "                interval_ms = round(1000 / len(timestamp_df.index))\n",
    "                for counter in range (0, len(timestamp_df.index)):\n",
    "                    df.loc[timestamp_df.index[counter],'Timestampms'] = df.loc[timestamp_df.index[counter],'Timestampms'] + counter*interval_ms\n",
    "                    print(df.loc[timestamp_df.index[counter],'EC2_Record Number'], df.loc[timestamp_df.index[counter],'EC2_Date'],\n",
    "                          df.loc[timestamp_df.index[counter],'Timestampms'],df.loc[timestamp_df.index[counter],'EOD_Record Number'])\n",
    "            \n",
    "            elif difference_previous >= 1000 and (od_difference_previous <= 1000 and od_difference_next >=2000):\n",
    "                print(\"---Handling Duplicates, Fill gap with duplicate, Forward 2---\")\n",
    "                for counter in range (0, len(timestamp_df.index)):\n",
    "                    df.loc[timestamp_df.index[counter],'Timestampms'] = df.loc[timestamp_df.index[counter],'Timestampms'] + counter*1000\n",
    "                    print(df.loc[timestamp_df.index[counter],'EC2_Record Number'], df.loc[timestamp_df.index[counter],'EC2_Date'],\n",
    "                          df.loc[timestamp_df.index[counter],'Timestampms'],df.loc[timestamp_df.index[counter],'EOD_Record Number'])\n",
    "        \n",
    "            elif difference_previous >= 1000 and (od_difference_previous > 1000 and od_difference_next == 1000):\n",
    "                # Loop backwards\n",
    "                print(\"---Handling Duplicates, Fill gap with duplicate, Backward---\")\n",
    "                count = 0\n",
    "                for counter in range (len(timestamp_df.index), 0, -1):\n",
    "                    df.loc[timestamp_df.index[counter-1],'Timestampms'] = df.loc[timestamp_df.index[counter-1],'Timestampms'] - count*1000\n",
    "                    count +=1\n",
    "                    print(df.loc[timestamp_df.index[counter-1],'EC2_Record Number'], df.loc[timestamp_df.index[counter-1],'EC2_Date'],\n",
    "                          df.loc[timestamp_df.index[counter-1],'Timestampms'],df.loc[timestamp_df.index[counter-1],'EOD_Record Number'])\n",
    "                    \n",
    "        # Both EC2 and OD has duplicate timestamp\n",
    "        else:\n",
    "            maximum_interval = max(len(df_ec_record_number['EC2_Record Number'].unique()), len(df_od_record_number['EOD_Record Number'].unique()))\n",
    "            minimum_interval = min(len(df_ec_record_number['EC2_Record Number'].unique()), len(df_od_record_number['EOD_Record Number'].unique()))\n",
    "            print(\"---Handling Duplicates, Split duplicate evenly---\")\n",
    "            print(maximum_interval, minimum_interval)\n",
    "            interval_ms = round(1000 / maximum_interval)\n",
    "            for counter in range (0, len(timestamp_df.index)):\n",
    "                print(counter//minimum_interval)\n",
    "                df.loc[timestamp_df.index[counter],'Timestampms'] = df.loc[timestamp_df.index[counter],'Timestampms'] + (counter//minimum_interval)*interval_ms\n",
    "                print(df.loc[timestamp_df.index[counter],'EC2_Record Number'], df.loc[timestamp_df.index[counter],'EC2_Date'],\n",
    "                      df.loc[timestamp_df.index[counter],'Timestampms'],df.loc[timestamp_df.index[counter],'EOD_Record Number'])\n",
    "            \n",
    "            #Keep length of maximum_interval\n",
    "            \n",
    "            #if(difference_previous > 1000 and )\n",
    "#         print(\"---Handling Duplicates, Split duplicate evenly---\")\n",
    "#         # For every Timestamp (1000 millisecond), find the interval\n",
    "#         interval_ms = round(1000 / len(timestamp_df.index))\n",
    "#         for counter in range (0, len(timestamp_df.index)):\n",
    "#             df.loc[timestamp_df.index[counter],'Timestampms'] = df.loc[timestamp_df.index[counter],'Timestampms'] + counter*interval_ms\n",
    "#             print(df.loc[timestamp_df.index[counter],'EC2_Record Number'], df.loc[timestamp_df.index[counter],'EC2_Date'],\n",
    "#                   df.loc[timestamp_df.index[counter],'Timestampms'],df.loc[timestamp_df.index[counter],'EOD_Record Number'])\n",
    "        \n",
    "        \n",
    "        \n",
    "    # Remove similar record numbers for both EC2 and EOD\n",
    "    #df_duplicate = df[df.duplicated('EC2_Record Number',keep='first')]\n",
    "    filter_col = [col for col in df if col.startswith('EC2_')]\n",
    "    df.loc[df.duplicated('EC2_Record Number',keep='first'), filter_col] = np.NaN\n",
    "    filter_col = [col for col in df if col.startswith('EOD_')]\n",
    "    df.loc[df.duplicated('EOD_Record Number',keep='first'), filter_col] = np.NaN\n",
    "    \n",
    "     # Group resulting same real timestamp together\n",
    "    df = df.groupby(\"Timestampms\").last().reset_index()\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_difference(timestamp_df, unique_datetime, df):\n",
    "    difference_previous, difference_next = 0 , 0\n",
    "    if timestamp_df.index[0]-1 >=0 and timestamp_df.index[-1]+1 <= len(df)-1:\n",
    "        difference_previous = unique_datetime - df.loc[timestamp_df.index[0]-1,'Timestampms']\n",
    "        difference_next = df.loc[timestamp_df.index[-1]+1,'Timestampms'] - unique_datetime\n",
    "    elif timestamp_df.index[-1] == len(df)-1:\n",
    "        difference_previous = unique_datetime - df.loc[timestamp_df.index[0]-1,'Timestampms']\n",
    "    elif timestamp_df.index[0] == 0:\n",
    "        difference_next = df.loc[timestamp_df.index[-1]+1,'Timestampms'] - unique_datetime\n",
    "    \n",
    "    return difference_previous, difference_next\n",
    "\n",
    "def output(sample_output_filename, df_result):\n",
    "    # Import Output File\n",
    "    df_output = pd.read_csv(sample_output_filename)\n",
    "    \n",
    "    df_combine = pd.DataFrame()\n",
    "    df_combine = df_combine.assign(epoch = df_output['epoch'])\n",
    "    df_combine = df_combine.assign(Timestampms = df_result['Timestampms']) \n",
    "    df_combine = df_combine.assign(EC2_Record_Number = df_result['EC2_Record Number']) \n",
    "    df_combine = df_combine.assign(EC2_Date = df_result['EC2_Date']) \n",
    "    df_combine = df_combine.assign(EOD_Record_Number = df_result['EOD_Record Number']) \n",
    "    df_combine = df_combine.assign(EOD_Date = df_result['EOD_Date']) \n",
    "\n",
    "    \n",
    "    df_combine.to_csv('./result_combine.csv', index=False, header=True)\n",
    "    \n",
    "def unit_test(sample_output_filename, name, df_test):\n",
    "    print(\"---Unit Test for \" + name + \" Dataframe---\")\n",
    "    # Unit Test for C2\n",
    "    # Import Output File\n",
    "    df_output = pd.read_csv(sample_output_filename)\n",
    "    # Only retrieve respective columns\n",
    "    if name == 'C2':\n",
    "        df_output = df_output.drop(df_output[(df_output['EC2_002_Record_Number'].isnull())].index)\n",
    "        #drop column with prefix EOD\n",
    "        df_output = df_output.loc[:, ~df_output.columns.str.startswith('EOD')]\n",
    "    elif name == 'OD':\n",
    "        df_output = df_output.drop(df_output[(df_output['EOD_002_Record_Number'].isnull())].index)\n",
    "        #drop column with prefix EOD\n",
    "        df_output = df_output.loc[:, ~df_output.columns.str.startswith('EC2')]\n",
    "    \n",
    "    df_output['epoch'] = df_output.epoch.values.astype(np.float64)\n",
    "    df_output = df_output.reset_index(drop=True)\n",
    "    print(df_output.shape, df_test.shape)    \n",
    "    #Output to CSV\n",
    "    #df_output.to_csv('./df_output.csv', index=False, header=True)\n",
    "\n",
    "    #Output to CSV\n",
    "    #df_test.to_csv('./df_test.csv', index=False, header=True)\n",
    "    #df_drop_test = pd.read_csv('./df_test.csv')\n",
    "    df_drop_test = df_test\n",
    "    df_test['result'] = np.where(df_test['Timestampms'] == df_output['epoch'], 'True', 'False')\n",
    "    df_test.to_csv('./df_test.csv', index=False, header=True)\n",
    "    #df_drop_test = df_drop_test.sort_values(by='ATO_Real_Timestampms',ascending=True).reset_index(drop=True)\n",
    "    # Assert whether sample output and self processed are equal\n",
    "    assert_equal = omap.nan_equal(df_drop_test['Timestampms'].values, df_output['epoch'].values)\n",
    "    print(\"Equality Between Sample Output and Self Processed: \", assert_equal)\n",
    "    assert_equal = omap.nan_equal(df_drop_test['E'+name+'_Record Number'].values, df_output['E'+name+'_002_Record_Number'].values)\n",
    "    print(\"Equality Between Sample Output and Self Processed: \", assert_equal)\n",
    "    \n",
    "    #df_drop_test.columns = df_output.columns\n",
    "    #print(np.testing.assert_allclose(df_drop_test.values, df_output.values, rtol=1e-10, atol=0))\n",
    "    #print(pd.testing.assert_frame_equal(df_drop_test, df_output, check_dtype=False))\n",
    "    #print(df_drop_test.compare(df_output, align_axis=0))\n",
    "    #assert_equal = nan_equal(df_drop_test.values, df_output.values)\n",
    "    #assert_equal = nan_equal(df_drop['ATO_* General'].values, df_output['ATO_0101__General'].values)\n",
    "    #print(\"Equality Between Sample Output and Self Processed: \", assert_equal)\n",
    "    #print(np.testing.assert_equal(df_drop_test.values, df_output.values))\n",
    "    \n",
    "def unit_test_all(sample_output_filename, df_test):\n",
    "    print(\"---Unit Test for entire Dataframe---\")\n",
    "    # Import Output File\n",
    "    df_output = pd.read_csv(sample_output_filename)\n",
    "    print(df_output.shape, df_test.shape)\n",
    "    assert_equal = omap.nan_equal(df_test['Timestampms'].values, df_output['epoch'].values)\n",
    "    print(\"Equality Between Sample Output and Self Processed: \", assert_equal)\n",
    "    df_test['result'] = np.where(df_test['Timestampms'] == df_output['epoch'], 'True', 'False')\n",
    "    #df_test.to_csv('./df_test.csv', index=False, header=True)\n",
    "        \n",
    "    df_test['result_1'] = np.where(df_test['EC2_Record Number'] == df_output['EC2_002_Record_Number'], 'True', 'False')\n",
    "    df_test.to_csv('./df_test.csv', index=False, header=True)\n",
    "    assert_equal = omap.nan_equal(df_test['EC2_Record Number'].values, df_output['EC2_002_Record_Number'].values)\n",
    "    print(\"Equality Between Sample Output and Self Processed: \", assert_equal)\n",
    "    assert_equal = omap.nan_equal(df_test['EOD_Record Number'].values, df_output['EOD_002_Record_Number'].values)\n",
    "    print(\"Equality Between Sample Output and Self Processed: \", assert_equal)\n",
    "    print(np.testing.assert_equal(df_test['EC2_Record Number'].values, df_output['EC2_002_Record_Number'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
