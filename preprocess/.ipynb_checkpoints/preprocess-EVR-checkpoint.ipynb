{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import timeit\n",
    "tqdm.pandas()\n",
    "import omap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-26 16:32:25\n",
      "2020-10-26 16:26:04\n",
      "2018-02-26 00:00:00\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "ts = int(\"1603729945\")\n",
    "\n",
    "# if you encounter a \"year is out of range\" error the timestamp\n",
    "# may be in milliseconds, try `ts /= 1000` in that case\n",
    "print(datetime.utcfromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "ts = int(\"1603729564\")\n",
    "\n",
    "# if you encounter a \"year is out of range\" error the timestamp\n",
    "# may be in milliseconds, try `ts /= 1000` in that case\n",
    "print(datetime.utcfromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "ts = int(\"1519603200\")\n",
    "\n",
    "# if you encounter a \"year is out of range\" error the timestamp\n",
    "# may be in milliseconds, try `ts /= 1000` in that case\n",
    "print(datetime.utcfromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVR folder exist!\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32239 2020-10-26 15:58:14 1603727894000\n",
      "32240 2020-10-26 15:58:14 1603727894500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32248 2020-10-26 15:58:22 1603727902000\n",
      "32249 2020-10-26 15:58:22 1603727902333\n",
      "32250 2020-10-26 15:58:22 1603727902666\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32259 2020-10-26 15:58:31 1603727911000\n",
      "32260 2020-10-26 15:58:31 1603727911500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32283 2020-10-26 15:58:54 1603727934000\n",
      "32284 2020-10-26 15:58:54 1603727934333\n",
      "32285 2020-10-26 15:58:54 1603727934666\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32291 2020-10-26 15:58:56 1603727936000\n",
      "32290 2020-10-26 15:58:56 1603727936200\n",
      "32288 2020-10-26 15:58:56 1603727936400\n",
      "32287 2020-10-26 15:58:56 1603727936600\n",
      "32289 2020-10-26 15:58:56 1603727936800\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32292 2020-10-26 15:58:57 1603727937000\n",
      "32293 2020-10-26 15:58:57 1603727937200\n",
      "32294 2020-10-26 15:58:57 1603727937400\n",
      "32295 2020-10-26 15:58:57 1603727937600\n",
      "32296 2020-10-26 15:58:57 1603727937800\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32300 2020-10-26 15:58:58 1603727938000\n",
      "32299 2020-10-26 15:58:58 1603727938250\n",
      "32298 2020-10-26 15:58:58 1603727938500\n",
      "32297 2020-10-26 15:58:58 1603727938750\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32301 2020-10-26 15:58:59 1603727939000\n",
      "32302 2020-10-26 15:58:59 1603727939500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32303 2020-10-26 15:59:00 1603727940000\n",
      "32304 2020-10-26 15:59:00 1603727940500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32310 2020-10-26 15:59:06 1603727946000\n",
      "32311 2020-10-26 15:59:06 1603727946333\n",
      "32312 2020-10-26 15:59:06 1603727946666\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32521 2020-10-26 16:02:35 1603728155000\n",
      "32522 2020-10-26 16:02:35 1603728155333\n",
      "32523 2020-10-26 16:02:35 1603728155666\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32527 2020-10-26 16:02:36 1603728156000\n",
      "32526 2020-10-26 16:02:36 1603728156250\n",
      "32525 2020-10-26 16:02:36 1603728156500\n",
      "32524 2020-10-26 16:02:36 1603728156750\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32528 2020-10-26 16:02:37 1603728157000\n",
      "32529 2020-10-26 16:02:37 1603728157500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32531 2020-10-26 16:02:39 1603728159000\n",
      "32532 2020-10-26 16:02:39 1603728159500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32538 2020-10-26 16:02:45 1603728165000\n",
      "32539 2020-10-26 16:02:45 1603728165333\n",
      "32540 2020-10-26 16:02:45 1603728165666\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33229 2020-10-26 16:14:14 1603728854000\n",
      "33230 2020-10-26 16:14:14 1603728854200\n",
      "33231 2020-10-26 16:14:14 1603728854400\n",
      "33232 2020-10-26 16:14:14 1603728854600\n",
      "33233 2020-10-26 16:14:14 1603728854800\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33236 2020-10-26 16:14:15 1603728855000\n",
      "33234 2020-10-26 16:14:15 1603728855333\n",
      "33235 2020-10-26 16:14:15 1603728855666\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33237 2020-10-26 16:14:16 1603728856000\n",
      "33238 2020-10-26 16:14:16 1603728856250\n",
      "33239 2020-10-26 16:14:16 1603728856500\n",
      "33240 2020-10-26 16:14:16 1603728856750\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33241 2020-10-26 16:14:17 1603728857000\n",
      "33242 2020-10-26 16:14:17 1603728857500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33246 2020-10-26 16:14:19 1603728859000\n",
      "33244 2020-10-26 16:14:19 1603728859333\n",
      "33245 2020-10-26 16:14:19 1603728859666\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33340 2020-10-26 16:15:53 1603728953000\n",
      "33341 2020-10-26 16:15:53 1603728953500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33349 2020-10-26 16:16:01 1603728961000\n",
      "33350 2020-10-26 16:16:01 1603728961333\n",
      "33351 2020-10-26 16:16:01 1603728961666\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33352 2020-10-26 16:16:02 1603728962000\n",
      "33353 2020-10-26 16:16:02 1603728962500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33354 2020-10-26 16:16:03 1603728963000\n",
      "33355 2020-10-26 16:16:03 1603728963500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33359 2020-10-26 16:16:04 1603728964000\n",
      "33358 2020-10-26 16:16:04 1603728964250\n",
      "33357 2020-10-26 16:16:04 1603728964500\n",
      "33356 2020-10-26 16:16:04 1603728964750\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33360 2020-10-26 16:16:05 1603728965000\n",
      "33361 2020-10-26 16:16:05 1603728965500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33383 2020-10-26 16:16:27 1603728987000\n",
      "33384 2020-10-26 16:16:27 1603728987500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33385 2020-10-26 16:16:28 1603728988000\n",
      "33386 2020-10-26 16:16:28 1603728988500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33388 2020-10-26 16:16:29 1603728989000\n",
      "33389 2020-10-26 16:16:29 1603728989333\n",
      "33387 2020-10-26 16:16:29 1603728989666\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33390 2020-10-26 16:16:30 1603728990000\n",
      "33391 2020-10-26 16:16:30 1603728990333\n",
      "33392 2020-10-26 16:16:30 1603728990666\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33393 2020-10-26 16:16:31 1603728991000\n",
      "33394 2020-10-26 16:16:31 1603728991500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33396 2020-10-26 16:16:33 1603728993000\n",
      "33397 2020-10-26 16:16:33 1603728993500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33399 2020-10-26 16:16:34 1603728994000\n",
      "33398 2020-10-26 16:16:34 1603728994500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33402 2020-10-26 16:16:37 1603728997000\n",
      "33403 2020-10-26 16:16:37 1603728997500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33415 2020-10-26 16:16:49 1603729009000\n",
      "33416 2020-10-26 16:16:49 1603729009500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33418 2020-10-26 16:16:50 1603729010000\n",
      "33417 2020-10-26 16:16:50 1603729010500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33423 2020-10-26 16:16:55 1603729015000\n",
      "33424 2020-10-26 16:16:55 1603729015500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33427 2020-10-26 16:16:57 1603729017000\n",
      "33426 2020-10-26 16:16:57 1603729017500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33433 2020-10-26 16:17:03 1603729023000\n",
      "33434 2020-10-26 16:17:03 1603729023500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33437 2020-10-26 16:17:06 1603729026000\n",
      "33438 2020-10-26 16:17:06 1603729026500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33454 2020-10-26 16:17:22 1603729042000\n",
      "33455 2020-10-26 16:17:22 1603729042500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33457 2020-10-26 16:17:24 1603729044000\n",
      "33458 2020-10-26 16:17:24 1603729044500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33468 2020-10-26 16:17:34 1603729054000\n",
      "33469 2020-10-26 16:17:34 1603729054500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33486 2020-10-26 16:17:51 1603729071000\n",
      "33487 2020-10-26 16:17:51 1603729071500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33491 2020-10-26 16:17:54 1603729074000\n",
      "33490 2020-10-26 16:17:54 1603729074500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33496 2020-10-26 16:17:59 1603729079000\n",
      "33497 2020-10-26 16:17:59 1603729079500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33517 2020-10-26 16:18:19 1603729099000\n",
      "33518 2020-10-26 16:18:19 1603729099500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33519 2020-10-26 16:18:20 1603729100000\n",
      "33520 2020-10-26 16:18:20 1603729100500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33533 2020-10-26 16:18:33 1603729113000\n",
      "33534 2020-10-26 16:18:33 1603729113333\n",
      "33535 2020-10-26 16:18:33 1603729113666\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33543 2020-10-26 16:18:41 1603729121000\n",
      "33544 2020-10-26 16:18:41 1603729121500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33546 2020-10-26 16:18:43 1603729123000\n",
      "33547 2020-10-26 16:18:43 1603729123500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33555 2020-10-26 16:18:51 1603729131000\n",
      "33556 2020-10-26 16:18:51 1603729131500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33558 2020-10-26 16:18:53 1603729133000\n",
      "33559 2020-10-26 16:18:53 1603729133500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33574 2020-10-26 16:19:08 1603729148000\n",
      "33575 2020-10-26 16:19:08 1603729148500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33576 2020-10-26 16:19:09 1603729149000\n",
      "33577 2020-10-26 16:19:09 1603729149500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33583 2020-10-26 16:19:15 1603729155000\n",
      "33584 2020-10-26 16:19:15 1603729155500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33585 2020-10-26 16:19:16 1603729156000\n",
      "33586 2020-10-26 16:19:16 1603729156500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33588 2020-10-26 16:19:18 1603729158000\n",
      "33589 2020-10-26 16:19:18 1603729158500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33590 2020-10-26 16:19:19 1603729159000\n",
      "33591 2020-10-26 16:19:19 1603729159500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33593 2020-10-26 16:19:20 1603729160000\n",
      "33592 2020-10-26 16:19:20 1603729160500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33594 2020-10-26 16:19:21 1603729161000\n",
      "33595 2020-10-26 16:19:21 1603729161250\n",
      "33596 2020-10-26 16:19:21 1603729161500\n",
      "33597 2020-10-26 16:19:21 1603729161750\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33598 2020-10-26 16:19:22 1603729162000\n",
      "33599 2020-10-26 16:19:22 1603729162250\n",
      "33600 2020-10-26 16:19:22 1603729162500\n",
      "33601 2020-10-26 16:19:22 1603729162750\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33604 2020-10-26 16:19:23 1603729163000\n",
      "33602 2020-10-26 16:19:23 1603729163333\n",
      "33603 2020-10-26 16:19:23 1603729163666\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33605 2020-10-26 16:19:24 1603729164000\n",
      "33606 2020-10-26 16:19:24 1603729164200\n",
      "33607 2020-10-26 16:19:24 1603729164400\n",
      "33608 2020-10-26 16:19:24 1603729164600\n",
      "33609 2020-10-26 16:19:24 1603729164800\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33611 2020-10-26 16:19:25 1603729165000\n",
      "33610 2020-10-26 16:19:25 1603729165333\n",
      "33612 2020-10-26 16:19:25 1603729165666\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33622 2020-10-26 16:19:34 1603729174000\n",
      "33621 2020-10-26 16:19:34 1603729174500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33723 2020-10-26 16:21:15 1603729275000\n",
      "33724 2020-10-26 16:21:15 1603729275500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33725 2020-10-26 16:21:16 1603729276000\n",
      "33726 2020-10-26 16:21:16 1603729276333\n",
      "33727 2020-10-26 16:21:16 1603729276666\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33731 2020-10-26 16:21:17 1603729277000\n",
      "33730 2020-10-26 16:21:17 1603729277250\n",
      "33728 2020-10-26 16:21:17 1603729277500\n",
      "33729 2020-10-26 16:21:17 1603729277750\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33732 2020-10-26 16:21:18 1603729278000\n",
      "33733 2020-10-26 16:21:18 1603729278250\n",
      "33734 2020-10-26 16:21:18 1603729278500\n",
      "33735 2020-10-26 16:21:18 1603729278750\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33737 2020-10-26 16:21:19 1603729279000\n",
      "33736 2020-10-26 16:21:19 1603729279500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33755 2020-10-26 16:21:37 1603729297000\n",
      "33756 2020-10-26 16:21:37 1603729297500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33757 2020-10-26 16:21:38 1603729298000\n",
      "33758 2020-10-26 16:21:38 1603729298500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33761 2020-10-26 16:21:39 1603729299000\n",
      "33759 2020-10-26 16:21:39 1603729299333\n",
      "33760 2020-10-26 16:21:39 1603729299666\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33762 2020-10-26 16:21:40 1603729300000\n",
      "33763 2020-10-26 16:21:40 1603729300333\n",
      "33764 2020-10-26 16:21:40 1603729300666\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33765 2020-10-26 16:21:41 1603729301000\n",
      "33766 2020-10-26 16:21:41 1603729301500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33768 2020-10-26 16:21:43 1603729303000\n",
      "33769 2020-10-26 16:21:43 1603729303500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33771 2020-10-26 16:21:44 1603729304000\n",
      "33770 2020-10-26 16:21:44 1603729304500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33774 2020-10-26 16:21:47 1603729307000\n",
      "33775 2020-10-26 16:21:47 1603729307500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33778 2020-10-26 16:21:50 1603729310000\n",
      "33779 2020-10-26 16:21:50 1603729310500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33782 2020-10-26 16:21:53 1603729313000\n",
      "33783 2020-10-26 16:21:53 1603729313500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33810 2020-10-26 16:22:19 1603729339000\n",
      "33809 2020-10-26 16:22:19 1603729339500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33813 2020-10-26 16:22:22 1603729342000\n",
      "33814 2020-10-26 16:22:22 1603729342500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33823 2020-10-26 16:22:31 1603729351000\n",
      "33824 2020-10-26 16:22:31 1603729351500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33825 2020-10-26 16:22:32 1603729352000\n",
      "33826 2020-10-26 16:22:32 1603729352500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33832 2020-10-26 16:22:38 1603729358000\n",
      "33833 2020-10-26 16:22:38 1603729358500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33834 2020-10-26 16:22:39 1603729359000\n",
      "33835 2020-10-26 16:22:39 1603729359500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33854 2020-10-26 16:22:58 1603729378000\n",
      "33855 2020-10-26 16:22:58 1603729378500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33861 2020-10-26 16:23:03 1603729383000\n",
      "33860 2020-10-26 16:23:03 1603729383500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33900 2020-10-26 16:23:42 1603729422000\n",
      "33901 2020-10-26 16:23:42 1603729422500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33903 2020-10-26 16:23:44 1603729424000\n",
      "33904 2020-10-26 16:23:44 1603729424500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33909 2020-10-26 16:23:49 1603729429000\n",
      "33910 2020-10-26 16:23:49 1603729429500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33913 2020-10-26 16:23:51 1603729431000\n",
      "33912 2020-10-26 16:23:51 1603729431500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33929 2020-10-26 16:24:07 1603729447000\n",
      "33930 2020-10-26 16:24:07 1603729447500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33933 2020-10-26 16:24:09 1603729449000\n",
      "33932 2020-10-26 16:24:09 1603729449500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33936 2020-10-26 16:24:12 1603729452000\n",
      "33937 2020-10-26 16:24:12 1603729452500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33939 2020-10-26 16:24:14 1603729454000\n",
      "33940 2020-10-26 16:24:14 1603729454500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33949 2020-10-26 16:24:23 1603729463000\n",
      "33950 2020-10-26 16:24:23 1603729463500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33953 2020-10-26 16:24:25 1603729465000\n",
      "33952 2020-10-26 16:24:25 1603729465500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33955 2020-10-26 16:24:27 1603729467000\n",
      "33956 2020-10-26 16:24:27 1603729467333\n",
      "33957 2020-10-26 16:24:27 1603729467666\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33959 2020-10-26 16:24:29 1603729469000\n",
      "33960 2020-10-26 16:24:29 1603729469500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33963 2020-10-26 16:24:31 1603729471000\n",
      "33964 2020-10-26 16:24:31 1603729471333\n",
      "33962 2020-10-26 16:24:31 1603729471666\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33965 2020-10-26 16:24:32 1603729472000\n",
      "33966 2020-10-26 16:24:32 1603729472500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33968 2020-10-26 16:24:34 1603729474000\n",
      "33969 2020-10-26 16:24:34 1603729474500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33970 2020-10-26 16:24:35 1603729475000\n",
      "33971 2020-10-26 16:24:35 1603729475500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33972 2020-10-26 16:24:36 1603729476000\n",
      "33973 2020-10-26 16:24:36 1603729476500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33975 2020-10-26 16:24:37 1603729477000\n",
      "33974 2020-10-26 16:24:37 1603729477500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33976 2020-10-26 16:24:38 1603729478000\n",
      "33977 2020-10-26 16:24:38 1603729478500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33978 2020-10-26 16:24:39 1603729479000\n",
      "33979 2020-10-26 16:24:39 1603729479500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33980 2020-10-26 16:24:40 1603729480000\n",
      "33981 2020-10-26 16:24:40 1603729480250\n",
      "33982 2020-10-26 16:24:40 1603729480500\n",
      "33983 2020-10-26 16:24:40 1603729480750\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33986 2020-10-26 16:24:41 1603729481000\n",
      "33984 2020-10-26 16:24:41 1603729481333\n",
      "33985 2020-10-26 16:24:41 1603729481666\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33987 2020-10-26 16:24:42 1603729482000\n",
      "33988 2020-10-26 16:24:42 1603729482200\n",
      "33989 2020-10-26 16:24:42 1603729482400\n",
      "33990 2020-10-26 16:24:42 1603729482600\n",
      "33991 2020-10-26 16:24:42 1603729482800\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "33992 2020-10-26 16:24:43 1603729483000\n",
      "33993 2020-10-26 16:24:43 1603729483500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "34001 2020-10-26 16:24:51 1603729491000\n",
      "34002 2020-10-26 16:24:51 1603729491500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "34006 2020-10-26 16:24:52 1603729492000\n",
      "34007 2020-10-26 16:24:52 1603729492200\n",
      "34005 2020-10-26 16:24:52 1603729492400\n",
      "34004 2020-10-26 16:24:52 1603729492600\n",
      "34003 2020-10-26 16:24:52 1603729492800\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "34008 2020-10-26 16:24:53 1603729493000\n",
      "34009 2020-10-26 16:24:53 1603729493500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "34010 2020-10-26 16:24:54 1603729494000\n",
      "34011 2020-10-26 16:24:54 1603729494250\n",
      "34012 2020-10-26 16:24:54 1603729494500\n",
      "34013 2020-10-26 16:24:54 1603729494750\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "34016 2020-10-26 16:24:55 1603729495000\n",
      "34015 2020-10-26 16:24:55 1603729495333\n",
      "34014 2020-10-26 16:24:55 1603729495666\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "34017 2020-10-26 16:24:56 1603729496000\n",
      "34018 2020-10-26 16:24:56 1603729496333\n",
      "34019 2020-10-26 16:24:56 1603729496666\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "34224 2020-10-26 16:34:41 1603730081000\n",
      "34225 2020-10-26 16:34:41 1603730081500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "34274 2020-10-26 16:35:28 1603730128000\n",
      "34272 2020-10-26 16:35:28 1603730128333\n",
      "34273 2020-10-26 16:35:28 1603730128666\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "8183 2020-10-26 15:50:08 1603727408000\n",
      "8182 2020-10-26 15:50:08 1603727408500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "8186 2020-10-26 15:50:11 1603727411000\n",
      "8185 2020-10-26 15:50:11 1603727411500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "4 2020-10-26 15:50:21 1603727421000\n",
      "3 2020-10-26 15:50:21 1603727421500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "7 2020-10-26 15:50:24 1603727424000\n",
      "6 2020-10-26 15:50:24 1603727424500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "9 2020-10-26 15:50:26 1603727426000\n",
      "8 2020-10-26 15:50:26 1603727426500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "26 2020-10-26 15:50:43 1603727443000\n",
      "25 2020-10-26 15:50:43 1603727443500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "28 2020-10-26 15:50:45 1603727445000\n",
      "27 2020-10-26 15:50:45 1603727445500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "46 2020-10-26 15:51:03 1603727463000\n",
      "45 2020-10-26 15:51:03 1603727463500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "48 2020-10-26 15:51:05 1603727465000\n",
      "47 2020-10-26 15:51:05 1603727465500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "50 2020-10-26 15:51:07 1603727467000\n",
      "49 2020-10-26 15:51:07 1603727467500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "67 2020-10-26 15:51:25 1603727485000\n",
      "68 2020-10-26 15:51:25 1603727485500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "71 2020-10-26 15:51:28 1603727488000\n",
      "70 2020-10-26 15:51:28 1603727488500\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward---\n",
      "85 2020-10-26 15:51:43 1603727503000\n",
      "86 2020-10-26 15:51:43 1603727504000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "88 2020-10-26 15:51:45 1603727505000\n",
      "87 2020-10-26 15:51:45 1603727505500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "108 2020-10-26 15:52:05 1603727525000\n",
      "107 2020-10-26 15:52:05 1603727525500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "110 2020-10-26 15:52:07 1603727527000\n",
      "109 2020-10-26 15:52:07 1603727527500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "115 2020-10-26 15:52:12 1603727532000\n",
      "114 2020-10-26 15:52:12 1603727532500\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward---\n",
      "127 2020-10-26 15:52:25 1603727545000\n",
      "128 2020-10-26 15:52:25 1603727546000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "129 2020-10-26 15:52:27 1603727547000\n",
      "130 2020-10-26 15:52:27 1603727547500\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward---\n",
      "144 2020-10-26 15:52:42 1603727562000\n",
      "145 2020-10-26 15:52:42 1603727563000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "148 2020-10-26 15:52:46 1603727566000\n",
      "149 2020-10-26 15:52:46 1603727566500\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward---\n",
      "155 2020-10-26 15:52:53 1603727573000\n",
      "156 2020-10-26 15:52:53 1603727574000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "165 2020-10-26 15:53:02 1603727582000\n",
      "164 2020-10-26 15:53:02 1603727582500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "191 2020-10-26 15:53:28 1603727608000\n",
      "190 2020-10-26 15:53:28 1603727608500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "223 2020-10-26 15:54:00 1603727640000\n",
      "222 2020-10-26 15:54:00 1603727640500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "260 2020-10-26 15:54:37 1603727677000\n",
      "259 2020-10-26 15:54:37 1603727677500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "265 2020-10-26 15:54:42 1603727682000\n",
      "264 2020-10-26 15:54:42 1603727682500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "268 2020-10-26 15:54:45 1603727685000\n",
      "267 2020-10-26 15:54:45 1603727685500\n",
      "---Handling Duplicates, Fill gap with duplicate, Backward---\n",
      "271 2020-10-26 15:54:48 1603727688000\n",
      "270 2020-10-26 15:54:48 1603727687000\n",
      "---Handling Duplicates, Fill gap with duplicate, Backward---\n",
      "274 2020-10-26 15:54:51 1603727691000\n",
      "273 2020-10-26 15:54:51 1603727690000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "277 2020-10-26 15:54:54 1603727694000\n",
      "276 2020-10-26 15:54:54 1603727694500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "282 2020-10-26 15:55:00 1603727700000\n",
      "283 2020-10-26 15:55:00 1603727700500\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward---\n",
      "291 2020-10-26 15:55:09 1603727709000\n",
      "292 2020-10-26 15:55:09 1603727710000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "296 2020-10-26 15:55:14 1603727714000\n",
      "297 2020-10-26 15:55:14 1603727714500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "300 2020-10-26 15:55:18 1603727718000\n",
      "301 2020-10-26 15:55:18 1603727718500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "308 2020-10-26 15:55:25 1603727725000\n",
      "307 2020-10-26 15:55:25 1603727725500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "310 2020-10-26 15:55:27 1603727727000\n",
      "309 2020-10-26 15:55:27 1603727727500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "317 2020-10-26 15:55:34 1603727734000\n",
      "316 2020-10-26 15:55:34 1603727734500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "325 2020-10-26 15:55:42 1603727742000\n",
      "324 2020-10-26 15:55:42 1603727742500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "334 2020-10-26 15:55:51 1603727751000\n",
      "333 2020-10-26 15:55:51 1603727751500\n",
      "---Handling Duplicates, Fill gap with duplicate, Backward---\n",
      "368 2020-10-26 15:56:25 1603727785000\n",
      "367 2020-10-26 15:56:25 1603727784000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "380 2020-10-26 15:56:37 1603727797000\n",
      "379 2020-10-26 15:56:37 1603727797500\n",
      "---Handling Duplicates, Fill gap with duplicate, Backward---\n",
      "385 2020-10-26 15:56:42 1603727802000\n",
      "384 2020-10-26 15:56:42 1603727801000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "388 2020-10-26 15:56:45 1603727805000\n",
      "387 2020-10-26 15:56:45 1603727805500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "391 2020-10-26 15:56:48 1603727808000\n",
      "390 2020-10-26 15:56:48 1603727808500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "394 2020-10-26 15:56:51 1603727811000\n",
      "393 2020-10-26 15:56:51 1603727811500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "397 2020-10-26 15:56:54 1603727814000\n",
      "396 2020-10-26 15:56:54 1603727814500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "400 2020-10-26 15:56:57 1603727817000\n",
      "399 2020-10-26 15:56:57 1603727817500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "403 2020-10-26 15:57:00 1603727820000\n",
      "402 2020-10-26 15:57:00 1603727820500\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward---\n",
      "405 2020-10-26 15:57:03 1603727823000\n",
      "406 2020-10-26 15:57:03 1603727824000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "409 2020-10-26 15:57:06 1603727826000\n",
      "408 2020-10-26 15:57:06 1603727826500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "414 2020-10-26 15:57:11 1603727831000\n",
      "413 2020-10-26 15:57:11 1603727831500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "444 2020-10-26 15:57:42 1603727862000\n",
      "445 2020-10-26 15:57:42 1603727862500\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward---\n",
      "457 2020-10-26 15:57:55 1603727875000\n",
      "458 2020-10-26 15:57:55 1603727876000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "459 2020-10-26 15:57:57 1603727877000\n",
      "460 2020-10-26 15:57:57 1603727877500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "478 2020-10-26 15:58:15 1603727895000\n",
      "477 2020-10-26 15:58:15 1603727895500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "480 2020-10-26 15:58:17 1603727897000\n",
      "479 2020-10-26 15:58:17 1603727897500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "498 2020-10-26 15:58:35 1603727915000\n",
      "497 2020-10-26 15:58:35 1603727915500\n",
      "---Handling Duplicates, Fill gap with duplicate, Backward---\n",
      "500 2020-10-26 15:58:37 1603727917000\n",
      "499 2020-10-26 15:58:37 1603727916000\n",
      "---Handling Duplicates, Fill gap with duplicate, Backward---\n",
      "505 2020-10-26 15:58:42 1603727922000\n",
      "504 2020-10-26 15:58:42 1603727921000\n",
      "---Handling Duplicates, Fill gap with duplicate, Backward---\n",
      "511 2020-10-26 15:58:48 1603727928000\n",
      "510 2020-10-26 15:58:48 1603727927000\n",
      "---Handling Duplicates, Fill gap with duplicate, Backward---\n",
      "514 2020-10-26 15:58:51 1603727931000\n",
      "513 2020-10-26 15:58:51 1603727930000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "614 2020-10-26 16:00:31 1603728031000\n",
      "613 2020-10-26 16:00:31 1603728031500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "663 2020-10-26 16:01:20 1603728080000\n",
      "662 2020-10-26 16:01:20 1603728080500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "718 2020-10-26 16:02:15 1603728135000\n",
      "717 2020-10-26 16:02:15 1603728135500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "723 2020-10-26 16:02:20 1603728140000\n",
      "722 2020-10-26 16:02:20 1603728140500\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward---\n",
      "736 2020-10-26 16:02:34 1603728154000\n",
      "737 2020-10-26 16:02:34 1603728155000\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward---\n",
      "760 2020-10-26 16:02:58 1603728178000\n",
      "761 2020-10-26 16:02:58 1603728179000\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward---\n",
      "780 2020-10-26 16:03:18 1603728198000\n",
      "781 2020-10-26 16:03:18 1603728199000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "801 2020-10-26 16:03:38 1603728218000\n",
      "800 2020-10-26 16:03:38 1603728218500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "804 2020-10-26 16:03:41 1603728221000\n",
      "803 2020-10-26 16:03:41 1603728221500\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward---\n",
      "820 2020-10-26 16:03:58 1603728238000\n",
      "821 2020-10-26 16:03:58 1603728239000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "841 2020-10-26 16:04:18 1603728258000\n",
      "840 2020-10-26 16:04:18 1603728258500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "861 2020-10-26 16:04:38 1603728278000\n",
      "860 2020-10-26 16:04:38 1603728278500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "866 2020-10-26 16:04:43 1603728283000\n",
      "865 2020-10-26 16:04:43 1603728283500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "883 2020-10-26 16:05:00 1603728300000\n",
      "882 2020-10-26 16:05:00 1603728300500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "885 2020-10-26 16:05:02 1603728302000\n",
      "884 2020-10-26 16:05:02 1603728302500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "888 2020-10-26 16:05:05 1603728305000\n",
      "887 2020-10-26 16:05:05 1603728305500\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward---\n",
      "893 2020-10-26 16:05:11 1603728311000\n",
      "894 2020-10-26 16:05:11 1603728312000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "895 2020-10-26 16:05:13 1603728313000\n",
      "896 2020-10-26 16:05:13 1603728313500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "899 2020-10-26 16:05:16 1603728316000\n",
      "898 2020-10-26 16:05:16 1603728316500\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward---\n",
      "901 2020-10-26 16:05:19 1603728319000\n",
      "902 2020-10-26 16:05:19 1603728320000\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward---\n",
      "907 2020-10-26 16:05:25 1603728325000\n",
      "908 2020-10-26 16:05:25 1603728326000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "911 2020-10-26 16:05:28 1603728328000\n",
      "910 2020-10-26 16:05:28 1603728328500\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward---\n",
      "915 2020-10-26 16:05:33 1603728333000\n",
      "916 2020-10-26 16:05:33 1603728334000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "933 2020-10-26 16:05:50 1603728350000\n",
      "932 2020-10-26 16:05:50 1603728350500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "991 2020-10-26 16:06:48 1603728408000\n",
      "990 2020-10-26 16:06:48 1603728408500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1004 2020-10-26 16:07:01 1603728421000\n",
      "1003 2020-10-26 16:07:01 1603728421500\n",
      "---Handling Duplicates, Fill gap with duplicate, Backward---\n",
      "1013 2020-10-26 16:07:10 1603728430000\n",
      "1012 2020-10-26 16:07:10 1603728429000\n",
      "---Handling Duplicates, Fill gap with duplicate, Backward---\n",
      "1016 2020-10-26 16:07:13 1603728433000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1015 2020-10-26 16:07:13 1603728432000\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward and Backward---\n",
      "1018 2020-10-26 16:07:16 1603728435000\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward---\n",
      "1021 2020-10-26 16:07:19 1603728439000\n",
      "1022 2020-10-26 16:07:19 1603728440000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1025 2020-10-26 16:07:22 1603728442000\n",
      "1024 2020-10-26 16:07:22 1603728442500\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward---\n",
      "1029 2020-10-26 16:07:27 1603728447000\n",
      "1030 2020-10-26 16:07:27 1603728448000\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward---\n",
      "1032 2020-10-26 16:07:30 1603728450000\n",
      "1033 2020-10-26 16:07:30 1603728451000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1036 2020-10-26 16:07:34 1603728454000\n",
      "1037 2020-10-26 16:07:34 1603728454500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1098 2020-10-26 16:08:36 1603728516000\n",
      "1099 2020-10-26 16:08:36 1603728516500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1126 2020-10-26 16:09:03 1603728543000\n",
      "1125 2020-10-26 16:09:03 1603728543500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1131 2020-10-26 16:09:08 1603728548000\n",
      "1130 2020-10-26 16:09:08 1603728548500\n",
      "---Handling Duplicates, Fill gap with duplicate, Backward---\n",
      "1134 2020-10-26 16:09:11 1603728551000\n",
      "1133 2020-10-26 16:09:11 1603728550000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1137 2020-10-26 16:09:14 1603728554000\n",
      "1136 2020-10-26 16:09:14 1603728554500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1140 2020-10-26 16:09:17 1603728557000\n",
      "1139 2020-10-26 16:09:17 1603728557500\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward---\n",
      "1145 2020-10-26 16:09:23 1603728563000\n",
      "1146 2020-10-26 16:09:23 1603728564000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1148 2020-10-26 16:09:25 1603728565000\n",
      "1147 2020-10-26 16:09:25 1603728565500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1160 2020-10-26 16:09:37 1603728577000\n",
      "1159 2020-10-26 16:09:37 1603728577500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1206 2020-10-26 16:10:24 1603728624000\n",
      "1207 2020-10-26 16:10:24 1603728624500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1246 2020-10-26 16:11:03 1603728663000\n",
      "1245 2020-10-26 16:11:03 1603728663500\n",
      "---Handling Duplicates, Fill gap with duplicate, Backward---\n",
      "1257 2020-10-26 16:11:14 1603728674000\n",
      "1256 2020-10-26 16:11:14 1603728673000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1260 2020-10-26 16:11:17 1603728677000\n",
      "1259 2020-10-26 16:11:17 1603728677500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1263 2020-10-26 16:11:20 1603728680000\n",
      "1262 2020-10-26 16:11:20 1603728680500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1266 2020-10-26 16:11:23 1603728683000\n",
      "1265 2020-10-26 16:11:23 1603728683500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1269 2020-10-26 16:11:26 1603728686000\n",
      "1268 2020-10-26 16:11:26 1603728686500\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward---\n",
      "1271 2020-10-26 16:11:29 1603728689000\n",
      "1272 2020-10-26 16:11:29 1603728690000\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward---\n",
      "1274 2020-10-26 16:11:32 1603728692000\n",
      "1275 2020-10-26 16:11:32 1603728693000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1278 2020-10-26 16:11:35 1603728695000\n",
      "1277 2020-10-26 16:11:35 1603728695500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1280 2020-10-26 16:11:37 1603728697000\n",
      "1279 2020-10-26 16:11:37 1603728697500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1317 2020-10-26 16:12:14 1603728734000\n",
      "1316 2020-10-26 16:12:14 1603728734500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1373 2020-10-26 16:13:10 1603728790000\n",
      "1372 2020-10-26 16:13:10 1603728790500\n",
      "---Handling Duplicates, Fill gap with duplicate, Backward---\n",
      "1376 2020-10-26 16:13:13 1603728793000\n",
      "1375 2020-10-26 16:13:13 1603728792000\n",
      "---Handling Duplicates, Fill gap with duplicate, Backward---\n",
      "1379 2020-10-26 16:13:16 1603728796000\n",
      "1378 2020-10-26 16:13:16 1603728795000\n",
      "---Handling Duplicates, Fill gap with duplicate, Backward---\n",
      "1382 2020-10-26 16:13:19 1603728799000\n",
      "1381 2020-10-26 16:13:19 1603728798000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1388 2020-10-26 16:13:25 1603728805000\n",
      "1387 2020-10-26 16:13:25 1603728805500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1390 2020-10-26 16:13:27 1603728807000\n",
      "1389 2020-10-26 16:13:27 1603728807500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1393 2020-10-26 16:13:30 1603728810000\n",
      "1392 2020-10-26 16:13:30 1603728810500\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward---\n",
      "1395 2020-10-26 16:13:33 1603728813000\n",
      "1396 2020-10-26 16:13:33 1603728814000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1399 2020-10-26 16:13:36 1603728816000\n",
      "1398 2020-10-26 16:13:36 1603728816500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1404 2020-10-26 16:13:41 1603728821000\n",
      "1403 2020-10-26 16:13:41 1603728821500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1407 2020-10-26 16:13:44 1603728824000\n",
      "1406 2020-10-26 16:13:44 1603728824500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1424 2020-10-26 16:14:02 1603728842000\n",
      "1425 2020-10-26 16:14:02 1603728842500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1438 2020-10-26 16:14:16 1603728856000\n",
      "1439 2020-10-26 16:14:16 1603728856500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1442 2020-10-26 16:14:19 1603728859000\n",
      "1441 2020-10-26 16:14:19 1603728859500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1445 2020-10-26 16:14:22 1603728862000\n",
      "1444 2020-10-26 16:14:22 1603728862500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1447 2020-10-26 16:14:24 1603728864000\n",
      "1446 2020-10-26 16:14:24 1603728864500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1449 2020-10-26 16:14:26 1603728866000\n",
      "1448 2020-10-26 16:14:26 1603728866500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1495 2020-10-26 16:15:12 1603728912000\n",
      "1494 2020-10-26 16:15:12 1603728912500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1498 2020-10-26 16:15:15 1603728915000\n",
      "1497 2020-10-26 16:15:15 1603728915500\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward---\n",
      "1503 2020-10-26 16:15:21 1603728921000\n",
      "1504 2020-10-26 16:15:21 1603728922000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1506 2020-10-26 16:15:23 1603728923000\n",
      "1505 2020-10-26 16:15:23 1603728923500\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward and Backward---\n",
      "1508 2020-10-26 16:15:26 1603728925000\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward---\n",
      "1511 2020-10-26 16:15:29 1603728929000\n",
      "1512 2020-10-26 16:15:29 1603728930000\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward---\n",
      "1514 2020-10-26 16:15:32 1603728932000\n",
      "1515 2020-10-26 16:15:32 1603728933000\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward---\n",
      "1517 2020-10-26 16:15:35 1603728935000\n",
      "1518 2020-10-26 16:15:35 1603728936000\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward---\n",
      "1520 2020-10-26 16:15:38 1603728938000\n",
      "1521 2020-10-26 16:15:38 1603728939000\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward---\n",
      "1525 2020-10-26 16:15:43 1603728943000\n",
      "1526 2020-10-26 16:15:43 1603728944000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1530 2020-10-26 16:15:48 1603728948000\n",
      "1531 2020-10-26 16:15:48 1603728948500\n",
      "---Handling Duplicates, Fill gap with duplicate, Backward---\n",
      "1571 2020-10-26 16:16:28 1603728988000\n",
      "1570 2020-10-26 16:16:28 1603728987000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1614 2020-10-26 16:17:11 1603729031000\n",
      "1613 2020-10-26 16:17:11 1603729031500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1617 2020-10-26 16:17:14 1603729034000\n",
      "1616 2020-10-26 16:17:14 1603729034500\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward---\n",
      "1620 2020-10-26 16:17:18 1603729038000\n",
      "1621 2020-10-26 16:17:18 1603729039000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1624 2020-10-26 16:17:21 1603729041000\n",
      "1623 2020-10-26 16:17:21 1603729041500\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward---\n",
      "1638 2020-10-26 16:17:36 1603729056000\n",
      "1639 2020-10-26 16:17:36 1603729057000\n",
      "---Handling Duplicates, Fill gap with duplicate, Forward---\n",
      "1643 2020-10-26 16:17:41 1603729061000\n",
      "1644 2020-10-26 16:17:41 1603729062000\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1664 2020-10-26 16:18:02 1603729082000\n",
      "1665 2020-10-26 16:18:02 1603729082500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1711 2020-10-26 16:18:48 1603729128000\n",
      "1710 2020-10-26 16:18:48 1603729128500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1716 2020-10-26 16:18:53 1603729133000\n",
      "1715 2020-10-26 16:18:53 1603729133500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1735 2020-10-26 16:19:12 1603729152000\n",
      "1734 2020-10-26 16:19:12 1603729152500\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "1738 2020-10-26 16:19:15 1603729155000\n",
      "1737 2020-10-26 16:19:15 1603729155500\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: './result_8.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-4ebacbe46389>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m \u001b[0mdf_c2_clean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_od_clean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_evr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_folder_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;31m#unit_test(sample_output_filename[0], 'C2', df_c2_clean)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-4ebacbe46389>\u001b[0m in \u001b[0;36mprocess_evr\u001b[1;34m(input_folder_path, start_time, end_time)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# Merge other Operating Data to C2 Dataframe based on Timestampms (ordered manner)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[0mdf_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge_ordered\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_c2_clean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_od_clean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'outer'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Timestampms'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m             \u001b[0mdf_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./result_8.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m             \u001b[1;31m# Merge C2 and OD files first\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3382\u001b[0m         )\n\u001b[0;32m   3383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3384\u001b[1;33m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[0;32m   3385\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3386\u001b[0m             \u001b[0mline_terminator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mline_terminator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\io\\formats\\format.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1081\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1082\u001b[0m         )\n\u001b[1;32m-> 1083\u001b[1;33m         \u001b[0mcsv_formatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1084\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1085\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    226\u001b[0m         \"\"\"\n\u001b[0;32m    227\u001b[0m         \u001b[1;31m# apply compression and byte/text conversion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m         with get_handle(\n\u001b[0m\u001b[0;32m    229\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    637\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 639\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    640\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: './result_8.csv'"
     ]
    }
   ],
   "source": [
    "input_folder_path =  './EVR_T23_Car46_261020/'\n",
    "#input_folder_path = './Train 23 14NOV2019 Work Folder/raw logs'\n",
    "start_time = '2020/10/26 15:50:00' \n",
    "end_time = '2020/10/26 16:50:00'\n",
    "#start_time = '2020/10/26 04:36:45'\n",
    "#end_time = '2020/10/26 04:37:09'\n",
    "sample_output_filename = ['./EVR_T23_Car46_261020/preprocessing_output/EVR_20201026_1550_to_20201026_1650.csv']\n",
    "\n",
    "# input_folder_path = './Train 23 14NOV2019 Work Folder/raw logs'\n",
    "# #input_folder_path = './Train 23 14NOV2019 Work Folder/raw logs'\n",
    "# start_time = '2019/11/14 17:00:00' \n",
    "# end_time = '2019/11/14 18:00:00'\n",
    "# sample_output_filename = ['./Train 23 14NOV2019 Work Folder/preprocessing_output/EVR_20191114_1700_to_20191114_1800.csv']\n",
    "\n",
    "# input_folder_path = './Train 23 14NOV2019 Work Folder/raw logs'\n",
    "# #input_folder_path = './Train 23 14NOV2019 Work Folder/raw logs'\n",
    "# start_time = '2019/11/14 18:00:00' \n",
    "# end_time = '2019/11/14 19:00:00'\n",
    "# sample_output_filename = ['./Train 23 14NOV2019 Work Folder/preprocessing_output/EVR_20191114_1800_to_20191114_1900.csv']\n",
    "\n",
    "def process_evr (input_folder_path, start_time, end_time):\n",
    "    # Determine time difference between start and end time\n",
    "    datetime_format = '%Y/%m/%d %H:%M:%S'\n",
    "    start_time = datetime.datetime.strptime(start_time, datetime_format)\n",
    "    end_time = datetime.datetime.strptime(end_time, datetime_format)\n",
    "    start_time_new = start_time.strftime('%m/%d/%Y %H:%M:%S')\n",
    "    end_time_new = end_time.strftime('%m/%d/%Y %H:%M:%S')\n",
    "    # Check if EVR folder exist\n",
    "    if not os.path.exists(os.path.join(input_folder_path, 'EVR')):\n",
    "        print(\"EVR folder does not exist. Kindly check the folder strucutre. No EVR processing will be performed\")\n",
    "    else:\n",
    "        print(\"EVR folder exist!\")\n",
    "        # Check if C2 file exist\n",
    "        c2_pattern = 'EVR_Car\\d+_\\d+_C2.txt'\n",
    "        norm_pattern = 'EVR_Car\\d+_\\d+.txt'\n",
    "        c2_flag = False\n",
    "        norm_flag = False\n",
    "        for evr_file in os.listdir(os.path.join(input_folder_path, 'EVR')):\n",
    "            if re.search(c2_pattern, evr_file):\n",
    "                c2_filename = evr_file\n",
    "                c2_flag = True\n",
    "            if re.search(norm_pattern,evr_file):\n",
    "                norm_filename = evr_file\n",
    "                norm_flag = True\n",
    "\n",
    "        if not (c2_flag and norm_flag):\n",
    "            sys.exit(\"Missing EVR files! Kindly check EVR folder!\")\n",
    "        else:\n",
    "            # Process C2 file\n",
    "            df_c2 = pd.read_csv(os.path.join(input_folder_path, 'EVR', c2_filename), sep=\";\")\n",
    "            df_c2['Date'] = pd.to_datetime(df_c2['Date'], format='%m/%d/%Y %H:%M:%S')\n",
    "            df_c2_period = df_c2.loc[(df_c2['Date'] >= start_time_new) & (df_c2['Date'] <= end_time_new)]\n",
    "            #df_c2_clean_timestamp = preprocess_timestamp_old(df_c2_period)\n",
    "            df_c2_clean = process_dataframe(df_c2_period, 'C2')\n",
    "            #df_c2_clean.to_csv('./result_c2.csv', index=False, header=True)\n",
    "            \n",
    "            # Processs Operating Data file\n",
    "            df_od = pd.read_csv(os.path.join(input_folder_path, 'EVR', norm_filename), sep=\";\")\n",
    "            df_od['Date'] = pd.to_datetime(df_od['Date'], format='%m/%d/%Y %H:%M:%S')\n",
    "            df_od_period = df_od.loc[(df_od['Date'] >= start_time_new) & (df_od['Date'] <= end_time_new)]\n",
    "            #df_od_clean_timestamp = preprocess_timestamp_old(df_od_period)\n",
    "            df_od_clean = process_dataframe(df_od_period, 'OD')\n",
    "            #df_od_clean.to_csv('./result_od.csv', index=False, header=True)\n",
    "            \n",
    "            # Merge other Operating Data to C2 Dataframe based on Timestampms (ordered manner)\n",
    "            df_result = pd.merge_ordered(df_c2_clean, df_od_clean, how='outer', on='Timestampms')\n",
    "            #df_result.to_csv('./result_8.csv', index=False, header=True)\n",
    "            # Merge C2 and OD files first\n",
    "            \n",
    "            # Group resulting same real timestamp together\n",
    "            df_result = df_result.groupby(\"Timestampms\").last().reset_index()\n",
    "\n",
    "            # Remove similar record numbers for both EC2 and EOD\n",
    "            #df_duplicate = df[df.duplicated('EC2_Record Number',keep='first')]\n",
    "            filter_col = [col for col in df_result if col.startswith('EC2_')]\n",
    "            df_result.loc[df_result.duplicated('EC2_Record Number',keep='first'), filter_col] = np.NaN\n",
    "            filter_col = [col for col in df_result if col.startswith('EOD_')]\n",
    "            df_result.loc[df_result.duplicated('EOD_Record Number',keep='first'), filter_col] = np.NaN\n",
    "            \n",
    "            #df_result.to_csv('./result_8.csv', index=False, header=True)\n",
    "            # Clean Timestamp\n",
    "            #df_result = preprocess_timestamp(df_result)\n",
    "            \n",
    "            \n",
    "    return df_c2_clean, df_od_clean, df_result\n",
    "\n",
    "\n",
    "df_c2_clean, df_od_clean, df_result = process_evr(input_folder_path, start_time, end_time)\n",
    "\n",
    "#unit_test(sample_output_filename[0], 'C2', df_c2_clean)  \n",
    "#unit_test(sample_output_filename[0], 'OD', df_od_clean)  \n",
    "unit_test_all(sample_output_filename[0], df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe(df, name):\n",
    "    # Drop columns with NaNs\n",
    "    df = df.dropna(how='any',axis=1)\n",
    "    # Sort df based on column 'Date'\n",
    "    df = df.sort_values(by='Date',ascending=True).reset_index(drop=True)\n",
    "    # Convert datetime to timestamp (Unix) in milisecond resolution\n",
    "    df['Timestamp'] = df.Date.values.astype(np.int64) // 10 ** 9 *1000\n",
    "    # Add column for Real_Time Timestampms\n",
    "    df['Timestampms'] = df['Timestamp']\n",
    "    \n",
    "     # Find duplicates datetime in column 'Date'\n",
    "    df_duplicate = df[df.duplicated('Timestampms',keep=False)]\n",
    "    for index, unique_datetime in enumerate (df_duplicate['Timestampms'].unique()):\n",
    "        timestamp_df = df_duplicate.loc[df_duplicate['Timestampms'] == unique_datetime]\n",
    "        \n",
    "        timestamp_df_next = pd.DataFrame()\n",
    "        if index+1 < len(df_duplicate['Timestampms'].unique()):\n",
    "            timestamp_df_next = df_duplicate.loc[df_duplicate['Timestampms'] == df_duplicate['Timestampms'].unique()[index+1]]         \n",
    "        \n",
    "        difference_previous, difference_next = 0 , 0\n",
    "        if timestamp_df.index[0]-1 >=0 and timestamp_df.index[-1]+1 <= len(df)-1:\n",
    "            difference_previous = unique_datetime - df.loc[timestamp_df.index[0]-1,'Timestampms']\n",
    "            difference_next = df.loc[timestamp_df.index[-1]+1,'Timestampms'] - unique_datetime\n",
    "        elif timestamp_df.index[-1] == len(df)-1:\n",
    "            difference_previous = unique_datetime - df.loc[timestamp_df.index[0]-1,'Timestampms']\n",
    "        elif timestamp_df.index[0] == 0:\n",
    "            difference_next = df.loc[timestamp_df.index[-1]+1,'Timestampms'] - unique_datetime\n",
    "            \n",
    "        if df.loc[timestamp_df.index[0],'Record Number'] > df.loc[timestamp_df.index[-1],'Record Number']:\n",
    "            #Split Duplicates Evenly\n",
    "            print(\"---Handling Duplicates, Split duplicate evenly---\")\n",
    "            # For every Timestamp (1000 millisecond), find the interval\n",
    "            interval_ms = round(1000 / len(timestamp_df.index))\n",
    "            for counter in range (0, len(timestamp_df.index)):\n",
    "                df.loc[timestamp_df.index[counter],'Timestampms'] = df.loc[timestamp_df.index[counter],'Timestampms'] + counter*interval_ms\n",
    "                print(df.loc[timestamp_df.index[counter],'Record Number'], df.loc[timestamp_df.index[counter],'Date'], df.loc[timestamp_df.index[counter],'Timestampms'])\n",
    "                \n",
    "        else:\n",
    "            if not(difference_previous == 0 and difference_next == 0) and ((difference_previous == difference_next and difference_previous <= 1000 and difference_next <= 1000) or \n",
    "                                                                       (difference_previous <= 1000 and difference_next <= 1000) or\n",
    "                                                                       (difference_previous == 1000 and difference_next == 0) or\n",
    "                                                                       (difference_previous == 0 and difference_next == 1000)):\n",
    "                print(\"---Handling Duplicates, Split duplicate evenly---\")\n",
    "                # For every Timestamp (1000 millisecond), find the interval\n",
    "                interval_ms = round(1000 / len(timestamp_df.index))\n",
    "                for counter in range (0, len(timestamp_df.index)):\n",
    "                    df.loc[timestamp_df.index[counter],'Timestampms'] = df.loc[timestamp_df.index[counter],'Timestampms'] + counter*interval_ms\n",
    "                    print(df.loc[timestamp_df.index[counter],'Record Number'], df.loc[timestamp_df.index[counter],'Date'], df.loc[timestamp_df.index[counter],'Timestampms'])\n",
    "            elif difference_previous == 1000 and difference_next > 1000:\n",
    "                print(\"---Handling Duplicates, Fill gap with duplicate, Forward---\")\n",
    "                for counter in range (0, len(timestamp_df.index)):\n",
    "                    df.loc[timestamp_df.index[counter],'Timestampms'] = df.loc[timestamp_df.index[counter],'Timestampms'] + counter*1000\n",
    "                    print(df.loc[timestamp_df.index[counter],'Record Number'], df.loc[timestamp_df.index[counter],'Date'], df.loc[timestamp_df.index[counter],'Timestampms'])\n",
    "            elif(difference_previous > 1000 and difference_next == 1000):\n",
    "                # Loop backwards\n",
    "                print(\"---Handling Duplicates, Fill gap with duplicate, Backward---\")\n",
    "                count = 0\n",
    "                for counter in range (len(timestamp_df.index), 0, -1):\n",
    "                    df.loc[timestamp_df.index[counter-1],'Timestampms'] = df.loc[timestamp_df.index[counter-1],'Timestampms'] - count*1000\n",
    "                    count +=1\n",
    "                    print(df.loc[timestamp_df.index[counter-1],'Record Number'], df.loc[timestamp_df.index[counter-1],'Date'], df.loc[timestamp_df.index[counter-1],'Timestampms'])\n",
    "            #Implement fill preceding gap first\n",
    "            elif(difference_previous == difference_next) and (difference_previous>1000 and difference_next>1000):\n",
    "                print(\"---Handling Duplicates, Fill gap with duplicate, Forward and Backward---\")\n",
    "                mid_point = len(timestamp_df.index) // 2\n",
    "                left_count, right_count = 0, 0\n",
    "                for counter in range (1, len(timestamp_df.index)):\n",
    "                    if counter % 2 == 0:\n",
    "                        right_count +=1\n",
    "                        df.loc[timestamp_df.index[mid_point+right_count],'Timestampms'] = df.loc[timestamp_df.index[mid_point],'Timestampms'] + right_count*1000\n",
    "                        print(df.loc[timestamp_df.index[mid_point+right_count],'Record Number'], df.loc[timestamp_df.index[mid_point+right_count],'Date'], df.loc[timestamp_df.index[mid_point+right_count],'Timestampms'])\n",
    "                    else:\n",
    "                        left_count +=1\n",
    "                        df.loc[timestamp_df.index[mid_point-left_count],'Timestampms'] = df.loc[timestamp_df.index[mid_point],'Timestampms'] - left_count*1000\n",
    "                        print(df.loc[timestamp_df.index[mid_point-left_count],'Record Number'], df.loc[timestamp_df.index[mid_point-left_count],'Date'], df.loc[timestamp_df.index[mid_point-left_count],'Timestampms'])\n",
    "    \n",
    "    # Add Prefix to Columns name (e.g. EC2/EOD_***) except Timestampms\n",
    "    if name == 'C2':\n",
    "        df.columns = df.columns.map(lambda x : 'EC2_'+x if x !='Timestampms' else x)\n",
    "    elif name == 'OD':\n",
    "        df.columns = df.columns.map(lambda x : 'EOD_'+x if x !='Timestampms' else x)\n",
    "        \n",
    "    # Shift Timestampms to first column\n",
    "    df = df[ ['Timestampms'] + [ col for col in df.columns if col != 'Timestampms' ] ]\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe(df, name):\n",
    "    # Drop columns with NaNs\n",
    "    df = df.dropna(how='any',axis=1)\n",
    "            \n",
    "    # Sort df based on column 'Date'\n",
    "    df = df.sort_values(by='Date',ascending=True).reset_index(drop=True)\n",
    "    \n",
    "    # Convert datetime to timestamp (Unix) in milisecond resolution\n",
    "    df['Timestamp'] = df.Date.values.astype(np.int64) // 10 ** 9 *1000\n",
    "    # Add column for Real_Time Timestampms\n",
    "    df['Timestampms'] = df['Timestamp']\n",
    "    \n",
    "     # Find duplicates datetime in column 'Date'\n",
    "    df_duplicate = df[df.duplicated('Timestampms',keep=False)]\n",
    "    for index, unique_datetime in enumerate (df_duplicate['Timestampms'].unique()):\n",
    "        timestamp_df = df_duplicate.loc[df_duplicate['Timestampms'] == unique_datetime]     \n",
    "        \n",
    "        if index < len(df_duplicate['Timestampms'].unique())-1:\n",
    "            timestamp_df_next = df_duplicate.loc[df_duplicate['Timestampms'] == df_duplicate['Timestampms'].unique()[index+1]]\n",
    "        if index > 0:\n",
    "             timestamp_df_previous = df_duplicate.loc[df_duplicate['Timestampms'] == df_duplicate['Timestampms'].unique()[index-1]]\n",
    "        difference_previous, difference_next = 0 , 0\n",
    "        if timestamp_df.index[0]-1 >=0 and timestamp_df.index[-1]+1 <= len(df)-1:\n",
    "            difference_previous = unique_datetime - df.loc[timestamp_df.index[0]-1,'Timestampms']\n",
    "            difference_next = df.loc[timestamp_df.index[-1]+1,'Timestampms'] - unique_datetime\n",
    "        elif timestamp_df.index[-1] == len(df)-1:\n",
    "            difference_previous = unique_datetime - df.loc[timestamp_df.index[0]-1,'Timestampms']\n",
    "        elif timestamp_df.index[0] == 0:\n",
    "            difference_next = df.loc[timestamp_df.index[-1]+1,'Timestampms'] - unique_datetime\n",
    "            \n",
    "        #print(timestamp_df.index)\n",
    "        if df.loc[timestamp_df.index[0],'Record Number'] < df.loc[timestamp_df.index[-1],'Record Number']:\n",
    "            if (difference_previous == 1000 and difference_next >= 2000):\n",
    "                print(\"---Handling Duplicates, Fill gap with duplicate, Forward---\")\n",
    "                for counter in range (0, len(timestamp_df.index)):\n",
    "                    df.loc[timestamp_df.index[counter],'Timestampms'] = df.loc[timestamp_df.index[counter],'Timestampms'] + counter*1000\n",
    "                    print(df.loc[timestamp_df.index[counter],'Record Number'], df.loc[timestamp_df.index[counter],'Date'], df.loc[timestamp_df.index[counter],'Timestampms'])\n",
    "            elif(difference_previous >= 2000 and difference_next == 1000):\n",
    "                # Loop backwards\n",
    "                print(\"---Handling Duplicates, Fill gap with duplicate, Backward---\")\n",
    "                count = 0\n",
    "                for counter in range (len(timestamp_df.index), 0, -1):\n",
    "                    df.loc[timestamp_df.index[counter-1],'Timestampms'] = df.loc[timestamp_df.index[counter-1],'Timestampms'] - count*1000\n",
    "                    count +=1\n",
    "                    print(df.loc[timestamp_df.index[counter-1],'Record Number'], df.loc[timestamp_df.index[counter-1],'Date'], df.loc[timestamp_df.index[counter-1],'Timestampms'])\n",
    "            #Implement fill preceding gap first\n",
    "            elif(difference_previous == difference_next) and (difference_previous>1000 and difference_next>1000):\n",
    "                print(\"---Handling Duplicates, Fill gap with duplicate, Forward and Backward---\")\n",
    "                mid_point = len(timestamp_df.index) // 2\n",
    "                left_count, right_count = 0, 0\n",
    "                for counter in range (1, len(timestamp_df.index)):\n",
    "                    if counter % 2 == 0:\n",
    "                        right_count +=1\n",
    "                        df.loc[timestamp_df.index[mid_point+right_count],'Timestampms'] = df.loc[timestamp_df.index[mid_point],'Timestampms'] + right_count*1000\n",
    "                        print(df.loc[timestamp_df.index[mid_point+right_count],'Record Number'], df.loc[timestamp_df.index[mid_point+right_count],'Date'], df.loc[timestamp_df.index[mid_point+right_count],'Timestampms'])\n",
    "                    else:\n",
    "                        left_count +=1\n",
    "                        df.loc[timestamp_df.index[mid_point-left_count],'Timestampms'] = df.loc[timestamp_df.index[mid_point],'Timestampms'] - left_count*1000\n",
    "                        print(df.loc[timestamp_df.index[mid_point-left_count],'Record Number'], df.loc[timestamp_df.index[mid_point-left_count],'Date'], df.loc[timestamp_df.index[mid_point-left_count],'Timestampms'])\n",
    "            elif not(difference_previous == 0 and difference_next == 0) and ((difference_previous == difference_next) or \n",
    "                                                                       (difference_previous <= 1000 and difference_next <= 1000) or\n",
    "                                                                       (difference_previous == 1000 and difference_next == 0) or\n",
    "                                                                       (difference_previous == 0 and difference_next == 1000)):\n",
    "                print(\"---Handling Duplicates, Split duplicate evenly---\")\n",
    "                # For every Timestamp (1000 millisecond), find the interval\n",
    "                interval_ms = round(1000 / len(timestamp_df.index))\n",
    "                for counter in range (0, len(timestamp_df.index)):\n",
    "                    df.loc[timestamp_df.index[counter],'Timestampms'] = df.loc[timestamp_df.index[counter],'Timestampms'] + counter*interval_ms\n",
    "                    print(df.loc[timestamp_df.index[counter],'Record Number'], df.loc[timestamp_df.index[counter],'Date'], df.loc[timestamp_df.index[counter],'Timestampms'])            \n",
    "        else:\n",
    "            if not(difference_previous == 0 and difference_next == 0) and ((difference_previous == difference_next) or \n",
    "                                                                       (difference_previous <= 1000 and difference_next <= 1000) or\n",
    "                                                                       (difference_previous == 1000 and difference_next == 0) or\n",
    "                                                                       (difference_previous == 0 and difference_next == 1000)):\n",
    "                print(\"---Handling Duplicates, Split duplicate evenly---\")\n",
    "                # For every Timestamp (1000 millisecond), find the interval\n",
    "                interval_ms = round(1000 / len(timestamp_df.index))\n",
    "                for counter in range (0, len(timestamp_df.index)):\n",
    "                    df.loc[timestamp_df.index[counter],'Timestampms'] = df.loc[timestamp_df.index[counter],'Timestampms'] + counter*interval_ms\n",
    "                    print(df.loc[timestamp_df.index[counter],'Record Number'], df.loc[timestamp_df.index[counter],'Date'], df.loc[timestamp_df.index[counter],'Timestampms'])\n",
    "        \n",
    "    # Add Prefix to Columns name (e.g. EC2/EOD_***) except Timestampms\n",
    "    if name == 'C2':\n",
    "        df.columns = df.columns.map(lambda x : 'EC2_'+x if x !='Timestampms' else x)\n",
    "    elif name == 'OD':\n",
    "        df.columns = df.columns.map(lambda x : 'EOD_'+x if x !='Timestampms' else x)\n",
    "        \n",
    "    # Shift Timestampms to first column\n",
    "    df = df[ ['Timestampms'] + [ col for col in df.columns if col != 'Timestampms' ] ]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def preprocess_timestamp(df):\n",
    "    # Find duplicates datetime in colum 'Date'\n",
    "    df_duplicate = df[df.duplicated('Timestampms',keep=False)]\n",
    "    for unique_datetime in df_duplicate['Timestampms'].unique():\n",
    "        timestamp_df = df_duplicate.loc[df_duplicate['Timestampms'] == unique_datetime]\n",
    "        difference_previous, difference_next = 0 , 0\n",
    "        #print(unique_datetime)\n",
    "        if timestamp_df.index[0]-1 >=0 and timestamp_df.index[-1]+1 <= len(df)-1:\n",
    "            difference_previous = unique_datetime - df.loc[timestamp_df.index[0]-1,'Timestampms']\n",
    "            difference_next = df.loc[timestamp_df.index[-1]+1,'Timestampms'] - unique_datetime\n",
    "            #print(difference_previous, difference_next)\n",
    "        \n",
    "        elif timestamp_df.index[-1] == len(df)-1:\n",
    "            difference_previous = unique_datetime - df.loc[timestamp_df.index[0]-1,'Timestampms']\n",
    "        elif timestamp_df.index[0] == 0:\n",
    "            difference_next = df.loc[timestamp_df.index[-1]+1,'Timestampms'] - unique_datetime\n",
    "        # Check if both (index[0]-1) and (index[-1]+1) are only difference of 1000 millisecond apart respectively\n",
    "        print(difference_previous, difference_next)\n",
    "        if not(difference_previous == 0 and difference_next == 0) and ((difference_previous == difference_next) or \n",
    "                                                                       (difference_previous <= 1000 and difference_next <= 1000) or\n",
    "                                                                       (difference_previous == 1000 and difference_next == 0) or\n",
    "                                                                       (difference_previous == 0 and difference_next == 1000)):\n",
    "            print(\"---Handling Duplicates, Split duplicate evenly---\")\n",
    "            #print(difference_previous, difference_next)\n",
    "            # Handle duplicates - Split duplicate evenly\n",
    "            # For every Timestamp (1000 millisecond), find the interval\n",
    "            interval_ms = round(1000 / len(timestamp_df.index))\n",
    "            for counter in range (0, len(timestamp_df.index)):\n",
    "                df.loc[timestamp_df.index[counter],'Timestampms'] = df.loc[timestamp_df.index[counter],'Timestampms'] + counter*interval_ms\n",
    "                print(df.loc[timestamp_df.index[counter],'EC2_Record Number'], df.loc[timestamp_df.index[counter],'EC2_Date'], df.loc[timestamp_df.index[counter],'Timestampms'])\n",
    "\n",
    "#         elif difference_previous == 1000 and difference_next >= 2000:\n",
    "#             print(\"---Handling Duplicates, Fill gap with duplicate---\")\n",
    "#             for counter in range (0, len(timestamp_df.index)):\n",
    "#                 df.loc[timestamp_df.index[counter],'Timestampms'] = df.loc[timestamp_df.index[counter],'Timestampms'] + counter*1000\n",
    "#                 print(df.loc[timestamp_df.index[counter],'EC2_Record Number'], df.loc[timestamp_df.index[counter],'EC2_Date'], df.loc[timestamp_df.index[counter],'Timestampms'])\n",
    "    \n",
    "    # Group resulting same real timestamp together\n",
    "    df = df.groupby(\"Timestampms\").last().reset_index()\n",
    "    \n",
    "    # Remove similar record numbers for both EC2 and EOD\n",
    "    #df_duplicate = df[df.duplicated('EC2_Record Number',keep='first')]\n",
    "    filter_col = [col for col in df if col.startswith('EC2_')]\n",
    "    df.loc[df.duplicated('EC2_Record Number',keep='first'), filter_col] = np.NaN\n",
    "    filter_col = [col for col in df if col.startswith('EOD_')]\n",
    "    df.loc[df.duplicated('EOD_Record Number',keep='first'), filter_col] = np.NaN\n",
    "    \n",
    "    df.to_csv('./result_2.csv', index=False, header=True)\n",
    "    #print(filter_col)\n",
    "#     print(df.head())\n",
    "#     print(df.loc[8, 'Timestampms'], df.loc[8, 'EC2_Record Number'], df.loc[8, 'EOD_Record Number'])\n",
    "#     print(df.loc[9, 'Timestampms'],df.loc[9, 'EC2_Record Number'], df.loc[9, 'EOD_Record Number'])\n",
    "#     print(df.loc[10, 'Timestampms'],df.loc[10, 'EC2_Record Number'], df.loc[10, 'EOD_Record Number'])\n",
    "        \n",
    "        \n",
    "    return df\n",
    "\n",
    "def unit_test(sample_output_filename, name, df_test):\n",
    "    print(\"---Unit Test for \" + name + \" Dataframe---\")\n",
    "    # Unit Test for C2\n",
    "    # Import Output File\n",
    "    df_output = pd.read_csv(sample_output_filename)\n",
    "    # Only retrieve respective columns\n",
    "    if name == 'C2':\n",
    "        df_output = df_output.drop(df_output[(df_output['EC2_002_Record_Number'].isnull())].index)\n",
    "        #drop column with prefix EOD\n",
    "        df_output = df_output.loc[:, ~df_output.columns.str.startswith('EOD')]\n",
    "    elif name == 'OD':\n",
    "        df_output = df_output.drop(df_output[(df_output['EOD_002_Record_Number'].isnull())].index)\n",
    "        #drop column with prefix EOD\n",
    "        df_output = df_output.loc[:, ~df_output.columns.str.startswith('EC2')]\n",
    "    \n",
    "    df_output['epoch'] = df_output.epoch.values.astype(np.float64)\n",
    "    df_output = df_output.reset_index(drop=True)\n",
    "    print(df_output.shape, df_test.shape)    \n",
    "    #Output to CSV\n",
    "    #df_output.to_csv('./df_output.csv', index=False, header=True)\n",
    "\n",
    "    #Output to CSV\n",
    "    #df_test.to_csv('./df_test.csv', index=False, header=True)\n",
    "    #df_drop_test = pd.read_csv('./df_test.csv')\n",
    "    df_drop_test = df_test\n",
    "    df_test['result'] = np.where(df_test['Timestampms'] == df_output['epoch'], 'True', 'False')\n",
    "    df_test.to_csv('./df_test.csv', index=False, header=True)\n",
    "    #df_drop_test = df_drop_test.sort_values(by='ATO_Real_Timestampms',ascending=True).reset_index(drop=True)\n",
    "    # Assert whether sample output and self processed are equal\n",
    "    assert_equal = omap.nan_equal(df_drop_test['Timestampms'].values, df_output['epoch'].values)\n",
    "    print(\"Equality Between Sample Output and Self Processed: \", assert_equal)\n",
    "    assert_equal = omap.nan_equal(df_drop_test['E'+name+'_Record Number'].values, df_output['E'+name+'_002_Record_Number'].values)\n",
    "    print(\"Equality Between Sample Output and Self Processed: \", assert_equal)\n",
    "    \n",
    "    #df_drop_test.columns = df_output.columns\n",
    "    #print(np.testing.assert_allclose(df_drop_test.values, df_output.values, rtol=1e-10, atol=0))\n",
    "    #print(pd.testing.assert_frame_equal(df_drop_test, df_output, check_dtype=False))\n",
    "    #print(df_drop_test.compare(df_output, align_axis=0))\n",
    "    #assert_equal = nan_equal(df_drop_test.values, df_output.values)\n",
    "    #assert_equal = nan_equal(df_drop['ATO_* General'].values, df_output['ATO_0101__General'].values)\n",
    "    #print(\"Equality Between Sample Output and Self Processed: \", assert_equal)\n",
    "    #print(np.testing.assert_equal(df_drop_test.values, df_output.values))\n",
    "    \n",
    "def unit_test_all(sample_output_filename, df_test):\n",
    "    print(\"---Unit Test for entire Dataframe---\")\n",
    "    # Import Output File\n",
    "    df_output = pd.read_csv(sample_output_filename)\n",
    "    print(df_output.shape, df_test.shape)\n",
    "    assert_equal = omap.nan_equal(df_test['Timestampms'].values, df_output['epoch'].values)\n",
    "    print(\"Equality Between Sample Output and Self Processed: \", assert_equal)\n",
    "    df_test['result'] = np.where(df_test['Timestampms'] == df_output['epoch'], 'True', 'False')\n",
    "    #df_test.to_csv('./df_test.csv', index=False, header=True)\n",
    "        \n",
    "    df_test['result_1'] = np.where(df_test['EC2_Record Number'] == df_output['EC2_002_Record_Number'], 'True', 'False')\n",
    "    df_test.to_csv('./df_test.csv', index=False, header=True)\n",
    "    assert_equal = omap.nan_equal(df_test['EC2_Record Number'].values, df_output['EC2_002_Record_Number'].values)\n",
    "    print(\"Equality Between Sample Output and Self Processed: \", assert_equal)\n",
    "    assert_equal = omap.nan_equal(df_test['EOD_Record Number'].values, df_output['EOD_002_Record_Number'].values)\n",
    "    print(\"Equality Between Sample Output and Self Processed: \", assert_equal)\n",
    "    print(np.testing.assert_equal(df_test['EC2_Record Number'].values, df_output['EC2_002_Record_Number'].values))\n",
    "    \n",
    "def output(sample_output_filename, df_result):\n",
    "    # Import Output File\n",
    "    df_output = pd.read_csv(sample_output_filename)\n",
    "    \n",
    "    df_combine = pd.DataFrame()\n",
    "    df_combine = df_combine.assign(epoch = df_output['epoch'])\n",
    "    df_combine = df_combine.assign(Timestampms = df_result['Timestampms']) \n",
    "    df_combine = df_combine.assign(EC2_Record_Number = df_result['EC2_Record Number']) \n",
    "    df_combine = df_combine.assign(EC2_Date = df_result['EC2_Date']) \n",
    "    df_combine = df_combine.assign(EOD_Record_Number = df_result['EOD_Record Number']) \n",
    "    df_combine = df_combine.assign(EOD_Date = df_result['EOD_Date']) \n",
    "\n",
    "    \n",
    "    df_combine.to_csv('./result_combine_1.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
