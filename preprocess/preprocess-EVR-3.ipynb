{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import timeit\n",
    "tqdm.pandas()\n",
    "import omap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-14 17:00:00\n",
      "2020-10-26 16:26:04\n",
      "2018-02-26 00:00:00\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "ts = int(\"1573750800\")\n",
    "\n",
    "# if you encounter a \"year is out of range\" error the timestamp\n",
    "# may be in milliseconds, try `ts /= 1000` in that case\n",
    "print(datetime.utcfromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "ts = int(\"1603729564\")\n",
    "\n",
    "# if you encounter a \"year is out of range\" error the timestamp\n",
    "# may be in milliseconds, try `ts /= 1000` in that case\n",
    "print(datetime.utcfromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "ts = int(\"1519603200\")\n",
    "\n",
    "# if you encounter a \"year is out of range\" error the timestamp\n",
    "# may be in milliseconds, try `ts /= 1000` in that case\n",
    "print(datetime.utcfromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVR folder exist!\n",
      "1603727407000 1603727408000.0\n",
      "1603727410000 1603727411000.0\n",
      "1603727420000 1603727421000.0\n",
      "1603727423000 1603727424000.0\n",
      "1603727423000 1603727426000.0\n",
      "1603727442000 1603727443000.0\n",
      "1603727442000 1603727445000.0\n",
      "1603727462000 1603727463000.0\n",
      "1603727462000 1603727465000.0\n",
      "1603727462000 1603727467000.0\n",
      "1603727484000 1603727485000.0\n",
      "1603727486000 1603727488000.0\n",
      "1603727502000 1603727503000.0\n",
      "1603727502000 1603727505000.0\n",
      "1603727524000 1603727525000.0\n",
      "1603727524000 1603727527000.0\n",
      "1603727530000 1603727532000.0\n",
      "1603727544000 1603727545000.0\n",
      "1603727544000 1603727547000.0\n",
      "1603727561000 1603727562000.0\n",
      "1603727565000 1603727566000.0\n",
      "1603727572000 1603727573000.0\n",
      "1603727581000 1603727582000.0\n",
      "1603727607000 1603727608000.0\n",
      "1603727639000 1603727640000.0\n",
      "1603727676000 1603727677000.0\n",
      "1603727680000 1603727682000.0\n",
      "1603727683000 1603727685000.0\n",
      "1603727686000 1603727688000.0\n",
      "1603727689000 1603727691000.0\n",
      "1603727692000 1603727694000.0\n",
      "1603727699000 1603727700000.0\n",
      "1603727708000 1603727709000.0\n",
      "1603727713000 1603727714000.0\n",
      "1603727717000 1603727718000.0\n",
      "1603727724000 1603727725000.0\n",
      "1603727724000 1603727727000.0\n",
      "1603727733000 1603727734000.0\n",
      "1603727741000 1603727742000.0\n",
      "1603727750000 1603727751000.0\n",
      "1603727783000 1603727785000.0\n",
      "1603727795000 1603727797000.0\n",
      "1603727800000 1603727802000.0\n",
      "1603727803000 1603727805000.0\n",
      "1603727806000 1603727808000.0\n",
      "1603727810000 1603727811000.0\n",
      "1603727813000 1603727814000.0\n",
      "1603727815000 1603727817000.0\n",
      "1603727818000 1603727820000.0\n",
      "1603727822000 1603727823000.0\n",
      "1603727825000 1603727826000.0\n",
      "1603727830000 1603727831000.0\n",
      "1603727861000 1603727862000.0\n",
      "1603727874000 1603727875000.0\n",
      "1603727874000 1603727877000.0\n",
      "---Handling Duplicates, Split duplicate evenly---\n",
      "32239 2020-10-26 15:58:14 1603727894000 32239\n",
      "32240 2020-10-26 15:58:14 1603727894500 32240\n",
      "1603727894500 1603727895000.0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-acebfa070d68>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m \u001b[0mdf_c2_clean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_od_clean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_evr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_folder_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_output_filename\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-72-acebfa070d68>\u001b[0m in \u001b[0;36mprocess_evr\u001b[1;34m(input_folder_path, start_time, end_time)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[0mdf_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_timestamp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_c2_clean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_od_clean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m             \u001b[1;31m#df_result.to_csv('./result_9.csv', index=False, header=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;31m# Merge C2 and OD files first\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-71-0447567eb6f5>\u001b[0m in \u001b[0;36mprocess_timestamp\u001b[1;34m(df_c2_clean, df_od_clean, start_time, end_time)\u001b[0m\n\u001b[0;32m     45\u001b[0m                 \u001b[0mresult_last_timestamp\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mdf_result_last_timestamp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTimestampms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_last_timestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_time_unix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[0mstart_time_unix\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mresult_last_timestamp\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1000\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdf_od_time_next\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mdf_od_time\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m                     \u001b[0mdf_od_processed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_od_time\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_od_clean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'OD'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m                     \u001b[0mdf_od_processed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_od_processed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_od_processed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Timestampms'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mstart_time_unix\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdf_od_processed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Timestampms'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mstart_time_unix\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4295\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4296\u001b[0m             \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast_scalar_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarn_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4297\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mgetitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4299\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "input_folder_path =  './EVR_T23_Car46_261020/'\n",
    "#input_folder_path = './Train 23 14NOV2019 Work Folder/raw logs'\n",
    "start_time = '2020/10/26 15:50:00' \n",
    "end_time = '2020/10/26 16:50:00'\n",
    "#start_time = '2020/10/26 04:36:45'\n",
    "#end_time = '2020/10/26 04:37:09'\n",
    "sample_output_filename = ['./EVR_T23_Car46_261020/preprocessing_output/EVR_20201026_1550_to_20201026_1650.csv']\n",
    "\n",
    "# input_folder_path = './Train 23 14NOV2019 Work Folder/raw logs'\n",
    "# #input_folder_path = './Train 23 14NOV2019 Work Folder/raw logs'\n",
    "# start_time = '2019/11/14 17:00:00' \n",
    "# end_time = '2019/11/14 18:00:00'\n",
    "# sample_output_filename = ['./Train 23 14NOV2019 Work Folder/preprocessing_output/EVR_20191114_1700_to_20191114_1800.csv']\n",
    "\n",
    "# input_folder_path = './Train 23 14NOV2019 Work Folder/raw logs'\n",
    "# #input_folder_path = './Train 23 14NOV2019 Work Folder/raw logs'\n",
    "# start_time = '2019/11/14 18:00:00' \n",
    "# end_time = '2019/11/14 19:00:00'\n",
    "# sample_output_filename = ['./Train 23 14NOV2019 Work Folder/preprocessing_output/EVR_20191114_1800_to_20191114_1900.csv']\n",
    "\n",
    "def process_evr (input_folder_path, start_time, end_time):\n",
    "    # Determine time difference between start and end time\n",
    "    datetime_format = '%Y/%m/%d %H:%M:%S'\n",
    "    start_time = datetime.datetime.strptime(start_time, datetime_format)\n",
    "    end_time = datetime.datetime.strptime(end_time, datetime_format)\n",
    "    start_time_new = start_time.strftime('%m/%d/%Y %H:%M:%S')\n",
    "    end_time_new = end_time.strftime('%m/%d/%Y %H:%M:%S')\n",
    "    # Check if EVR folder exist\n",
    "    if not os.path.exists(os.path.join(input_folder_path, 'EVR')):\n",
    "        print(\"EVR folder does not exist. Kindly check the folder strucutre. No EVR processing will be performed\")\n",
    "    else:\n",
    "        print(\"EVR folder exist!\")\n",
    "        # Check if C2 file exist\n",
    "        c2_pattern = 'EVR_Car\\d+_\\d+_C2.txt'\n",
    "        norm_pattern = 'EVR_Car\\d+_\\d+.txt'\n",
    "        c2_flag = False\n",
    "        norm_flag = False\n",
    "        for evr_file in os.listdir(os.path.join(input_folder_path, 'EVR')):\n",
    "            if re.search(c2_pattern, evr_file):\n",
    "                c2_filename = evr_file\n",
    "                c2_flag = True\n",
    "            if re.search(norm_pattern,evr_file):\n",
    "                norm_filename = evr_file\n",
    "                norm_flag = True\n",
    "\n",
    "        if not (c2_flag and norm_flag):\n",
    "            sys.exit(\"Missing EVR files! Kindly check EVR folder!\")\n",
    "        else:\n",
    "            # Process C2 file\n",
    "            df_c2 = pd.read_csv(os.path.join(input_folder_path, 'EVR', c2_filename), sep=\";\")\n",
    "            df_c2['Date'] = pd.to_datetime(df_c2['Date'], format='%m/%d/%Y %H:%M:%S')\n",
    "            df_c2_period = df_c2.loc[(df_c2['Date'] >= start_time_new) & (df_c2['Date'] <= end_time_new)]\n",
    "            #df_c2_clean_timestamp = preprocess_timestamp_old(df_c2_period)\n",
    "            df_c2_clean = clean_dataframe(df_c2_period, 'C2')\n",
    "            #df_c2_clean.to_csv('./result_c2.csv', index=False, header=True)\n",
    "            \n",
    "            # Processs Operating Data file\n",
    "            df_od = pd.read_csv(os.path.join(input_folder_path, 'EVR', norm_filename), sep=\";\")\n",
    "            df_od['Date'] = pd.to_datetime(df_od['Date'], format='%m/%d/%Y %H:%M:%S')\n",
    "            df_od_period = df_od.loc[(df_od['Date'] >= start_time_new) & (df_od['Date'] <= end_time_new)]\n",
    "            #df_od_clean_timestamp = preprocess_timestamp_old(df_od_period)\n",
    "            df_od_clean = clean_dataframe(df_od_period, 'OD')\n",
    "            #df_od_clean.to_csv('./result_od.csv', index=False, header=True)\n",
    "            \n",
    "            # Merge other Operating Data to C2 Dataframe based on Timestampms (ordered manner)\n",
    "            #df_result = pd.merge_ordered(df_c2_clean, df_od_clean, how='left', on='Timestampms')\n",
    "            #df_result.to_csv('./result_1.csv', index=False, header=True)\n",
    "            \n",
    "            \n",
    "            df_result = process_timestamp(df_c2_clean, df_od_clean, start_time, end_time)\n",
    "            #df_result.to_csv('./result_9.csv', index=False, header=True)\n",
    "            # Merge C2 and OD files first\n",
    "            \n",
    "#             # Group resulting same real timestamp together\n",
    "#             df_result = df_result.groupby(\"Timestampms\").last().reset_index()\n",
    "\n",
    "#             # Remove similar record numbers for both EC2 and EOD\n",
    "#             #df_duplicate = df[df.duplicated('EC2_Record Number',keep='first')]\n",
    "#             filter_col = [col for col in df_result if col.startswith('EC2_')]\n",
    "#             df_result.loc[df_result.duplicated('EC2_Record Number',keep='first'), filter_col] = np.NaN\n",
    "#             filter_col = [col for col in df_result if col.startswith('EOD_')]\n",
    "#             df_result.loc[df_result.duplicated('EOD_Record Number',keep='first'), filter_col] = np.NaN\n",
    "            \n",
    "            #df_result.to_csv('./result_8.csv', index=False, header=True)\n",
    "            # Clean Timestamp\n",
    "            #df_result = preprocess_timestamp(df_result)\n",
    "            \n",
    "            \n",
    "    return df_c2_clean, df_od_clean, df_result\n",
    "\n",
    "\n",
    "df_c2_clean, df_od_clean, df_result = process_evr(input_folder_path, start_time, end_time)\n",
    "\n",
    "output(sample_output_filename[0], df_result)\n",
    "#unit_test(sample_output_filename[0], 'C2', df_c2_clean)  \n",
    "#unit_test(sample_output_filename[0], 'OD', df_od_clean)  \n",
    "#unit_test_all(sample_output_filename[0], df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe(df, name):\n",
    "    # Drop columns with NaNs\n",
    "    df = df.dropna(how='any',axis=1)\n",
    "    # Sort df based on column 'Date'\n",
    "    df = df.sort_values(by='Date',ascending=True).reset_index(drop=True)\n",
    "    # Convert datetime to timestamp (Unix) in milisecond resolution\n",
    "    df['Timestamp'] = df.Date.values.astype(np.int64) // 10 ** 9 *1000\n",
    "    # Add column for Real_Time Timestampms\n",
    "    df['Timestampms'] = df['Timestamp'] \n",
    "    \n",
    "    # Add Prefix to Columns name (e.g. EC2/EOD_***) except Timestampms\n",
    "    if name == 'C2':\n",
    "        df.columns = df.columns.map(lambda x : 'EC2_'+x if x !='Timestampms' else x)\n",
    "    elif name == 'OD':\n",
    "        df.columns = df.columns.map(lambda x : 'EOD_'+x if x !='Timestampms' else x)\n",
    "                \n",
    "    df = df[ ['Timestampms'] + [ col for col in df.columns if col != 'Timestampms' ] ]    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_timestamp(df_c2_clean, df_od_clean, start_time, end_time):\n",
    "    start_time_unix = start_time.replace(tzinfo=datetime.timezone.utc).timestamp() *1000\n",
    "    end_time_unix = end_time.replace(tzinfo=datetime.timezone.utc).timestamp() *1000\n",
    "    \n",
    "    c2_columns = [ col for col in df_c2_clean.columns if col != 'Timestampms' ]\n",
    "    od_columns = [ col for col in df_od_clean.columns if col != 'Timestampms' ]\n",
    "    columns_names = ['Timestampms'] + c2_columns + od_columns\n",
    "    #print(columns_names)\n",
    "    df_result = pd.DataFrame(columns = columns_names)\n",
    "    while start_time_unix <= end_time_unix:\n",
    "        # Find time in both c2 and od\n",
    "        df_c2_time = df_c2_clean.loc[df_c2_clean['Timestampms'] == start_time_unix]\n",
    "        df_od_time = df_od_clean.loc[df_od_clean['Timestampms'] == start_time_unix]\n",
    "        \n",
    "        # Find time in both c2 and od next timestamp\n",
    "        df_c2_time_next = df_c2_clean.loc[df_c2_clean['Timestampms'] == start_time_unix+1000]\n",
    "        df_od_time_next = df_od_clean.loc[df_od_clean['Timestampms'] == start_time_unix+1000]\n",
    "        \n",
    "        # No valid timestamp in raw logs\n",
    "        if len(df_c2_time) == 0 and len(df_od_time) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            if len(df_c2_time) == 1 and len(df_od_time) == 1:\n",
    "                # Merge \n",
    "                df_merge = pd.merge_ordered(df_c2_time, df_od_time, how='outer', on='Timestampms')\n",
    "                #print(df_merge.values)\n",
    "                df_result = pd.concat([df_result, df_merge], ignore_index=True)\n",
    "                #df_result = df_result.append(df_merge)\n",
    "\n",
    "            # There are duplicates in C2\n",
    "            elif len(df_c2_time) > 1 and len(df_od_time) == 1:\n",
    "                #difference_previous, difference_next = find_difference(df_c2_time, start_time_unix, df_c2_clean)\n",
    "                #print(difference_previous, difference_next)\n",
    "                df_c2_processed = split_equally(df_c2_time, df_c2_clean, 'C2')\n",
    "                df_c2_processed = df_c2_processed.loc[(df_c2_processed['Timestampms'] >= start_time_unix) & (df_c2_processed['Timestampms'] < start_time_unix+1000)]              \n",
    "                #print(df_c2_processed)\n",
    "                # Merge \n",
    "                df_merge = pd.merge_ordered(df_c2_processed, df_od_time, how='outer', on='Timestampms')\n",
    "                #print(df_merge.values)\n",
    "                df_result = pd.concat([df_result, df_merge], ignore_index=True)\n",
    "                            \n",
    "            # There are duplicates in OD\n",
    "            elif len(df_c2_time) == 1 and len(df_od_time) > 1:\n",
    "                df_result_last_timestamp = df_result.loc[(df_result['Timestampms'] <= start_time_unix)]\n",
    "                result_last_timestamp =  df_result_last_timestamp.Timestampms.iat[-1] \n",
    "                print(result_last_timestamp, start_time_unix)\n",
    "                if start_time_unix - result_last_timestamp < 1000 and (df_od_time_next.index[0]-df_od_time.index[-1]==1):\n",
    "                    df_od_processed = split_forward(df_od_time, df_od_clean, 'OD')\n",
    "                    df_od_processed = df_od_processed.loc[(df_od_processed['Timestampms'] >= start_time_unix) & (df_od_processed['Timestampms'] <= start_time_unix+1000)]\n",
    "                    # Merge \n",
    "                    df_merge = pd.merge_ordered(df_c2_time, df_od_processed, how='outer', on='Timestampms')\n",
    "                    #print(df_merge.values)\n",
    "                    df_result = pd.concat([df_result, df_merge], ignore_index=True)\n",
    "                \n",
    "                \n",
    "                #difference_previous, difference_next = find_difference(df_od_time, start_time_unix, df_c2_clean)\n",
    "                \n",
    "                \n",
    "        start_time_unix += 1000\n",
    "    \n",
    "    print(df_result.head())\n",
    "    return df_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_difference(timestamp_df, unique_datetime, df):\n",
    "    difference_previous, difference_next = 0 , 0\n",
    "    if timestamp_df.index[0]-1 >=0 and timestamp_df.index[-1]+1 <= len(df)-1:\n",
    "        difference_previous = unique_datetime - df.loc[timestamp_df.index[0]-1,'Timestampms']\n",
    "        difference_next = df.loc[timestamp_df.index[-1]+1,'Timestampms'] - unique_datetime\n",
    "    elif timestamp_df.index[-1] == len(df)-1:\n",
    "        difference_previous = unique_datetime - df.loc[timestamp_df.index[0]-1,'Timestampms']\n",
    "    elif timestamp_df.index[0] == 0:\n",
    "        difference_next = df.loc[timestamp_df.index[-1]+1,'Timestampms'] - unique_datetime\n",
    "    \n",
    "    return difference_previous, difference_next\n",
    "\n",
    "def split_equally(timestamp_df, df, name):\n",
    "    print(\"---Handling Duplicates, Split duplicate evenly---\")\n",
    "    # For every Timestamp (1000 millisecond), find the interval\n",
    "    interval_ms = round(1000 / len(timestamp_df.index))\n",
    "    for counter in range (0, len(timestamp_df.index)):\n",
    "        df.loc[timestamp_df.index[counter],'Timestampms'] = df.loc[timestamp_df.index[counter],'Timestampms'] + counter*interval_ms\n",
    "        print(df.loc[timestamp_df.index[counter],'E'+name+'_Record Number'], df.loc[timestamp_df.index[counter],'E'+name+'_Date'],\n",
    "              df.loc[timestamp_df.index[counter],'Timestampms'],df.loc[timestamp_df.index[counter],'E'+name+'_Record Number'])\n",
    "    return df\n",
    "\n",
    "def split_forward(timestamp_df, df, name):\n",
    "    print(\"---Handling Duplicates, Fill gap with duplicate, Forward---\")\n",
    "    for counter in range (0, len(timestamp_df.index)):\n",
    "        df.loc[timestamp_df.index[counter],'Timestampms'] = df.loc[timestamp_df.index[counter],'Timestampms'] + counter*1000\n",
    "        print(df.loc[timestamp_df.index[counter],'E'+name+'_Record Number'], df.loc[timestamp_df.index[counter],'E'+name+'_Date'],\n",
    "              df.loc[timestamp_df.index[counter],'Timestampms'],df.loc[timestamp_df.index[counter],'E'+name+'_Record Number'])\n",
    "    return df\n",
    "    \n",
    "def output(sample_output_filename, df_result):\n",
    "    # Import Output File\n",
    "    df_output = pd.read_csv(sample_output_filename)\n",
    "    \n",
    "    df_combine = pd.DataFrame()\n",
    "    df_combine = df_combine.assign(epoch = df_output['epoch'])\n",
    "    df_combine = df_combine.assign(Timestampms = df_result['Timestampms']) \n",
    "    df_combine = df_combine.assign(EC2_Record_Number = df_result['EC2_Record Number']) \n",
    "    df_combine = df_combine.assign(EC2_Date = df_result['EC2_Date']) \n",
    "    df_combine = df_combine.assign(EOD_Record_Number = df_result['EOD_Record Number']) \n",
    "    df_combine = df_combine.assign(EOD_Date = df_result['EOD_Date']) \n",
    "\n",
    "    \n",
    "    df_combine.to_csv('./result_combine_1.csv', index=False, header=True)\n",
    "    \n",
    "def unit_test(sample_output_filename, name, df_test):\n",
    "    print(\"---Unit Test for \" + name + \" Dataframe---\")\n",
    "    # Unit Test for C2\n",
    "    # Import Output File\n",
    "    df_output = pd.read_csv(sample_output_filename)\n",
    "    # Only retrieve respective columns\n",
    "    if name == 'C2':\n",
    "        df_output = df_output.drop(df_output[(df_output['EC2_002_Record_Number'].isnull())].index)\n",
    "        #drop column with prefix EOD\n",
    "        df_output = df_output.loc[:, ~df_output.columns.str.startswith('EOD')]\n",
    "    elif name == 'OD':\n",
    "        df_output = df_output.drop(df_output[(df_output['EOD_002_Record_Number'].isnull())].index)\n",
    "        #drop column with prefix EOD\n",
    "        df_output = df_output.loc[:, ~df_output.columns.str.startswith('EC2')]\n",
    "    \n",
    "    df_output['epoch'] = df_output.epoch.values.astype(np.float64)\n",
    "    df_output = df_output.reset_index(drop=True)\n",
    "    print(df_output.shape, df_test.shape)    \n",
    "    #Output to CSV\n",
    "    #df_output.to_csv('./df_output.csv', index=False, header=True)\n",
    "\n",
    "    #Output to CSV\n",
    "    #df_test.to_csv('./df_test.csv', index=False, header=True)\n",
    "    #df_drop_test = pd.read_csv('./df_test.csv')\n",
    "    df_drop_test = df_test\n",
    "    df_test['result'] = np.where(df_test['Timestampms'] == df_output['epoch'], 'True', 'False')\n",
    "    df_test.to_csv('./df_test.csv', index=False, header=True)\n",
    "    #df_drop_test = df_drop_test.sort_values(by='ATO_Real_Timestampms',ascending=True).reset_index(drop=True)\n",
    "    # Assert whether sample output and self processed are equal\n",
    "    assert_equal = omap.nan_equal(df_drop_test['Timestampms'].values, df_output['epoch'].values)\n",
    "    print(\"Equality Between Sample Output and Self Processed: \", assert_equal)\n",
    "    assert_equal = omap.nan_equal(df_drop_test['E'+name+'_Record Number'].values, df_output['E'+name+'_002_Record_Number'].values)\n",
    "    print(\"Equality Between Sample Output and Self Processed: \", assert_equal)\n",
    "    \n",
    "    #df_drop_test.columns = df_output.columns\n",
    "    #print(np.testing.assert_allclose(df_drop_test.values, df_output.values, rtol=1e-10, atol=0))\n",
    "    #print(pd.testing.assert_frame_equal(df_drop_test, df_output, check_dtype=False))\n",
    "    #print(df_drop_test.compare(df_output, align_axis=0))\n",
    "    #assert_equal = nan_equal(df_drop_test.values, df_output.values)\n",
    "    #assert_equal = nan_equal(df_drop['ATO_* General'].values, df_output['ATO_0101__General'].values)\n",
    "    #print(\"Equality Between Sample Output and Self Processed: \", assert_equal)\n",
    "    #print(np.testing.assert_equal(df_drop_test.values, df_output.values))\n",
    "    \n",
    "def unit_test_all(sample_output_filename, df_test):\n",
    "    print(\"---Unit Test for entire Dataframe---\")\n",
    "    # Import Output File\n",
    "    df_output = pd.read_csv(sample_output_filename)\n",
    "    print(df_output.shape, df_test.shape)\n",
    "    assert_equal = omap.nan_equal(df_test['Timestampms'].values, df_output['epoch'].values)\n",
    "    print(\"Equality Between Sample Output and Self Processed: \", assert_equal)\n",
    "    df_test['result'] = np.where(df_test['Timestampms'] == df_output['epoch'], 'True', 'False')\n",
    "    #df_test.to_csv('./df_test.csv', index=False, header=True)\n",
    "        \n",
    "    df_test['result_1'] = np.where(df_test['EC2_Record Number'] == df_output['EC2_002_Record_Number'], 'True', 'False')\n",
    "    df_test.to_csv('./df_test.csv', index=False, header=True)\n",
    "    assert_equal = omap.nan_equal(df_test['EC2_Record Number'].values, df_output['EC2_002_Record_Number'].values)\n",
    "    print(\"Equality Between Sample Output and Self Processed: \", assert_equal)\n",
    "    assert_equal = omap.nan_equal(df_test['EOD_Record Number'].values, df_output['EOD_002_Record_Number'].values)\n",
    "    print(\"Equality Between Sample Output and Self Processed: \", assert_equal)\n",
    "    print(np.testing.assert_equal(df_test['EC2_Record Number'].values, df_output['EC2_002_Record_Number'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
